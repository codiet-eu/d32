{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5d43a848",
      "metadata": {
        "id": "5d43a848"
      },
      "source": [
        "## PC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e189ec03",
      "metadata": {
        "id": "e189ec03"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# Copyright (C) 2021. Huawei Technologies Co., Ltd. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "from copy import deepcopy\n",
        "from itertools import combinations, permutations\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "from castle.common import BaseLearner, Tensor\n",
        "from castle.common.independence_tests import CITest\n",
        "from castle.common.priori_knowledge import orient_by_priori_knowledge\n",
        "\n",
        "\n",
        "class PC(BaseLearner):\n",
        "    \"\"\"PC algorithm\n",
        "\n",
        "    A classic causal discovery algorithm based on conditional independence tests.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    [1] original-PC\n",
        "        https://www.jmlr.org/papers/volume8/kalisch07a/kalisch07a.pdf\n",
        "    [2] stable-PC\n",
        "        https://arxiv.org/pdf/1211.3295.pdf\n",
        "    [3] parallel-PC\n",
        "        https://arxiv.org/pdf/1502.02454.pdf\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    variant : str\n",
        "        A variant of PC-algorithm, one of [`original`, `stable`, `parallel`].\n",
        "    alpha: float, default 0.05\n",
        "        Significance level.\n",
        "    ci_test : str, callable\n",
        "        ci_test method, if str, must be one of [`fisherz`, `g2`, `chi2`]\n",
        "        See more: `castle.common.independence_tests.CITest`\n",
        "    priori_knowledge: PrioriKnowledge\n",
        "        a class object PrioriKnowledge\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    causal_matrix : array\n",
        "        Learned causal structure matrix.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from castle.common import GraphDAG\n",
        "    >>> from castle.metrics import MetricsDAG\n",
        "    >>> from castle.datasets import load_dataset\n",
        "\n",
        "    >>> X, true_dag, _ = load_dataset(name='IID_Test')\n",
        "    >>> pc = PC(variant='stable')\n",
        "    >>> pc.learn(X)\n",
        "    >>> GraphDAG(pc.causal_matrix, true_dag, save_name='result_pc')\n",
        "    >>> met = MetricsDAG(pc.causal_matrix, true_dag)\n",
        "    >>> print(met.metrics)\n",
        "\n",
        "    >>> pc = PC(variant='parallel')\n",
        "    >>> pc.learn(X, p_cores=2)\n",
        "    >>> GraphDAG(pc.causal_matrix, true_dag, save_name='result_pc')\n",
        "    >>> met = MetricsDAG(pc.causal_matrix, true_dag)\n",
        "    >>> print(met.metrics)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, variant='original', alpha=0.05, ci_test='fisherz',\n",
        "                 priori_knowledge=None):\n",
        "        super(PC, self).__init__()\n",
        "        self.variant = variant\n",
        "        self.alpha = alpha\n",
        "        self.ci_test = ci_test\n",
        "        self.causal_matrix = None\n",
        "        self.priori_knowledge = priori_knowledge\n",
        "\n",
        "    def learn(self, data, columns=None, **kwargs):\n",
        "        \"\"\"Set up and run the PC algorithm.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: array or Tensor\n",
        "            Training data\n",
        "        columns : Index or array-like\n",
        "            Column labels to use for resulting tensor. Will default to\n",
        "            RangeIndex (0, 1, 2, ..., n) if no column labels are provided.\n",
        "        kwargs: [optional]\n",
        "            p_cores : int\n",
        "                number of CPU cores to be used\n",
        "            s : boolean\n",
        "                memory-efficient indicator\n",
        "            batch : int\n",
        "                number of edges per batch\n",
        "            if s is None or False, or without batch, batch=|J|.\n",
        "            |J| denote number of all pairs of adjacency vertices (X, Y) in G.\n",
        "        \"\"\"\n",
        "\n",
        "        data = Tensor(data, columns=columns)\n",
        "\n",
        "        skeleton, sep_set = find_skeleton(data,\n",
        "                                          alpha=self.alpha,\n",
        "                                          ci_test=self.ci_test,\n",
        "                                          variant=self.variant,\n",
        "                                          priori_knowledge=self.priori_knowledge,\n",
        "                                          **kwargs)\n",
        "\n",
        "        self._causal_matrix = Tensor(\n",
        "            orient(skeleton, sep_set, self.priori_knowledge).astype(int),\n",
        "            index=data.columns,\n",
        "            columns=data.columns\n",
        "        )\n",
        "\n",
        "\n",
        "def _loop(G, d):\n",
        "    \"\"\"\n",
        "    Check if |adj(x, G)\\{y}| < d for every pair of adjacency vertices in G\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    G: numpy.ndarray\n",
        "        The undirected graph  G\n",
        "    d: int\n",
        "        depth of conditional vertices\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    out: bool\n",
        "        if False, denote |adj(i, G)\\{j}| < d for every pair of adjacency\n",
        "        vertices in G, then finished loop.\n",
        "    \"\"\"\n",
        "\n",
        "    assert G.shape[0] == G.shape[1]\n",
        "\n",
        "    pairs = [(x, y) for x, y in combinations(set(range(G.shape[0])), 2)]\n",
        "    less_d = 0\n",
        "    for i, j in pairs:\n",
        "        adj_i = set(np.argwhere(G[i] != 0).reshape(-1, ))\n",
        "        z = adj_i - {j}  # adj(C, i)\\{j}\n",
        "        if len(z) < d:\n",
        "            less_d += 1\n",
        "        else:\n",
        "            break\n",
        "    if less_d == len(pairs):\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "\n",
        "def orient(skeleton, sep_set, priori_knowledge=None):\n",
        "    \"\"\"Extending the Skeleton to the Equivalence Class\n",
        "\n",
        "    it orients the undirected edges to form an equivalence class of DAGs.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    skeleton : array\n",
        "        The undirected graph\n",
        "    sep_set : dict\n",
        "        separation sets\n",
        "        if key is (x, y), then value is a set of other variables\n",
        "        not contains x and y\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    out : array\n",
        "        An equivalence class of DAGs can be uniquely described\n",
        "        by a completed partially directed acyclic graph (CPDAG)\n",
        "        which includes both directed and undirected edges.\n",
        "    \"\"\"\n",
        "\n",
        "    if priori_knowledge is not None:\n",
        "        skeleton = orient_by_priori_knowledge(skeleton, priori_knowledge)\n",
        "\n",
        "    columns = list(range(skeleton.shape[1]))\n",
        "    cpdag = deepcopy(abs(skeleton))\n",
        "    # pre-processing\n",
        "    for ij in sep_set.keys():\n",
        "        i, j = ij\n",
        "        all_k = [x for x in columns if x not in ij]\n",
        "        for k in all_k:\n",
        "            if cpdag[i, k] + cpdag[k, i] != 0 \\\n",
        "                    and cpdag[k, j] + cpdag[j, k] != 0:\n",
        "                if k not in sep_set[ij]:\n",
        "                    if cpdag[i, k] + cpdag[k, i] == 2:\n",
        "                        cpdag[k, i] = 0\n",
        "                    if cpdag[j, k] + cpdag[k, j] == 2:\n",
        "                        cpdag[k, j] = 0\n",
        "    while True:\n",
        "        old_cpdag = deepcopy(cpdag)\n",
        "        pairs = list(combinations(columns, 2))\n",
        "        for ij in pairs:\n",
        "            i, j = ij\n",
        "            if cpdag[i, j] * cpdag[j, i] == 1:\n",
        "                # rule1\n",
        "                for i, j in permutations(ij, 2):\n",
        "                    all_k = [x for x in columns if x not in ij]\n",
        "                    for k in all_k:\n",
        "                        if cpdag[k, i] == 1 and cpdag[i, k] == 0 \\\n",
        "                                and cpdag[k, j] + cpdag[j, k] == 0:\n",
        "                            cpdag[j, i] = 0\n",
        "                # rule2\n",
        "                for i, j in permutations(ij, 2):\n",
        "                    all_k = [x for x in columns if x not in ij]\n",
        "                    for k in all_k:\n",
        "                        if (cpdag[i, k] == 1 and cpdag[k, i] == 0) \\\n",
        "                            and (cpdag[k, j] == 1 and cpdag[j, k] == 0):\n",
        "                            cpdag[j, i] = 0\n",
        "                # rule3\n",
        "                for i, j in permutations(ij, 2):\n",
        "                    for kl in sep_set.keys():  # k and l are nonadjacent.\n",
        "                        k, l = kl\n",
        "                        # if i——k——>j and  i——l——>j\n",
        "                        if cpdag[i, k] == 1 \\\n",
        "                                and cpdag[k, i] == 1 \\\n",
        "                                and cpdag[k, j] == 1 \\\n",
        "                                and cpdag[j, k] == 0 \\\n",
        "                                and cpdag[i, l] == 1 \\\n",
        "                                and cpdag[l, i] == 1 \\\n",
        "                                and cpdag[l, j] == 1 \\\n",
        "                                and cpdag[j, l] == 0:\n",
        "                            cpdag[j, i] = 0\n",
        "                # rule4\n",
        "                for i, j in permutations(ij, 2):\n",
        "                    for kj in sep_set.keys():  # k and j are nonadjacent.\n",
        "                        if j not in kj:\n",
        "                            continue\n",
        "                        else:\n",
        "                            kj = list(kj)\n",
        "                            kj.remove(j)\n",
        "                            k = kj[0]\n",
        "                            ls = [x for x in columns if x not in [i, j, k]]\n",
        "                            for l in ls:\n",
        "                                if cpdag[k, l] == 1 \\\n",
        "                                        and cpdag[l, k] == 0 \\\n",
        "                                        and cpdag[i, k] == 1 \\\n",
        "                                        and cpdag[k, i] == 1 \\\n",
        "                                        and cpdag[l, j] == 1 \\\n",
        "                                        and cpdag[j, l] == 0:\n",
        "                                    cpdag[j, i] = 0\n",
        "        if np.all(cpdag == old_cpdag):\n",
        "            break\n",
        "\n",
        "    return cpdag\n",
        "\n",
        "\n",
        "def find_skeleton(data, alpha, ci_test, variant='original',\n",
        "                  priori_knowledge=None, base_skeleton=None,\n",
        "                  p_cores=1, s=None, batch=None):\n",
        "    \"\"\"Find skeleton graph from G using PC algorithm\n",
        "\n",
        "    It learns a skeleton graph which contains only undirected edges\n",
        "    from data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : array, (n_samples, n_features)\n",
        "        Dataset with a set of variables V\n",
        "    alpha : float, default 0.05\n",
        "        significant level\n",
        "    ci_test : str, callable\n",
        "        ci_test method, if str, must be one of [`fisherz`, `g2`, `chi2`].\n",
        "        if callable, must return a tuple that  the last element is `p_value` ,\n",
        "        like (_, _, p_value) or (chi2, dof, p_value).\n",
        "        See more: `castle.common.independence_tests.CITest`\n",
        "    variant : str, default 'original'\n",
        "        variant of PC algorithm, contains [`original`, `stable`, `parallel`].\n",
        "        If variant == 'parallel', need to provide the flowing 3 parameters.\n",
        "    base_skeleton : array, (n_features, n_features)\n",
        "        prior matrix, must be undirected graph.\n",
        "        The two conditionals `base_skeleton[i, j] == base_skeleton[j, i]`\n",
        "        and `and base_skeleton[i, i] == 0` must be satisified which i != j.\n",
        "    p_cores : int\n",
        "        Number of CPU cores to be used\n",
        "    s : bool, default False\n",
        "        memory-efficient indicator\n",
        "    batch : int\n",
        "        number of edges per batch\n",
        "\n",
        "    if s is None or False, or without batch, batch=|J|.\n",
        "    |J| denote number of all pairs of adjacency vertices (X, Y) in G.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    skeleton : array\n",
        "        The undirected graph\n",
        "    seq_set : dict\n",
        "        Separation sets\n",
        "        Such as key is (x, y), then value is a set of other variables\n",
        "        not contains x and y.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from castle.algorithms.pc.pc import find_skeleton\n",
        "    >>> from castle.datasets import load_dataset\n",
        "\n",
        "    >>> true_dag, X = load_dataset(name='iid_test')\n",
        "    >>> skeleton, sep_set = find_skeleton(data, 0.05, 'fisherz')\n",
        "    >>> print(skeleton)\n",
        "    [[0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
        "     [0. 0. 0. 1. 1. 1. 1. 0. 1. 0.]\n",
        "     [1. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
        "     [0. 1. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
        "     [0. 1. 1. 1. 0. 0. 0. 0. 0. 1.]\n",
        "     [1. 1. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
        "     [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
        "     [0. 0. 1. 1. 0. 1. 0. 0. 0. 1.]\n",
        "     [0. 1. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
        "     [0. 0. 0. 1. 1. 1. 0. 1. 1. 0.]]\n",
        "    \"\"\"\n",
        "\n",
        "    def test(x, y):\n",
        "\n",
        "        K_x_y = 1\n",
        "        sub_z = None\n",
        "        # On X's neighbours\n",
        "        adj_x = set(np.argwhere(skeleton[x] == 1).reshape(-1, ))\n",
        "        z_x = adj_x - {y}  # adj(X, G)\\{Y}\n",
        "        if len(z_x) >= d:\n",
        "            # |adj(X, G)\\{Y}| >= d\n",
        "            for sub_z in combinations(z_x, d):\n",
        "                sub_z = list(sub_z)\n",
        "                _, _, p_value = ci_test(data, x, y, sub_z)\n",
        "                if p_value >= alpha:\n",
        "                    K_x_y = 0\n",
        "                    # sep_set[(x, y)] = sub_z\n",
        "                    break\n",
        "            if K_x_y == 0:\n",
        "                return K_x_y, sub_z\n",
        "\n",
        "        return K_x_y, sub_z\n",
        "\n",
        "    def parallel_cell(x, y):\n",
        "\n",
        "        # On X's neighbours\n",
        "        K_x_y, sub_z = test(x, y)\n",
        "        if K_x_y == 1:\n",
        "            # On Y's neighbours\n",
        "            K_x_y, sub_z = test(y, x)\n",
        "\n",
        "        return (x, y), K_x_y, sub_z\n",
        "\n",
        "    if ci_test == 'fisherz':\n",
        "        ci_test = CITest.fisherz_test\n",
        "    elif ci_test == 'g2':\n",
        "        ci_test = CITest.g2_test\n",
        "    elif ci_test == 'chi2':\n",
        "        ci_test = CITest.chi2_test\n",
        "    elif callable(ci_test):\n",
        "        ci_test = ci_test\n",
        "    else:\n",
        "        raise ValueError(f'The type of param `ci_test` expect callable,'\n",
        "                         f'but got {type(ci_test)}.')\n",
        "\n",
        "    n_feature = data.shape[1]\n",
        "    if base_skeleton is None:\n",
        "        skeleton = np.ones((n_feature, n_feature)) - np.eye(n_feature)\n",
        "    else:\n",
        "        row, col = np.diag_indices_from(base_skeleton)\n",
        "        base_skeleton[row, col] = 0\n",
        "        skeleton = base_skeleton\n",
        "    nodes = set(range(n_feature))\n",
        "\n",
        "    # update skeleton based on priori knowledge\n",
        "    for i, j in combinations(nodes, 2):\n",
        "        if priori_knowledge is not None and (\n",
        "                priori_knowledge.is_forbidden(i, j)\n",
        "                and priori_knowledge.is_forbidden(j, i)):\n",
        "            skeleton[i, j] = skeleton[j, i] = 0\n",
        "\n",
        "    sep_set = {}\n",
        "    d = -1\n",
        "    while _loop(skeleton, d):  # until for each adj(C,i)\\{j} < l\n",
        "        d += 1\n",
        "        if variant == 'stable':\n",
        "            C = deepcopy(skeleton)\n",
        "        else:\n",
        "            C = skeleton\n",
        "        if variant != 'parallel':\n",
        "            for i, j in combinations(nodes, 2):\n",
        "                if skeleton[i, j] == 0:\n",
        "                    continue\n",
        "                adj_i = set(np.argwhere(C[i] == 1).reshape(-1, ))\n",
        "                z = adj_i - {j}  # adj(C, i)\\{j}\n",
        "                if len(z) >= d:\n",
        "                    # |adj(C, i)\\{j}| >= l\n",
        "                    for sub_z in combinations(z, d):\n",
        "                        sub_z = list(sub_z)\n",
        "                        _, _, p_value = ci_test(data, i, j, sub_z)\n",
        "                        if p_value >= alpha:\n",
        "                            skeleton[i, j] = skeleton[j, i] = 0\n",
        "                            sep_set[(i, j)] = sub_z\n",
        "                            break\n",
        "        else:\n",
        "            J = [(x, y) for x, y in combinations(nodes, 2)\n",
        "                 if skeleton[x, y] == 1]\n",
        "            if not s or not batch:\n",
        "                batch = len(J)\n",
        "            if batch < 1:\n",
        "                batch = 1\n",
        "            if not p_cores or p_cores == 0:\n",
        "                raise ValueError(f'If variant is parallel, type of p_cores '\n",
        "                                 f'must be int, but got {type(p_cores)}.')\n",
        "            for i in range(int(np.ceil(len(J) / batch))):\n",
        "                each_batch = J[batch * i: batch * (i + 1)]\n",
        "                parallel_result = joblib.Parallel(n_jobs=p_cores,\n",
        "                                                  max_nbytes=None)(\n",
        "                    joblib.delayed(parallel_cell)(x, y) for x, y in\n",
        "                    each_batch\n",
        "                )\n",
        "                # Synchronisation Step\n",
        "                for (x, y), K_x_y, sub_z in parallel_result:\n",
        "                    if K_x_y == 0:\n",
        "                        skeleton[x, y] = skeleton[y, x] = 0\n",
        "                        sep_set[(x, y)] = sub_z\n",
        "\n",
        "    return skeleton, sep_set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "892f21e3",
      "metadata": {
        "id": "892f21e3"
      },
      "source": [
        "## PC_Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a14c05",
      "metadata": {
        "scrolled": true,
        "id": "44a14c05",
        "outputId": "7a218de8-2681-4069-8c5a-2d791e99a234"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-03 15:05:52,636 - C:\\Users\\notebook\\castle\\datasets\\simulator.py[line:270] - INFO: Finished synthetic dataset\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'fdr': 0.3158, 'tpr': 0.65, 'fpr': 0.24, 'shd': 10, 'nnz': 19, 'precision': 0.65, 'recall': 0.65, 'F1': 0.65, 'gscore': 0.3}\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'fdr': 0.2, 'tpr': 0.6, 'fpr': 0.12, 'shd': 8, 'nnz': 15, 'precision': 0.7059, 'recall': 0.6, 'F1': 0.6486, 'gscore': 0.35}\n"
          ]
        }
      ],
      "source": [
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "from castle.datasets import load_dataset\n",
        "\n",
        "X, true_dag, _ = load_dataset(name='IID_Test')\n",
        "pc = PC(variant='stable')\n",
        "pc.learn(X)\n",
        "GraphDAG(pc.causal_matrix, true_dag, save_name='result_pc')\n",
        "met = MetricsDAG(pc.causal_matrix, true_dag)\n",
        "print(met.metrics)\n",
        "\n",
        "pc = PC(variant='parallel')\n",
        "pc.learn(X, p_cores=2)\n",
        "GraphDAG(pc.causal_matrix, true_dag, save_name='result_pc')\n",
        "met = MetricsDAG(pc.causal_matrix, true_dag)\n",
        "print(met.metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dfa8c74",
      "metadata": {
        "id": "5dfa8c74",
        "outputId": "38442130-565e-4f92-dca7-d034c2386623"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-03 15:03:08,273 - C:\\Users\\notebook\\castle\\datasets\\simulator.py[line:270] - INFO: Finished synthetic dataset\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 1. 0. 0. 1. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 1. 1. 1. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
            " [0. 1. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
            " [0. 1. 1. 1. 0. 0. 0. 1. 0. 1.]\n",
            " [1. 1. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 1. 1. 1. 0. 0. 1. 0.]]\n"
          ]
        }
      ],
      "source": [
        "from castle.algorithms.pc.pc import find_skeleton\n",
        "from castle.datasets import load_dataset\n",
        "\n",
        "# true_dag, X = load_dataset(name='iid_test')\n",
        "X,true_dag, _ = load_dataset('IID_Test')\n",
        "skeleton, sep_set = find_skeleton(X, 0.05, 'fisherz')\n",
        "print(skeleton)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36f9afdf",
      "metadata": {
        "id": "36f9afdf"
      },
      "source": [
        "# ANMNonlinear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3856ada2",
      "metadata": {
        "id": "3856ada2"
      },
      "outputs": [],
      "source": [
        "# %load _anm.py\n",
        "# Copyright (C) 2021. Huawei Technologies Co., Ltd. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import scale\n",
        "from itertools import combinations\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from castle.common import BaseLearner, Tensor\n",
        "from castle.common.independence_tests import hsic_test\n",
        "\n",
        "\n",
        "class GPR(object):\n",
        "    \"\"\"Estimator based on Gaussian Process Regressor\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha : float or ndarray of shape (n_samples,), default=1e-10\n",
        "        Value added to the diagonal of the kernel matrix during fitting.\n",
        "        This can prevent a potential numerical issue during fitting, by\n",
        "        ensuring that the calculated values form a positive definite matrix.\n",
        "        It can also be interpreted as the variance of additional Gaussian\n",
        "        measurement noise on the training observations. Note that this is\n",
        "        different from using a `WhiteKernel`. If an array is passed, it must\n",
        "        have the same number of entries as the data used for fitting and is\n",
        "        used as datapoint-dependent noise level. Allowing to specify the\n",
        "        noise level directly as a parameter is mainly for convenience and\n",
        "        for consistency with Ridge.\n",
        "\n",
        "    kernel : kernel instance, default=None\n",
        "        The kernel specifying the covariance function of the GP. If None is\n",
        "        passed, the kernel ``ConstantKernel(1.0, constant_value_bounds=\"fixed\"\n",
        "        * RBF(1.0, length_scale_bounds=\"fixed\")`` is used as default. Note that\n",
        "        the kernel hyperparameters are optimized during fitting unless the\n",
        "        bounds are marked as \"fixed\".\n",
        "\n",
        "    optimizer : \"fmin_l_bfgs_b\" or callable, default=\"fmin_l_bfgs_b\"\n",
        "        Can either be one of the internally supported optimizers for optimizing\n",
        "        the kernel's parameters, specified by a string, or an externally\n",
        "        defined optimizer passed as a callable. If a callable is passed, it\n",
        "        must have the signature::\n",
        "\n",
        "            def optimizer(obj_func, initial_theta, bounds):\n",
        "                # * 'obj_func' is the objective function to be minimized, which\n",
        "                #   takes the hyperparameters theta as parameter and an\n",
        "                #   optional flag eval_gradient, which determines if the\n",
        "                #   gradient is returned additionally to the function value\n",
        "                # * 'initial_theta': the initial value for theta, which can be\n",
        "                #   used by local optimizers\n",
        "                # * 'bounds': the bounds on the values of theta\n",
        "                ....\n",
        "                # Returned are the best found hyperparameters theta and\n",
        "                # the corresponding value of the target function.\n",
        "                return theta_opt, func_min\n",
        "\n",
        "        Per default, the 'L-BGFS-B' algorithm from scipy.optimize.minimize\n",
        "        is used. If None is passed, the kernel's parameters are kept fixed.\n",
        "        Available internal optimizers are::\n",
        "\n",
        "            'fmin_l_bfgs_b'\n",
        "\n",
        "    n_restarts_optimizer : int, default=0\n",
        "        The number of restarts of the optimizer for finding the kernel's\n",
        "        parameters which maximize the log-marginal likelihood. The first run\n",
        "        of the optimizer is performed from the kernel's initial parameters,\n",
        "        the remaining ones (if any) from thetas sampled log-uniform randomly\n",
        "        from the space of allowed theta-values. If greater than 0, all bounds\n",
        "        must be finite. Note that n_restarts_optimizer == 0 implies that one\n",
        "        run is performed.\n",
        "\n",
        "    normalize_y : bool, default=False\n",
        "        Whether the target values y are normalized, the mean and variance of\n",
        "        the target values are set equal to 0 and 1 respectively. This is\n",
        "        recommended for cases where zero-mean, unit-variance priors are used.\n",
        "        Note that, in this implementation, the normalisation is reversed\n",
        "        before the GP predictions are reported.\n",
        "\n",
        "    copy_X_train : bool, default=True\n",
        "        If True, a persistent copy of the training data is stored in the\n",
        "        object. Otherwise, just a reference to the training data is stored,\n",
        "        which might cause predictions to change if the data is modified\n",
        "        externally.\n",
        "\n",
        "    random_state : int, RandomState instance or None, default=None\n",
        "        Determines random number generation used to initialize the centers.\n",
        "        Pass an int for reproducible results across multiple function calls.\n",
        "        See :term: `Glossary <random_state>`.\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "    from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> import numpy as np\n",
        "    >>> x = np.random.rand(10).reshape((-1, 1))\n",
        "    >>> y = np.random.rand(10).reshape((-1, 1))\n",
        "    >>> gpr = GPR(alpha=1e-10)\n",
        "    >>> y_pred = gpr.estimate(x, y)\n",
        "    >>> print(y_pred)\n",
        "    [[0.30898833]\n",
        "     [0.51335394]\n",
        "     [0.378371  ]\n",
        "     [0.47051942]\n",
        "     [0.51290679]\n",
        "     [0.29678631]\n",
        "     [0.77848816]\n",
        "     [0.47589755]\n",
        "     [0.21743226]\n",
        "     [0.35258412]]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(GPR, self).__init__()\n",
        "        self.regressor = GaussianProcessRegressor(**kwargs)\n",
        "\n",
        "    def estimate(self, x, y):\n",
        "        \"\"\"Fit Gaussian process regression model and predict x.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array\n",
        "            Variable seen as cause\n",
        "        y: array\n",
        "            Variable seen as effect\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_predict: array\n",
        "            regression predict values of x\n",
        "        \"\"\"\n",
        "\n",
        "        self.regressor.fit(x, y)\n",
        "        y_predict = self.regressor.predict(x)\n",
        "\n",
        "        return y_predict\n",
        "\n",
        "\n",
        "class ANMNonlinear(BaseLearner):\n",
        "    \"\"\"\n",
        "    Nonlinear causal discovery with additive noise models\n",
        "\n",
        "    Use GPML with Gaussian kernel and independent Gaussian noise,\n",
        "    optimizing the hyper-parameters for each regression individually.\n",
        "    For the independence test, we implemented the HSIC with a Gaussian kernel,\n",
        "    where we used the gamma distribution as an approximation for the\n",
        "    distribution of the HSIC under the null hypothesis of independence\n",
        "    in order to calculate the p-value of the test result.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    Hoyer, Patrik O and Janzing, Dominik and Mooij, Joris M and Peters,\n",
        "    Jonas and Schölkopf, Bernhard,\n",
        "    \"Nonlinear causal discovery with additive noise models\", NIPS 2009\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha : float, default 0.05\n",
        "        significance level be used to compute threshold\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    causal_matrix : array like shape of (n_features, n_features)\n",
        "        Learned causal structure matrix.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from castle.common import GraphDAG\n",
        "    >>> from castle.metrics import MetricsDAG\n",
        "    >>> from castle.datasets import DAG, IIDSimulation\n",
        "    >>> from castle.algorithms.anm import ANMNonlinear\n",
        "\n",
        "    >>> weighted_random_dag = DAG.erdos_renyi(n_nodes=6, n_edges=10,\n",
        "    >>>                                      weight_range=(0.5, 2.0), seed=1)\n",
        "    >>> dataset = IIDSimulation(W=weighted_random_dag, n=1000,\n",
        "    >>>                         method='nonlinear', sem_type='gp-add')\n",
        "    >>> true_dag, X = dataset.B, dataset.X\n",
        "\n",
        "    >>> anm = ANMNonlinear(alpha=0.05)\n",
        "    >>> anm.learn(data=X)\n",
        "\n",
        "    >>> # plot predict_dag and true_dag\n",
        "    >>> GraphDAG(anm.causal_matrix, true_dag, show=False, save_name='result')\n",
        "\n",
        "    you can also provide more parameters to use it. like the flowing:\n",
        "    >>> from sklearn.gaussian_process.kernels import Matern, RBF\n",
        "    >>> kernel = Matern(nu=1.5)\n",
        "    >>> # kernel = 1.0 * RBF(1.0)\n",
        "    >>> anm = ANMNonlinear(alpha=0.05)\n",
        "    >>> anm.learn(data=X, regressor=GPR(kernel=kernel))\n",
        "    >>> # plot predict_dag and true_dag\n",
        "    >>> GraphDAG(anm.causal_matrix, true_dag, show=False, save_name='result')\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=0.05):\n",
        "        super(ANMNonlinear, self).__init__()\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def learn(self, data, columns=None, regressor=GPR(), test_method=hsic_test, **kwargs):\n",
        "        \"\"\"Set up and run the ANM_Nonlinear algorithm.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: numpy.ndarray or Tensor\n",
        "            Training data.\n",
        "        columns : Index or array-like\n",
        "            Column labels to use for resulting tensor. Will default to\n",
        "            RangeIndex (0, 1, 2, ..., n) if no column labels are provided.\n",
        "        regressor: Class\n",
        "            Nonlinear regression estimator, if not provided, it is GPR.\n",
        "            If user defined, must implement `estimate` method. such as :\n",
        "                `regressor.estimate(x, y)`\n",
        "        test_method: callable, default test_method\n",
        "            independence test method, if not provided, it is HSIC.\n",
        "            If user defined, must accept three arguments--x, y and keyword\n",
        "            argument--alpha. such as :\n",
        "                `test_method(x, y, alpha=0.05)`\n",
        "        \"\"\"\n",
        "\n",
        "        self.regressor = regressor\n",
        "\n",
        "        # create learning model and ground truth model\n",
        "        data = Tensor(data, columns=columns)\n",
        "\n",
        "        node_num = data.shape[1]\n",
        "        self.causal_matrix = Tensor(np.zeros((node_num, node_num)),\n",
        "                                    index=data.columns,\n",
        "                                    columns=data.columns)\n",
        "\n",
        "        for i, j in combinations(range(node_num), 2):\n",
        "            x = data[:, i].reshape((-1, 1))\n",
        "            y = data[:, j].reshape((-1, 1))\n",
        "\n",
        "            flag = test_method(x, y, alpha=self.alpha)\n",
        "            if flag == 1:\n",
        "                continue\n",
        "            # test x-->y\n",
        "            flag = self.anm_estimate(x, y, regressor=regressor,\n",
        "                                     test_method=test_method)\n",
        "            if flag:\n",
        "                self.causal_matrix[i, j] = 1\n",
        "            # test y-->x\n",
        "            flag = self.anm_estimate(y, x, regressor=regressor,\n",
        "                                     test_method=test_method)\n",
        "            if flag:\n",
        "                self.causal_matrix[j, i] = 1\n",
        "\n",
        "    def anm_estimate(self, x, y, regressor=GPR(), test_method=hsic_test):\n",
        "        \"\"\"Compute the fitness score of the ANM model in the x->y direction.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: array\n",
        "            Variable seen as cause\n",
        "        y: array\n",
        "            Variable seen as effect\n",
        "        regressor: Class\n",
        "            Nonlinear regression estimator, if not provided, it is GPR.\n",
        "            If user defined, must implement `estimate` method. such as :\n",
        "                `regressor.estimate(x, y)`\n",
        "        test_method: callable, default test_method\n",
        "            independence test method, if not provided, it is HSIC.\n",
        "            If user defined, must accept three arguments--x, y and keyword\n",
        "            argument--alpha. such as :\n",
        "                `test_method(x, y, alpha=0.05)`\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        out: int, 0 or 1\n",
        "            If 1, residuals n is independent of x, then accept x --> y\n",
        "            If 0, residuals n is not independent of x, then reject x --> y\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "        >>> import numpy as np\n",
        "        >>> from castle.algorithms.anm import ANMNonlinear\n",
        "        >>> np.random.seed(1)\n",
        "        >>> x = np.random.rand(500, 2)\n",
        "        >>> anm = ANMNonlinear(alpha=0.05)\n",
        "        >>> print(anm.anm_estimate(x[:, [0]], x[:, [1]]))\n",
        "        1\n",
        "        \"\"\"\n",
        "\n",
        "        x = scale(x).reshape((-1, 1))\n",
        "        y = scale(y).reshape((-1, 1))\n",
        "\n",
        "        y_predict = regressor.estimate(x, y)\n",
        "        flag = test_method(y - y_predict, x, alpha=self.alpha)\n",
        "\n",
        "        return flag\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "389636db",
      "metadata": {
        "id": "389636db"
      },
      "source": [
        "# ANMNonlinear_Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce14d3aa",
      "metadata": {
        "scrolled": true,
        "id": "ce14d3aa",
        "outputId": "868de99a-1bad-4785-e25b-58f9fbea43bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.523]\n",
            " [0.081]\n",
            " [0.64 ]\n",
            " [0.355]\n",
            " [0.571]\n",
            " [0.687]\n",
            " [0.53 ]\n",
            " [0.702]\n",
            " [0.352]\n",
            " [0.742]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "x = np.random.rand(10).reshape((-1, 1))\n",
        "y = np.random.rand(10).reshape((-1, 1))\n",
        "gpr = GPR(alpha=1e-10)\n",
        "y_pred = gpr.estimate(x, y)\n",
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "145fb6d8",
      "metadata": {
        "id": "145fb6d8",
        "outputId": "3fb73f9a-5894-4844-fb10-e2d7fee835b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-03-31 18:43:47,647 - C:\\Users\\notebook\\castle\\datasets\\simulator.py[line:270] - INFO: Finished synthetic dataset\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'fdr': 0.1, 'tpr': 1.0, 'fpr': 0.1667, 'shd': 1, 'nnz': 10, 'precision': 0.8182, 'recall': 1.0, 'F1': 0.9, 'gscore': 0.7778}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'fdr': 0.0, 'tpr': 0.0, 'fpr': 0.0, 'shd': 9, 'nnz': 0, 'precision': nan, 'recall': 0.0, 'F1': nan, 'gscore': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\notebook\\castle\\metrics\\evaluation.py:224: RuntimeWarning: invalid value encountered in true_divide\n",
            "  precision = TP/TP_FP\n"
          ]
        }
      ],
      "source": [
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "from castle.datasets import DAG, IIDSimulation\n",
        "from castle.algorithms.anm import ANMNonlinear\n",
        "\n",
        "weighted_random_dag = DAG.erdos_renyi(n_nodes=6, n_edges=10,\n",
        "                                     weight_range=(0.5, 2.0), seed=1)\n",
        "dataset = IIDSimulation(W=weighted_random_dag, n=1000,\n",
        "                        method='nonlinear', sem_type='gp-add')\n",
        "true_dag, X = dataset.B, dataset.X\n",
        "\n",
        "anm = ANMNonlinear(alpha=0.05)\n",
        "anm.learn(data=X)\n",
        "\n",
        "# plot predict_dag and true_dag\n",
        "GraphDAG(anm.causal_matrix, true_dag, save_name='result_anm')\n",
        "# from castle.datasets import load_dataset\n",
        "# X, true_dag, _ = load_dataset(name='IID_Test')\n",
        "met = MetricsDAG(anm.causal_matrix, true_dag)\n",
        "print(met.metrics)\n",
        "\n",
        "from sklearn.gaussian_process.kernels import Matern, RBF\n",
        "kernel = Matern(nu=1.5)\n",
        "# kernel = 1.0 * RBF(1.0)\n",
        "anm = ANMNonlinear(alpha=0.05)\n",
        "anm.learn(data=X, regressor=GPR(kernel=kernel))\n",
        "# plot predict_dag and true_dag\n",
        "GraphDAG(anm.causal_matrix, true_dag, save_name='result_anm')\n",
        "met = MetricsDAG(anm.causal_matrix, true_dag)\n",
        "print(met.metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3761a94f",
      "metadata": {
        "id": "3761a94f",
        "outputId": "e2cfaeed-a1ae-40e1-be39-a39c20865747"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from castle.algorithms.anm import ANMNonlinear\n",
        "np.random.seed(1)\n",
        "x = np.random.rand(500, 2)\n",
        "anm = ANMNonlinear(alpha=0.05)\n",
        "print(anm.anm_estimate(x[:, [0]], x[:, [1]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "764a3dfc",
      "metadata": {
        "id": "764a3dfc"
      },
      "source": [
        "# DirectLiNGAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77e74cb6",
      "metadata": {
        "id": "77e74cb6"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# Copyright (C) 2021. Huawei Technologies Co., Ltd. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.utils import check_array\n",
        "from sklearn.preprocessing import scale\n",
        "from castle.algorithms.lingam.utils.base import _BaseLiNGAM\n",
        "from castle.common import BaseLearner, Tensor\n",
        "from castle.common.consts import DIRECT_LINGAM_VALID_PARAMS\n",
        "from castle.common.validator import check_args_value\n",
        "\n",
        "\n",
        "class DirectLiNGAM(_BaseLiNGAM, BaseLearner):\n",
        "    \"\"\"\n",
        "    DirectLiNGAM Algorithm.\n",
        "    A direct learning algorithm for linear non-Gaussian acyclic model (LiNGAM).\n",
        "    Implementation of DirectLiNGAM Algorithm [1]_ [2]_, Construct a DirectLiNGAM model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    prior_knowledge : array-like, shape (n_features, n_features), optional (default=None)\n",
        "        Prior knowledge used for causal discovery, where ``n_features`` is the number of features.\n",
        "\n",
        "        The elements of prior knowledge matrix are defined as follows [1]_:\n",
        "\n",
        "        * ``0`` : :math:`x_i` does not have a directed path to :math:`x_j`\n",
        "        * ``1`` : :math:`x_i` has a directed path to :math:`x_j`\n",
        "        * ``-1`` : No prior knowledge is available to know if either of the two cases above (0 or 1) is true.\n",
        "    measure : {'pwling', 'kernel'}, default='pwling'\n",
        "        Measure to evaluate independence: 'pwling' [2]_ or 'kernel' [1]_.\n",
        "    thresh : float,  default='0.3'\n",
        "        Drop edge if |weight| < threshold\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    causal_matrix : numpy.ndarray\n",
        "        Learned causal structure matrix.\n",
        "    weight_causal_matrix: numpy.ndarray\n",
        "        Learned weighted causal structure matrix.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] S. Shimizu, T. Inazumi, Y. Sogawa, A. Hyvärinen, Y. Kawahara, T. Washio, P. O. Hoyer and K. Bollen.\n",
        "       DirectLiNGAM: A direct method for learning a linear non-Gaussian structural equation model.\n",
        "       Journal of Machine Learning Research, 12(Apr): 1225--1248, 2011.\n",
        "    .. [2] A. Hyvärinen and S. M. Smith. Pairwise likelihood ratios for estimation of non-Gaussian structural eauation models.\n",
        "       Journal of Machine Learning Research 14:111-152, 2013.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from castle.algorithms import DirectLiNGAM\n",
        "    >>> from castle.datasets import load_dataset\n",
        "    >>> from castle.common import GraphDAG\n",
        "    >>> from castle.metrics import MetricsDAG\n",
        "    >>> X, true_dag, _ = load_dataset(name='IID_Test')\n",
        "    >>> n = DirectLiNGAM()\n",
        "    >>> n.learn(X)\n",
        "    >>> GraphDAG(n.causal_matrix, true_dag)\n",
        "    >>> met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "    >>> print(met.metrics)\n",
        "    \"\"\"\n",
        "\n",
        "    @check_args_value(DIRECT_LINGAM_VALID_PARAMS)\n",
        "    def __init__(self, prior_knowledge=None, measure='pwling', thresh=0.3):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self._prior_knowledge = prior_knowledge\n",
        "        self._measure = measure\n",
        "        self._thresh = thresh\n",
        "\n",
        "    def learn(self, data, columns=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Set up and run the DirectLiNGAM algorithm.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: castle.Tensor or numpy.ndarray\n",
        "            The castle.Tensor or numpy.ndarray format data you want to learn.\n",
        "        columns : Index or array-like\n",
        "            Column labels to use for resulting tensor. Will default to\n",
        "            RangeIndex (0, 1, 2, ..., n) if no column labels are provided.\n",
        "        \"\"\"\n",
        "\n",
        "        X = Tensor(data, columns=columns)\n",
        "\n",
        "        self.fit(X)\n",
        "\n",
        "        weight_causal_matrix = self.adjacency_matrix_.T\n",
        "        self.weight_causal_matrix = Tensor(weight_causal_matrix,\n",
        "                                           index=X.columns,\n",
        "                                           columns=X.columns)\n",
        "\n",
        "        causal_matrix = (abs(self.adjacency_matrix_) > self._thresh).astype(int).T\n",
        "        self.causal_matrix = Tensor(causal_matrix,\n",
        "                                    index=X.columns,\n",
        "                                    columns=X.columns)\n",
        "\n",
        "    def fit(self, X):\n",
        "        \"\"\"\n",
        "        Fit the model to X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Training data, where ``n_samples`` is the number of samples\n",
        "            and ``n_features`` is the number of features.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "            Returns the instance itself.\n",
        "        \"\"\"\n",
        "        # Check parameters\n",
        "        X = check_array(X)\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        if self._prior_knowledge is not None:\n",
        "            self._Aknw = check_array(self._prior_knowledge)\n",
        "            self._Aknw = np.where(self._Aknw < 0, np.nan, self._Aknw)\n",
        "            if (n_features, n_features) != self._Aknw.shape:\n",
        "                raise ValueError(\n",
        "                    'The shape of prior knowledge must be (n_features, n_features)')\n",
        "        else:\n",
        "            self._Aknw = None\n",
        "\n",
        "        # Causal discovery\n",
        "        U = np.arange(n_features)\n",
        "        K = []\n",
        "        X_ = np.copy(X)\n",
        "        if self._measure == 'kernel':\n",
        "            X_ = scale(X_)\n",
        "\n",
        "        for _ in range(n_features):\n",
        "            if self._measure == 'kernel':\n",
        "                m = self._search_causal_order_kernel(X_, U)\n",
        "            else:\n",
        "                m = self._search_causal_order(X_, U)\n",
        "            for i in U:\n",
        "                if i != m:\n",
        "                    X_[:, i] = self._residual(X_[:, i], X_[:, m])\n",
        "            K.append(m)\n",
        "            U = U[U != m]\n",
        "\n",
        "        self._causal_order = K\n",
        "        return self._estimate_adjacency_matrix(X)\n",
        "\n",
        "    def _residual(self, xi, xj):\n",
        "        \"\"\"The residual when xi is regressed on xj.\"\"\"\n",
        "        return xi - (np.cov(xi, xj)[0, 1] / np.var(xj)) * xj\n",
        "\n",
        "    def _entropy(self, u):\n",
        "        \"\"\"Calculate entropy using the maximum entropy approximations.\"\"\"\n",
        "        k1 = 79.047\n",
        "        k2 = 7.4129\n",
        "        gamma = 0.37457\n",
        "        return (1 + np.log(2 * np.pi)) / 2 - \\\n",
        "            k1 * (np.mean(np.log(np.cosh(u))) - gamma)**2 - \\\n",
        "            k2 * (np.mean(u * np.exp((-u**2) / 2)))**2\n",
        "\n",
        "    def _diff_mutual_info(self, xi_std, xj_std, ri_j, rj_i):\n",
        "        \"\"\"Calculate the difference of the mutual informations.\"\"\"\n",
        "        return (self._entropy(xj_std) + self._entropy(ri_j / np.std(ri_j))) - \\\n",
        "               (self._entropy(xi_std) + self._entropy(rj_i / np.std(rj_i)))\n",
        "\n",
        "    def _search_candidate(self, U):\n",
        "        \"\"\" Search for candidate features \"\"\"\n",
        "        # If no prior knowledge is specified, nothing to do.\n",
        "        if self._Aknw is None:\n",
        "            return U, []\n",
        "\n",
        "        # Find exogenous features\n",
        "        Uc = []\n",
        "        for j in U:\n",
        "            index = U[U != j]\n",
        "            if self._Aknw[j][index].sum() == 0:\n",
        "                Uc.append(j)\n",
        "\n",
        "        # Find endogenous features, and then find candidate features\n",
        "        if len(Uc) == 0:\n",
        "            U_end = []\n",
        "            for j in U:\n",
        "                index = U[U != j]\n",
        "                if np.nansum(self._Aknw[j][index]) > 0:\n",
        "                    U_end.append(j)\n",
        "\n",
        "            # Find sink features (original)\n",
        "            for i in U:\n",
        "                index = U[U != i]\n",
        "                if self._Aknw[index, i].sum() == 0:\n",
        "                    U_end.append(i)\n",
        "            Uc = [i for i in U if i not in set(U_end)]\n",
        "\n",
        "        # make V^(j)\n",
        "        Vj = []\n",
        "        for i in U:\n",
        "            if i in Uc:\n",
        "                continue\n",
        "            if self._Aknw[i][Uc].sum() == 0:\n",
        "                Vj.append(i)\n",
        "        return Uc, Vj\n",
        "\n",
        "    def _search_causal_order(self, X, U):\n",
        "        \"\"\"Search the causal ordering.\"\"\"\n",
        "        Uc, Vj = self._search_candidate(U)\n",
        "        if len(Uc) == 1:\n",
        "            return Uc[0]\n",
        "\n",
        "        M_list = []\n",
        "        for i in Uc:\n",
        "            M = 0\n",
        "            for j in U:\n",
        "                if i != j:\n",
        "                    xi_std = (X[:, i] - np.mean(X[:, i])) / np.std(X[:, i])\n",
        "                    xj_std = (X[:, j] - np.mean(X[:, j])) / np.std(X[:, j])\n",
        "                    ri_j = xi_std if i in Vj and j in Uc else self._residual(xi_std, xj_std)\n",
        "                    rj_i = xj_std if j in Vj and i in Uc else self._residual(xj_std, xi_std)\n",
        "                    M += np.min([0, self._diff_mutual_info(xi_std, xj_std, ri_j, rj_i)])**2\n",
        "            M_list.append(-1.0 * M)\n",
        "        return Uc[np.argmax(M_list)]\n",
        "\n",
        "    def _mutual_information(self, x1, x2, param):\n",
        "        \"\"\"Calculate the mutual informations.\"\"\"\n",
        "        kappa, sigma = param\n",
        "        n = len(x1)\n",
        "        X1 = np.tile(x1, (n, 1))\n",
        "        K1 = np.exp(-1/(2*sigma**2) * (X1**2 + X1.T**2 - 2*X1*X1.T))\n",
        "        X2 = np.tile(x2, (n, 1))\n",
        "        K2 = np.exp(-1/(2*sigma**2) * (X2**2 + X2.T**2 - 2*X2*X2.T))\n",
        "\n",
        "        tmp1 = K1 + n*kappa*np.identity(n)/2\n",
        "        tmp2 = K2 + n*kappa*np.identity(n)/2\n",
        "        K_kappa = np.r_[np.c_[tmp1 @ tmp1, K1 @ K2],\n",
        "                        np.c_[K2 @ K1, tmp2 @ tmp2]]\n",
        "        D_kappa = np.r_[np.c_[tmp1 @ tmp1, np.zeros([n, n])],\n",
        "                        np.c_[np.zeros([n, n]), tmp2 @ tmp2]]\n",
        "\n",
        "        sigma_K = np.linalg.svd(K_kappa, compute_uv=False)\n",
        "        sigma_D = np.linalg.svd(D_kappa, compute_uv=False)\n",
        "\n",
        "        return (-1/2)*(np.sum(np.log(sigma_K)) - np.sum(np.log(sigma_D)))\n",
        "\n",
        "    def _search_causal_order_kernel(self, X, U):\n",
        "        \"\"\"Search the causal ordering by kernel method.\"\"\"\n",
        "        Uc, Vj = self._search_candidate(U)\n",
        "        if len(Uc) == 1:\n",
        "            return Uc[0]\n",
        "\n",
        "        if X.shape[0] > 1000:\n",
        "            param = [2e-3, 0.5]\n",
        "        else:\n",
        "            param = [2e-2, 1.0]\n",
        "\n",
        "        Tkernels = []\n",
        "        for j in Uc:\n",
        "            Tkernel = 0\n",
        "            for i in U:\n",
        "                if i != j:\n",
        "                    ri_j = X[:, i] if j in Vj and i in Uc else self._residual(\n",
        "                        X[:, i], X[:, j])\n",
        "                    Tkernel += self._mutual_information(X[:, j], ri_j, param)\n",
        "            Tkernels.append(Tkernel)\n",
        "\n",
        "        return Uc[np.argmin(Tkernels)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c9eba70",
      "metadata": {
        "id": "1c9eba70"
      },
      "source": [
        "# DirectLiNGAM_Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "038699cb",
      "metadata": {
        "id": "038699cb",
        "outputId": "1ceb1e39-2fca-46fc-86c4-339625bc6869"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-03-31 18:55:13,687 - C:\\Users\\notebook\\castle\\datasets\\simulator.py[line:270] - INFO: Finished synthetic dataset\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'fdr': 0.5652, 'tpr': 0.5, 'fpr': 0.52, 'shd': 19, 'nnz': 23, 'precision': 0.4348, 'recall': 0.5, 'F1': 0.4651, 'gscore': 0.0}\n"
          ]
        }
      ],
      "source": [
        "from castle.algorithms import DirectLiNGAM\n",
        "from castle.datasets import load_dataset\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "X, true_dag, _ = load_dataset(name='IID_Test')\n",
        "n = DirectLiNGAM()\n",
        "n.learn(X)\n",
        "GraphDAG(n.causal_matrix, true_dag)\n",
        "met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "print(met.metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db11c954",
      "metadata": {
        "id": "db11c954"
      },
      "source": [
        "# ICALiNGAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f803c9a",
      "metadata": {
        "id": "1f803c9a"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# Copyright (C) 2021. Huawei Technologies Co., Ltd. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import numpy as np\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from sklearn.utils import check_array\n",
        "from sklearn.decomposition import FastICA\n",
        "\n",
        "from castle.algorithms.lingam.utils.base import _BaseLiNGAM\n",
        "from castle.common import BaseLearner, Tensor\n",
        "\n",
        "\n",
        "class ICALiNGAM(_BaseLiNGAM, BaseLearner):\n",
        "    \"\"\"\n",
        "    ICALiNGAM Algorithm.\n",
        "    An ICA-based learning algorithm for linear non-Gaussian acyclic model (LiNGAM).\n",
        "    Implementation of ICA-based LiNGAM Algorithm [1]_, Construct a ICA-based LiNGAM model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    random_state : int, optional (default=None)\n",
        "        ``random_state`` is the seed used by the random number generator.\n",
        "    max_iter : int, optional (default=1000)\n",
        "        The maximum number of iterations of FastICA.\n",
        "    thresh : float,  default='0.3'\n",
        "        Drop edge if |weight| < threshold\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    causal_matrix : numpy.ndarray\n",
        "        Learned causal structure matrix\n",
        "    weight_causal_matrix: numpy.ndarray\n",
        "        Learned weighted causal structure matrix.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] S. Shimizu, P. O. Hoyer, A. Hyvärinen, and A. J. Kerminen.\n",
        "       A linear non-gaussian acyclic model for causal discovery.\n",
        "       Journal of Machine Learning Research, 7:2003-2030, 2006.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from castle.algorithms import ICALiNGAM\n",
        "    >>> from castle.datasets import load_dataset\n",
        "    >>> from castle.common import GraphDAG\n",
        "    >>> from castle.metrics import MetricsDAG\n",
        "    >>> X, true_dag, _ = load_dataset(name='IID_Test')\n",
        "    >>> n = ICALiNGAM()\n",
        "    >>> n.learn(X)\n",
        "    >>> GraphDAG(n.causal_matrix, true_dag)\n",
        "    >>> met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "    >>> print(met.metrics)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, random_state=None, max_iter=1000, thresh=0.3):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self._random_state = random_state\n",
        "        self._max_iter = max_iter\n",
        "        self._thresh = thresh\n",
        "\n",
        "    def learn(self, data, columns=None):\n",
        "        \"\"\"\n",
        "        Set up and run the ICALiNGAM algorithm.\n",
        "\n",
        "        ----------\n",
        "        data: castle.Tensor or numpy.ndarray\n",
        "            The castle.Tensor or numpy.ndarray format data you want to learn.\n",
        "        columns : Index or array-like\n",
        "            Column labels to use for resulting tensor. Will default to\n",
        "            RangeIndex (0, 1, 2, ..., n) if no column labels are provided.\n",
        "        \"\"\"\n",
        "        X = Tensor(data, columns=columns)\n",
        "\n",
        "        self.fit(X)\n",
        "\n",
        "        weight_causal_matrix = self.adjacency_matrix_.T\n",
        "        self.weight_causal_matrix = Tensor(weight_causal_matrix,\n",
        "                                           index=X.columns, columns=X.columns)\n",
        "\n",
        "        causal_matrix = (abs(self.adjacency_matrix_) > self._thresh).astype(int).T\n",
        "        self.causal_matrix = Tensor(causal_matrix,\n",
        "                                    index=X.columns, columns=X.columns)\n",
        "\n",
        "    def fit(self, X):\n",
        "        \"\"\"\n",
        "        Fit the model to X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Training data, where ``n_samples`` is the number of samples\n",
        "            and ``n_features`` is the number of features.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "            Returns the instance of self.\n",
        "        \"\"\"\n",
        "        X = check_array(X)\n",
        "\n",
        "        # obtain a unmixing matrix from the given data\n",
        "        ica = FastICA(max_iter=self._max_iter, random_state=self._random_state)\n",
        "        ica.fit(X)\n",
        "        W_ica = ica.components_\n",
        "\n",
        "        # obtain a permuted W_ica\n",
        "        _, col_index = linear_sum_assignment(1 / np.abs(W_ica))\n",
        "        PW_ica = np.zeros_like(W_ica)\n",
        "        PW_ica[col_index] = W_ica\n",
        "\n",
        "        # obtain a vector to scale\n",
        "        D = np.diag(PW_ica)[:, np.newaxis]\n",
        "\n",
        "        # estimate an adjacency matrix\n",
        "        W_estimate = PW_ica / D\n",
        "        B_estimate = np.eye(len(W_estimate)) - W_estimate\n",
        "\n",
        "        causal_order = self._estimate_causal_order(B_estimate)\n",
        "        self._causal_order = causal_order\n",
        "\n",
        "        return self._estimate_adjacency_matrix(X)\n",
        "\n",
        "    def _search_causal_order(self, matrix):\n",
        "        \"\"\"\n",
        "        Obtain a causal order from the given matrix strictly.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        matrix : array-like, shape (n_features, n_samples)\n",
        "            Target matrix.\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        causal_order : array, shape [n_features, ]\n",
        "            A causal order of the given matrix on success, None otherwise.\n",
        "        \"\"\"\n",
        "        causal_order = []\n",
        "\n",
        "        row_num = matrix.shape[0]\n",
        "        original_index = np.arange(row_num)\n",
        "\n",
        "        while 0 < len(matrix):\n",
        "            # find a row all of which elements are zero\n",
        "            row_index_list = np.where(np.sum(np.abs(matrix), axis=1) == 0)[0]\n",
        "            if len(row_index_list) == 0:\n",
        "                break\n",
        "\n",
        "            target_index = row_index_list[0]\n",
        "\n",
        "            # append i to the end of the list\n",
        "            causal_order.append(original_index[target_index])\n",
        "            original_index = np.delete(original_index, target_index, axis=0)\n",
        "\n",
        "            # remove the i-th row and the i-th column from matrix\n",
        "            mask = np.delete(np.arange(len(matrix)), target_index, axis=0)\n",
        "            matrix = matrix[mask][:, mask]\n",
        "\n",
        "        if len(causal_order) != row_num:\n",
        "            causal_order = None\n",
        "\n",
        "        return causal_order\n",
        "\n",
        "    def _estimate_causal_order(self, matrix):\n",
        "        \"\"\"\n",
        "        Obtain a lower triangular from the given matrix approximately.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        matrix : array-like, shape (n_features, n_samples)\n",
        "            Target matrix.\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        causal_order : array, shape [n_features, ]\n",
        "            A causal order of the given matrix on success, None otherwise.\n",
        "        \"\"\"\n",
        "        causal_order = None\n",
        "\n",
        "        # set the m(m + 1)/2 smallest(in absolute value) elements of the matrix to zero\n",
        "        pos_list = np.argsort(np.abs(matrix), axis=None)\n",
        "        pos_list = np.vstack(np.unravel_index(pos_list, matrix.shape)).T\n",
        "        initial_zero_num = int(matrix.shape[0] * (matrix.shape[0] + 1) / 2)\n",
        "        for i, j in pos_list[:initial_zero_num]:\n",
        "            matrix[i, j] = 0\n",
        "\n",
        "        for i, j in pos_list[initial_zero_num:]:\n",
        "            # set the smallest(in absolute value) element to zero\n",
        "            matrix[i, j] = 0\n",
        "\n",
        "            causal_order = self._search_causal_order(matrix)\n",
        "            if causal_order is not None:\n",
        "                break\n",
        "\n",
        "        return causal_order\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24c3c3f0",
      "metadata": {
        "id": "24c3c3f0"
      },
      "source": [
        "# ICALiNGAM_Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a81c35f7",
      "metadata": {
        "id": "a81c35f7",
        "outputId": "eeb62459-1cb4-401d-b18a-18a91824af37"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-03-31 18:56:27,057 - C:\\Users\\notebook\\castle\\datasets\\simulator.py[line:270] - INFO: Finished synthetic dataset\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:116: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'fdr': 0.1429, 'tpr': 0.9, 'fpr': 0.12, 'shd': 3, 'nnz': 21, 'precision': 0.8571, 'recall': 0.9, 'F1': 0.878, 'gscore': 0.75}\n"
          ]
        }
      ],
      "source": [
        "from castle.algorithms import ICALiNGAM\n",
        "from castle.datasets import load_dataset\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "X, true_dag, _ = load_dataset(name='IID_Test')\n",
        "n = ICALiNGAM()\n",
        "n.learn(X)\n",
        "GraphDAG(n.causal_matrix, true_dag)\n",
        "met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "print(met.metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06366bfd",
      "metadata": {
        "id": "06366bfd"
      },
      "source": [
        "# GES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4360364d",
      "metadata": {
        "id": "4360364d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from castle.common import BaseLearner, Tensor\n",
        "from castle.algorithms.ges.operators import search\n",
        "from castle.algorithms.ges.score.local_scores import (BICScore, BDeuScore, DecomposableScore)\n",
        "\n",
        "\n",
        "class GES(BaseLearner):\n",
        "    \"\"\"\n",
        "    Greedy equivalence search for causal discovering\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    [1]: https://www.sciencedirect.com/science/article/pii/S0888613X12001636\n",
        "    [2]: https://www.jmlr.org/papers/volume3/chickering02b/chickering02b.pdf\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    criterion: str for DecomposableScore object\n",
        "        scoring criterion, one of ['bic', 'bdeu'].\n",
        "\n",
        "        Notes:\n",
        "            1. 'bdeu' just for discrete variable.\n",
        "            2. if you want to customize criterion, you must create a class\n",
        "            and inherit the base class `DecomposableScore` in module\n",
        "            `ges.score.local_scores`\n",
        "    method: str\n",
        "        effective when `criterion='bic'`, one of ['r2', 'scatter'].\n",
        "    k: float, default: 0.001\n",
        "        structure prior, effective when `criterion='bdeu'`.\n",
        "    N: int, default: 10\n",
        "        prior equivalent sample size, effective when `criterion='bdeu'`\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from castle.common import GraphDAG\n",
        "    >>> from castle.metrics import MetricsDAG\n",
        "    >>> from castle.datasets import load_dataset\n",
        "\n",
        "    >>> X, true_dag, _ = load_dataset(name='IID_Test')\n",
        "    >>> algo = GES()\n",
        "    >>> algo.learn(X)\n",
        "    >>> GraphDAG(algo.causal_matrix, true_dag, save_name='result_pc')\n",
        "    >>> met = MetricsDAG(algo.causal_matrix, true_dag)\n",
        "    >>> print(met.metrics)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, criterion='bic', method='scatter', k=0.001, N=10):\n",
        "        super(GES, self).__init__()\n",
        "        if isinstance(criterion, str):\n",
        "            if criterion not in ['bic', 'bdeu']:\n",
        "                raise ValueError(f\"if criterion is str, it must be one of \"\n",
        "                                 f\"['bic', 'bdeu'], but got {criterion}.\")\n",
        "        else:\n",
        "            if not isinstance(criterion, DecomposableScore):\n",
        "                raise TypeError(f\"The criterion is not instance of \"\n",
        "                                f\"DecomposableScore.\")\n",
        "        self.criterion = criterion\n",
        "        self.method = method\n",
        "        self.k = k\n",
        "        self.N = N\n",
        "\n",
        "    def learn(self, data, columns=None, **kwargs):\n",
        "\n",
        "        d = data.shape[1]\n",
        "        e = np.zeros((d, d), dtype=int)\n",
        "\n",
        "        if self.criterion == 'bic':\n",
        "            self.criterion = BICScore(data=data,\n",
        "                                      method=self.method)\n",
        "        elif self.criterion == 'bdeu':\n",
        "            self.criterion = BDeuScore(data=data, k=self.k, N=self.N)\n",
        "\n",
        "        c = search.fes(C=e, criterion=self.criterion)\n",
        "        c = search.bes(C=c, criterion=self.criterion)\n",
        "\n",
        "        self._causal_matrix = Tensor(c, index=columns, columns=columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9ae531e",
      "metadata": {
        "id": "f9ae531e"
      },
      "source": [
        "# GES_Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8799aa85",
      "metadata": {
        "id": "8799aa85",
        "outputId": "5310c0e6-906c-42c1-eae3-0d251ddb1eed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-03-31 19:01:55,766 - C:\\Users\\notebook\\castle\\datasets\\simulator.py[line:270] - INFO: Finished synthetic dataset\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'fdr': 0.4483, 'tpr': 0.8, 'fpr': 0.52, 'shd': 14, 'nnz': 29, 'precision': 0.4706, 'recall': 0.8, 'F1': 0.5926, 'gscore': 0.0}\n"
          ]
        }
      ],
      "source": [
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "from castle.datasets import load_dataset\n",
        "\n",
        "X, true_dag, _ = load_dataset(name='IID_Test')\n",
        "algo = GES()\n",
        "algo.learn(X)\n",
        "GraphDAG(algo.causal_matrix, true_dag, save_name='result_pc')\n",
        "met = MetricsDAG(algo.causal_matrix, true_dag)\n",
        "print(met.metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b910136c",
      "metadata": {
        "id": "b910136c"
      },
      "source": [
        "# PNL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "061ccd69",
      "metadata": {
        "id": "061ccd69"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# Copyright (C) 2021. Huawei Technologies Co., Ltd. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import os\n",
        "import itertools\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "from castle.common import BaseLearner, Tensor\n",
        "from castle.common.independence_tests import hsic_test\n",
        "\n",
        "from castle.algorithms.gradient.pnl.torch.utils import batch_loader, compute_jacobian, compute_entropy\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-layer perceptron\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_layers, hidden_units, output_dim,\n",
        "                 bias=True, activation=None, device=None) -> None:\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.hidden_units = hidden_units\n",
        "        self.output_dim = output_dim\n",
        "        self.bias = bias\n",
        "        self.activation = activation\n",
        "        self.device = device\n",
        "\n",
        "        mlp = []\n",
        "        for i in range(self.hidden_layers):\n",
        "            input_size = self.hidden_units\n",
        "            if i == 0:\n",
        "                input_size = self.input_dim\n",
        "            weight = nn.Linear(in_features=input_size,\n",
        "                               out_features=self.hidden_units,\n",
        "                               bias=self.bias,\n",
        "                               device=self.device)\n",
        "            mlp.append(weight)\n",
        "            if self.activation is not None:\n",
        "                mlp.append(self.activation)\n",
        "        out_layer = nn.Linear(in_features=self.hidden_units,\n",
        "                              out_features=self.output_dim,\n",
        "                              bias=self.bias,\n",
        "                              device=self.device)\n",
        "        mlp.append(out_layer)\n",
        "\n",
        "        self.mlp = nn.Sequential(*mlp)\n",
        "\n",
        "    def forward(self, x) -> torch.Tensor:\n",
        "\n",
        "        out = self.mlp(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class PNL(BaseLearner):\n",
        "    \"\"\"\n",
        "    On the Identifiability of the Post-Nonlinear Causal Model\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    https://arxiv.org/ftp/arxiv/papers/1205/1205.2599.pdf\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    hidden_layers: int\n",
        "        number of hidden layer of mlp\n",
        "    hidden_units: int\n",
        "        number of unit of per hidden layer\n",
        "    batch_size: int\n",
        "        size of training batch\n",
        "    epochs: int\n",
        "        training times on all samples\n",
        "    lr: float\n",
        "        learning rate\n",
        "    alpha: float\n",
        "        significance level\n",
        "    bias: bool\n",
        "        whether use bias\n",
        "    activation: callable\n",
        "        nonlinear activation function\n",
        "    device_type: str\n",
        "        'cpu' or 'gpu', default: 'cpu'\n",
        "    device_ids: int or str\n",
        "        e.g. 0 or '0,1', denotes which gpu that you want to use.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from castle.algorithms.gradient.pnl.torch import PNL\n",
        "    >>> from castle.datasets import load_dataset\n",
        "    >>> from castle.common import GraphDAG\n",
        "    >>> from castle.metrics import MetricsDAG\n",
        "    >>> X, true_dag, _ = load_dataset('IID_Test')\n",
        "    >>> n = PNL()\n",
        "    >>> n.learn(X)\n",
        "    >>> GraphDAG(n.causal_matrix, true_dag)\n",
        "    >>> met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "    >>> print(met.metrics)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_layers=1, hidden_units=10, batch_size=64,\n",
        "                 epochs=100, lr=1e-4, alpha=0.01, bias=True,\n",
        "                 activation=nn.LeakyReLU(), device_type='cpu', device_ids=None):\n",
        "        super(PNL, self).__init__()\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.hidden_units = hidden_units\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.lr = lr\n",
        "        self.alpha = alpha\n",
        "        self.bias = bias\n",
        "        self.activation = activation\n",
        "        self.device_type = device_type\n",
        "        self.device_ids = device_ids\n",
        "        if torch.cuda.is_available():\n",
        "            logging.info('GPU is available.')\n",
        "        else:\n",
        "            logging.info('GPU is unavailable.')\n",
        "            if self.device_type == 'gpu':\n",
        "                raise ValueError(\"GPU is unavailable, \"\n",
        "                                 \"please set device_type='cpu'.\")\n",
        "        if self.device_type == 'gpu':\n",
        "            if self.device_ids:\n",
        "                os.environ['CUDA_VISIBLE_DEVICES'] = str(self.device_ids)\n",
        "            device = torch.device('cuda')\n",
        "        else:\n",
        "            device = torch.device('cpu')\n",
        "        self.device = device\n",
        "\n",
        "    def learn(self, data, columns=None, **kwargs):\n",
        "\n",
        "        n_nodes = data.shape[1]\n",
        "        g = np.zeros((n_nodes, n_nodes))\n",
        "\n",
        "        all_nodes_pair = itertools.permutations(range(n_nodes), 2)\n",
        "        for i, j in all_nodes_pair:\n",
        "            x1 = torch.tensor(data[:, i], device=self.device).unsqueeze(-1)\n",
        "            x2 = torch.tensor(data[:, j], device=self.device).unsqueeze(-1)\n",
        "\n",
        "            # initialize model and parameters\n",
        "            l1 = MLP(input_dim=1, hidden_layers=self.hidden_layers,hidden_units=self.hidden_units, output_dim=1,bias=self.bias, activation=self.activation,device=self.device)\n",
        "            l2 = MLP(input_dim=1, hidden_layers=self.hidden_layers,\n",
        "                     hidden_units=self.hidden_units, output_dim=1,\n",
        "                     bias=self.bias, activation=self.activation,\n",
        "                     device=self.device)\n",
        "            optimizer = torch.optim.SGD([{'params': l1.parameters()},\n",
        "                                         {'params': l2.parameters()}],\n",
        "                                        lr=self.lr)\n",
        "            # nonlinear ICA\n",
        "            e2 = self._nonlinear_ica(l1, l2, x1, x2, optimizer=optimizer)\n",
        "\n",
        "            # kernel-based independent test\n",
        "            ind = hsic_test(x1.cpu().detach().numpy(),\n",
        "                            e2.cpu().detach().numpy(), alpha=self.alpha)\n",
        "            if ind == 0:  # x1->x2\n",
        "                g[i, j] = 1\n",
        "\n",
        "        self.causal_matrix = Tensor(g, index=columns, columns=columns)\n",
        "\n",
        "    def _nonlinear_ica(self, f1, f2, x1, x2, optimizer):\n",
        "\n",
        "        batch_generator = batch_loader(x1, x2, batch_size=self.batch_size)\n",
        "        for i in range(self.epochs):\n",
        "            for x1_batch, x2_batch in batch_generator:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                l2_jacob = torch.diag(compute_jacobian(f2, x2_batch).squeeze())\n",
        "                e2 = f2(x2_batch) - f1(x1_batch)\n",
        "                entropy = compute_entropy(e2)\n",
        "                loss = entropy - torch.log(torch.abs(l2_jacob)).sum()\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        e2 = f2(x2) - f1(x1)\n",
        "\n",
        "        return e2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1146f7f",
      "metadata": {
        "id": "a1146f7f"
      },
      "source": [
        "# PNL_Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d6f4976",
      "metadata": {
        "id": "1d6f4976",
        "outputId": "ed55f3a9-9188-42df-abdd-c7fba18d4a3e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-04 09:51:08,979 - C:\\Users\\notebook\\castle\\datasets\\simulator.py[line:270] - INFO: Finished synthetic dataset\n",
            "2023-04-04 09:51:08,981 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\pnl\\torch\\pnl.py[line:135] - INFO: GPU is unavailable.\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "__init__() got an unexpected keyword argument 'device'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11532\\2703199517.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_dag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'IID_Test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPNL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mGraphDAG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcausal_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_dag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMetricsDAG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcausal_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_dag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\pnl\\torch\\pnl.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, data, columns, **kwargs)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;31m# initialize model and parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             l1 = MLP(input_dim=1, hidden_layers=self.hidden_layers,\n\u001b[0m\u001b[0;32m    159\u001b[0m                      \u001b[0mhidden_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m                      \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\pnl\\torch\\pnl.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dim, hidden_layers, hidden_units, output_dim, bias, activation, device)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0minput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             weight = nn.Linear(in_features=input_size,\n\u001b[0m\u001b[0;32m     52\u001b[0m                                \u001b[0mout_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_units\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                                \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'device'"
          ]
        }
      ],
      "source": [
        "from castle.algorithms.gradient.pnl.torch import PNL\n",
        "from castle.datasets import load_dataset\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "X, true_dag, _ = load_dataset('IID_Test')\n",
        "n = PNL()\n",
        "n.learn(X)\n",
        "GraphDAG(n.causal_matrix, true_dag)\n",
        "met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "print(met.metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e9c4f6b",
      "metadata": {
        "id": "1e9c4f6b"
      },
      "outputs": [],
      "source": [
        "  def __init__(self, input_dim, hidden_layers, hidden_units, output_dim,\n",
        "                 bias=True, activation=None, device=None) -> None:\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.hidden_units = hidden_units\n",
        "        self.output_dim = output_dim\n",
        "        self.bias = bias\n",
        "        self.activation = activation\n",
        "        self.device = device\n",
        "\n",
        "        mlp = []\n",
        "        for i in range(self.hidden_layers):\n",
        "            input_size = self.hidden_units\n",
        "            if i == 0:\n",
        "                input_size = self.input_dim\n",
        "            weight = nn.Linear(in_features=input_size,\n",
        "                               out_features=self.hidden_units,\n",
        "                               bias=self.bias,\n",
        "                               device=self.device)\n",
        "            mlp.append(weight)\n",
        "            if self.activation is not None:\n",
        "                mlp.append(self.activation)\n",
        "        out_layer = nn.Linear(in_features=self.hidden_units,\n",
        "                              out_features=self.output_dim,\n",
        "                              bias=self.bias,\n",
        "                              device=self.device)\n",
        "        mlp.append(out_layer)\n",
        "\n",
        "        self.mlp = nn.Sequential(*mlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26334006",
      "metadata": {
        "id": "26334006"
      },
      "source": [
        "# NOTEARS_Linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f49638c7",
      "metadata": {
        "id": "f49638c7"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# 2021.03 modified (1) notears_linear(def) to Notears(class)\n",
        "# 2021.03 added    (1) logging;\n",
        "#                  (2) BaseLearner\n",
        "# 2021.03 deleted  (1) __main__\n",
        "# Huawei Technologies Co., Ltd.\n",
        "#\n",
        "# Copyright (C) 2021. Huawei Technologies Co., Ltd. All rights reserved.\n",
        "#\n",
        "# Copyright (c) Xun Zheng (https://github.com/xunzheng/notears)\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import logging\n",
        "import numpy as np\n",
        "import scipy.optimize as sopt\n",
        "from scipy.special import expit as sigmoid\n",
        "\n",
        "from castle.common import BaseLearner, Tensor\n",
        "from castle.common.consts import NOTEARS_VALID_PARAMS\n",
        "from castle.common.validator import check_args_value\n",
        "\n",
        "\n",
        "class Notears(BaseLearner):\n",
        "    \"\"\"\n",
        "    Notears Algorithm.\n",
        "    A gradient-based algorithm for linear data models (typically with least-squares loss).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lambda1: float\n",
        "        l1 penalty parameter\n",
        "    loss_type: str\n",
        "        l2, logistic, poisson\n",
        "    max_iter: int\n",
        "        max num of dual ascent steps\n",
        "    h_tol: float\n",
        "        exit if |h(w_est)| <= htol\n",
        "    rho_max: float\n",
        "        exit if rho >= rho_max\n",
        "    w_threshold: float\n",
        "        drop edge if |weight| < threshold\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    causal_matrix : numpy.ndarray\n",
        "        Learned causal structure matrix\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    https://arxiv.org/abs/1803.01422\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from castle.algorithms import Notears\n",
        "    >>> from castle.datasets import load_dataset\n",
        "    >>> from castle.common import GraphDAG\n",
        "    >>> from castle.metrics import MetricsDAG\n",
        "    >>> X, true_dag, _ = load_dataset('IID_Test')\n",
        "    >>> n = Notears()\n",
        "    >>> n.learn(X)\n",
        "    >>> GraphDAG(n.causal_matrix, true_dag)\n",
        "    >>> met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "    >>> print(met.metrics)\n",
        "    \"\"\"\n",
        "\n",
        "    @check_args_value(NOTEARS_VALID_PARAMS)\n",
        "    def __init__(self, lambda1=0.1,\n",
        "                 loss_type='l2',\n",
        "                 max_iter=100,\n",
        "                 h_tol=1e-8,\n",
        "                 rho_max=1e+16,\n",
        "                 w_threshold=0.3):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.lambda1 = lambda1\n",
        "        self.loss_type = loss_type\n",
        "        self.max_iter = max_iter\n",
        "        self.h_tol = h_tol\n",
        "        self.rho_max = rho_max\n",
        "        self.w_threshold = w_threshold\n",
        "\n",
        "    def learn(self, data, columns=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Set up and run the Notears algorithm.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: castle.Tensor or numpy.ndarray\n",
        "            The castle.Tensor or numpy.ndarray format data you want to learn.\n",
        "        columns : Index or array-like\n",
        "            Column labels to use for resulting tensor. Will default to\n",
        "            RangeIndex (0, 1, 2, ..., n) if no column labels are provided.\n",
        "        \"\"\"\n",
        "        X = Tensor(data, columns=columns)\n",
        "\n",
        "        W_est = self.notears_linear(X, lambda1=self.lambda1,\n",
        "                                    loss_type=self.loss_type,\n",
        "                                    max_iter=self.max_iter,\n",
        "                                    h_tol=self.h_tol,\n",
        "                                    rho_max=self.rho_max)\n",
        "        causal_matrix = (abs(W_est) > self.w_threshold).astype(int)\n",
        "        self.weight_causal_matrix = Tensor(W_est,\n",
        "                                           index=X.columns,\n",
        "                                           columns=X.columns)\n",
        "        self.causal_matrix = Tensor(causal_matrix, index=X.columns,\n",
        "                                    columns=X.columns)\n",
        "\n",
        "    def notears_linear(self, X, lambda1, loss_type, max_iter, h_tol,\n",
        "                       rho_max):\n",
        "        \"\"\"\n",
        "        Solve min_W L(W; X) + lambda1 ‖W‖_1 s.t. h(W) = 0 using\n",
        "        augmented Lagrangian.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: np.ndarray\n",
        "            n*d sample matrix\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        W_est: np.ndarray\n",
        "            d*d estimated DAG\n",
        "        \"\"\"\n",
        "        def _loss(W):\n",
        "            \"\"\"Evaluate value and gradient of loss.\"\"\"\n",
        "            M = X @ W\n",
        "            if loss_type == 'l2':\n",
        "                R = X - M\n",
        "                loss = 0.5 / X.shape[0] * (R ** 2).sum()\n",
        "                G_loss = - 1.0 / X.shape[0] * X.T @ R\n",
        "            elif loss_type == 'logistic':\n",
        "                loss = 1.0 / X.shape[0] * (np.logaddexp(0, M) - X * M).sum()\n",
        "                G_loss = 1.0 / X.shape[0] * X.T @ (sigmoid(M) - X)\n",
        "            elif loss_type == 'poisson':\n",
        "                S = np.exp(M)\n",
        "                loss = 1.0 / X.shape[0] * (S - X * M).sum()\n",
        "                G_loss = 1.0 / X.shape[0] * X.T @ (S - X)\n",
        "            else:\n",
        "                raise ValueError('unknown loss type')\n",
        "            return loss, G_loss\n",
        "\n",
        "        def _h(W):\n",
        "            \"\"\"\n",
        "            Evaluate value and gradient of acyclicity constraint.\n",
        "            \"\"\"\n",
        "            #     E = slin.expm(W * W)  # (Zheng et al. 2018)\n",
        "            #     h = np.trace(E) - d\n",
        "            M = np.eye(d) + W * W / d  # (Yu et al. 2019)\n",
        "            E = np.linalg.matrix_power(M, d - 1)\n",
        "            h = (E.T * M).sum() - d\n",
        "            G_h = E.T * W * 2\n",
        "            return h, G_h\n",
        "\n",
        "        def _adj(w):\n",
        "            \"\"\"\n",
        "            Convert doubled variables ([2 d^2] array) back to original\n",
        "            variables ([d, d] matrix).\n",
        "            \"\"\"\n",
        "            return (w[:d * d] - w[d * d:]).reshape([d, d])\n",
        "\n",
        "        def _func(w):\n",
        "            \"\"\"\n",
        "            Evaluate value and gradient of augmented Lagrangian for\n",
        "            doubled variables ([2 d^2] array).\n",
        "            \"\"\"\n",
        "            W = _adj(w)\n",
        "            loss, G_loss = _loss(W)\n",
        "            h, G_h = _h(W)\n",
        "            obj = loss + 0.5 * rho * h * h + alpha * h + lambda1 * w.sum()\n",
        "            G_smooth = G_loss + (rho * h + alpha) * G_h\n",
        "            g_obj = np.concatenate((G_smooth + lambda1, - G_smooth + lambda1),\n",
        "                                   axis=None)\n",
        "            return obj, g_obj\n",
        "\n",
        "        n, d = X.shape\n",
        "        # double w_est into (w_pos, w_neg)\n",
        "        w_est, rho, alpha, h = np.zeros(2 * d * d), 1.0, 0.0, np.inf\n",
        "        bnds = [(0, 0) if i == j else (0, None) for _ in range(2)\n",
        "                for i in range(d) for j in range(d)]\n",
        "        if loss_type == 'l2':\n",
        "            X = X - np.mean(X, axis=0, keepdims=True)\n",
        "\n",
        "        logging.info('[start]: n={}, d={}, iter_={}, h_={}, rho_={}'.format( \\\n",
        "                    n, d, max_iter, h_tol, rho_max))\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            w_new, h_new = None, None\n",
        "            while rho < rho_max:\n",
        "                sol = sopt.minimize(_func, w_est, method='L-BFGS-B',\n",
        "                                    jac=True, bounds=bnds)\n",
        "                w_new = sol.x\n",
        "                h_new, _ = _h(_adj(w_new))\n",
        "\n",
        "                logging.info(\n",
        "                    '[iter {}] h={:.3e}, loss={:.3f}, rho={:.1e}'.format( \\\n",
        "                    i, h_new, _func(w_est)[0], rho))\n",
        "\n",
        "                if h_new > 0.25 * h:\n",
        "                    rho *= 10\n",
        "                else:\n",
        "                    break\n",
        "            w_est, h = w_new, h_new\n",
        "            alpha += rho * h\n",
        "\n",
        "            if h <= h_tol or rho >= rho_max:\n",
        "                break\n",
        "\n",
        "        W_est = _adj(w_est)\n",
        "\n",
        "        logging.info('FINISHED')\n",
        "\n",
        "        return W_est\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3da9f736",
      "metadata": {
        "id": "3da9f736"
      },
      "source": [
        "# NOTEARS_Linear_Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d87af24f",
      "metadata": {
        "id": "d87af24f",
        "outputId": "5c06631c-1e5b-4941-92f7-f28d4588e14d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-03-31 19:08:15,152 - C:\\Users\\notebook\\castle\\datasets\\simulator.py[line:270] - INFO: Finished synthetic dataset\n",
            "2023-03-31 19:08:15,154 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:195] - INFO: [start]: n=2000, d=10, iter_=100, h_=1e-08, rho_=1e+16\n",
            "2023-03-31 19:08:15,238 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 0] h=7.021e-01, loss=171.611, rho=1.0e+00\n",
            "2023-03-31 19:08:15,451 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 1] h=4.557e-01, loss=4.686, rho=1.0e+00\n",
            "2023-03-31 19:08:15,673 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 1] h=2.163e-01, loss=6.904, rho=1.0e+01\n",
            "2023-03-31 19:08:15,939 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 1] h=7.563e-02, loss=29.087, rho=1.0e+02\n",
            "2023-03-31 19:08:16,253 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 2] h=4.484e-02, loss=6.143, rho=1.0e+02\n",
            "2023-03-31 19:08:16,610 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 2] h=1.807e-02, loss=8.718, rho=1.0e+03\n",
            "2023-03-31 19:08:16,815 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 3] h=1.122e-02, loss=6.662, rho=1.0e+03\n",
            "2023-03-31 19:08:17,122 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 3] h=4.650e-03, loss=8.130, rho=1.0e+04\n",
            "2023-03-31 19:08:17,521 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 3] h=1.464e-03, loss=22.818, rho=1.0e+05\n",
            "2023-03-31 19:08:17,792 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 4] h=8.556e-04, loss=7.269, rho=1.0e+05\n",
            "2023-03-31 19:08:18,433 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 4] h=3.207e-04, loss=8.234, rho=1.0e+06\n",
            "2023-03-31 19:08:18,777 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 5] h=1.932e-04, loss=7.433, rho=1.0e+06\n",
            "2023-03-31 19:08:19,915 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 5] h=7.360e-05, loss=7.896, rho=1.0e+07\n",
            "2023-03-31 19:08:20,675 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 6] h=4.550e-05, loss=7.521, rho=1.0e+07\n",
            "2023-03-31 19:08:21,815 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 6] h=1.688e-05, loss=7.764, rho=1.0e+08\n",
            "2023-03-31 19:08:22,428 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 7] h=9.627e-06, loss=7.569, rho=1.0e+08\n",
            "2023-03-31 19:08:23,235 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 7] h=3.497e-06, loss=7.697, rho=1.0e+09\n",
            "2023-03-31 19:08:23,567 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 8] h=2.044e-06, loss=7.587, rho=1.0e+09\n",
            "2023-03-31 19:08:24,010 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 8] h=7.598e-07, loss=7.642, rho=1.0e+10\n",
            "2023-03-31 19:08:24,060 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 9] h=4.462e-07, loss=7.596, rho=1.0e+10\n",
            "2023-03-31 19:08:24,115 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 9] h=1.762e-07, loss=7.622, rho=1.0e+11\n",
            "2023-03-31 19:08:24,827 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 10] h=8.328e-08, loss=7.601, rho=1.0e+11\n",
            "2023-03-31 19:08:24,887 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 10] h=3.872e-08, loss=7.615, rho=1.0e+12\n",
            "2023-03-31 19:08:24,930 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 11] h=2.202e-08, loss=7.604, rho=1.0e+12\n",
            "2023-03-31 19:08:24,993 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:206] - INFO: [iter 11] h=8.265e-09, loss=7.611, rho=1.0e+13\n",
            "2023-03-31 19:08:24,993 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\linear.py[line:222] - INFO: FINISHED\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'fdr': 0.1, 'tpr': 0.9, 'fpr': 0.08, 'shd': 4, 'nnz': 20, 'precision': 0.9, 'recall': 0.9, 'F1': 0.9, 'gscore': 0.8}\n"
          ]
        }
      ],
      "source": [
        "from castle.algorithms import Notears\n",
        "from castle.datasets import load_dataset\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "X, true_dag, _ = load_dataset('IID_Test')\n",
        "n = Notears()\n",
        "n.learn(X)\n",
        "GraphDAG(n.causal_matrix, true_dag)\n",
        "met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "print(met.metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d44cd245",
      "metadata": {
        "id": "d44cd245"
      },
      "source": [
        "# NOTEARS_Nonlinear_MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "086a6e48",
      "metadata": {
        "id": "086a6e48"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d67aaa4b",
      "metadata": {
        "id": "d67aaa4b"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# 2021.03 added    (1) logging;\n",
        "#                  (2) BaseLearner;\n",
        "#                  (3) NotearsMLP, NotearsSob;\n",
        "# 2021.03 deleted  (1) __main__\n",
        "# Huawei Technologies Co., Ltd.\n",
        "#\n",
        "# Copyright (C) 2021. Huawei Technologies Co., Ltd. All rights reserved.\n",
        "#\n",
        "# Copyright (c) Xun Zheng (https://github.com/xunzheng/notears)\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from castle.algorithms.gradient.notears.torch.utils.locally_connected import LocallyConnected\n",
        "\n",
        "torch.set_default_dtype(torch.double)\n",
        "\n",
        "\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, dims, bias=True, device=None):\n",
        "        \"\"\"\n",
        "        Multilayer perceptron.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dims: tuple\n",
        "            Network shape parameters\n",
        "        bias:\n",
        "            Indicates whether to use weight deviation.\n",
        "        device: option, default: None\n",
        "            torch.device('cpu') or torch.device('cuda')\n",
        "        \"\"\"\n",
        "        super(MLPModel, self).__init__()\n",
        "        if len(dims) < 2:\n",
        "            raise ValueError(f\"The size of dims at least greater equal to 2, contains one \"\n",
        "                             f\"one hidden layer and one output_layer\")\n",
        "        if dims[-1] != 1:\n",
        "            raise ValueError(f\"The dimension of output layer must be 1, but got {dims[-1]}.\")\n",
        "        d = dims[0]\n",
        "        self.dims = dims\n",
        "        self.device = device\n",
        "        # fc1: variable splitting for l1\n",
        "        self.fc1_pos = nn.Linear(d, d * dims[1], bias=bias, device=self.device)\n",
        "        self.fc1_neg = nn.Linear(d, d * dims[1], bias=bias, device=self.device)\n",
        "        self.fc1_pos.weight.bounds = self._bounds()\n",
        "        self.fc1_neg.weight.bounds = self._bounds()\n",
        "        # fc2: local linear layers\n",
        "        layers = []\n",
        "        for l in range(len(dims) - 2):\n",
        "            layers.append(LocallyConnected(d, dims[l + 1], dims[l + 2], bias=bias))\n",
        "        self.fc2 = nn.ModuleList(layers).to(device=self.device)\n",
        "\n",
        "\n",
        "    def _bounds(self):\n",
        "        d = self.dims[0]\n",
        "        bounds = []\n",
        "        for j in range(d):\n",
        "            for m in range(self.dims[1]):\n",
        "                for i in range(d):\n",
        "                    if i == j:\n",
        "                        bound = (0, 0)\n",
        "                    else:\n",
        "                        bound = (0, None)\n",
        "                    bounds.append(bound)\n",
        "        return bounds\n",
        "\n",
        "    def forward(self, x):  # [n, d] -> [n, d]\n",
        "        x = self.fc1_pos(x) - self.fc1_neg(x)  # [n, d * m1]\n",
        "        x = x.view(-1, self.dims[0], self.dims[1])  # [n, d, m1]\n",
        "        for fc in self.fc2:\n",
        "            x = torch.sigmoid(x)  # [n, d, m1]\n",
        "            x = fc(x)  # [n, d, m2]\n",
        "        x = x.squeeze(dim=2)  # [n, d]\n",
        "        return x\n",
        "\n",
        "    def h_func(self):\n",
        "        \"\"\"\n",
        "        Constrain 2-norm-squared of fc1 weights along m1 dim to be a DAG.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "\n",
        "        \"\"\"\n",
        "        d = self.dims[0]\n",
        "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j * m1, i]\n",
        "        fc1_weight = fc1_weight.view(d, -1, d)  # [j, m1, i]\n",
        "        A = torch.sum(fc1_weight * fc1_weight, dim=1).t()  # [i, j]\n",
        "        # h = trace_expm(A) - d  # (Zheng et al. 2018)\n",
        "        init_e = torch.eye(d).to(self.device)\n",
        "        M = init_e + A / d  # (Yu et al. 2019)\n",
        "        E = torch.matrix_power(M, d - 1)\n",
        "        h = (E.t() * M).sum() - d\n",
        "        return h\n",
        "\n",
        "    def l2_reg(self):\n",
        "        \"\"\"\n",
        "        Take 2-norm-squared of all parameters.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "\n",
        "        \"\"\"\n",
        "        reg = 0.\n",
        "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j * m1, i]\n",
        "        reg += torch.sum(fc1_weight ** 2)\n",
        "        for fc in self.fc2:\n",
        "            reg += torch.sum(fc.weight ** 2)\n",
        "        return reg\n",
        "\n",
        "    def fc1_l1_reg(self):\n",
        "        \"\"\"\n",
        "        Take l1 norm of fc1 weight.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "\n",
        "        \"\"\"\n",
        "        reg = torch.sum(self.fc1_pos.weight + self.fc1_neg.weight)\n",
        "        return reg\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def fc1_to_adj(self) -> np.ndarray:  # [j * m1, i] -> [i, j]\n",
        "        \"\"\"\n",
        "        Get W from fc1 weights, take 2-norm over m1 dim.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "\n",
        "        \"\"\"\n",
        "        d = self.dims[0]\n",
        "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j * m1, i]\n",
        "        fc1_weight = fc1_weight.view(d, -1, d)  # [j, m1, i]\n",
        "        A = torch.sum(fc1_weight * fc1_weight, dim=1).t()  # [i, j]\n",
        "        W = torch.sqrt(A)  # [i, j]\n",
        "        W = W.cpu().detach().numpy()  # [i, j]\n",
        "        return W\n",
        "\n",
        "\n",
        "class SobolevModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Sobolev network.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    d: int\n",
        "        Num variables.\n",
        "    k: int\n",
        "        Num expansion of each variable.\n",
        "    bias:\n",
        "        Indicates whether to use weight deviation.\n",
        "    device: option, default: None\n",
        "        torch.device('cpu') or torch.device('cuda')\n",
        "    \"\"\"\n",
        "    def __init__(self, d, k, bias=False, device=None):\n",
        "        super(SobolevModel, self).__init__()\n",
        "        self.d, self.k = d, k\n",
        "        self.l2_reg_store = None\n",
        "        self.device = device\n",
        "        self.fc1_pos = nn.Linear(d * k, d, bias=bias, device=self.device)  # ik -> j\n",
        "        self.fc1_neg = nn.Linear(d * k, d, bias=bias, device=self.device)\n",
        "        self.fc1_pos.weight.bounds = self._bounds()\n",
        "        self.fc1_neg.weight.bounds = self._bounds()\n",
        "        nn.init.zeros_(self.fc1_pos.weight)\n",
        "        nn.init.zeros_(self.fc1_neg.weight)\n",
        "\n",
        "    def _bounds(self):\n",
        "        # weight shape [j, ik]\n",
        "        bounds = []\n",
        "        for j in range(self.d):\n",
        "            for i in range(self.d):\n",
        "                for _ in range(self.k):\n",
        "                    if i == j:\n",
        "                        bound = (0, 0)\n",
        "                    else:\n",
        "                        bound = (0, None)\n",
        "                    bounds.append(bound)\n",
        "        return bounds\n",
        "\n",
        "    def sobolev_basis(self, x):  # [n, d] -> [n, dk]\n",
        "        seq = []\n",
        "        for kk in range(self.k):\n",
        "            mu = 2.0 / (2 * kk + 1) / math.pi  # sobolev basis\n",
        "            psi = mu * torch.sin(x / mu)\n",
        "            seq.append(psi)  # [n, d] * k\n",
        "        bases = torch.stack(seq, dim=2)  # [n, d, k]\n",
        "        bases = bases.view(-1, self.d * self.k)  # [n, dk]\n",
        "        return bases\n",
        "\n",
        "    def forward(self, x):  # [n, d] -> [n, d]\n",
        "        bases = self.sobolev_basis(x)  # [n, dk]\n",
        "        x = self.fc1_pos(bases) - self.fc1_neg(bases)  # [n, d]\n",
        "        self.l2_reg_store = torch.sum(x ** 2) / x.shape[0]\n",
        "        return x\n",
        "\n",
        "    def h_func(self):\n",
        "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j, ik]\n",
        "        fc1_weight = fc1_weight.view(self.d, self.d, self.k)  # [j, i, k]\n",
        "        A = torch.sum(fc1_weight * fc1_weight, dim=2).t()  # [i, j]\n",
        "        # h = trace_expm(A) - d  # (Zheng et al. 2018)\n",
        "        init_e = torch.eye(self.d).to(self.device)\n",
        "        M = init_e + A / self.d  # (Yu et al. 2019)\n",
        "        E = torch.matrix_power(M, self.d - 1)\n",
        "        h = (E.t() * M).sum() - self.d\n",
        "        return h\n",
        "\n",
        "    def l2_reg(self):\n",
        "        reg = self.l2_reg_store\n",
        "        return reg\n",
        "\n",
        "    def fc1_l1_reg(self):\n",
        "        reg = torch.sum(self.fc1_pos.weight + self.fc1_neg.weight)\n",
        "        return reg\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def fc1_to_adj(self) -> np.ndarray:\n",
        "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j, ik]\n",
        "        fc1_weight = fc1_weight.view(self.d, self.d, self.k)  # [j, i, k]\n",
        "        A = torch.sum(fc1_weight * fc1_weight, dim=2).t()  # [i, j]\n",
        "        W = torch.sqrt(A)  # [i, j]\n",
        "        W = W.cpu().detach().numpy()  # [i, j]\n",
        "        return W\n",
        "\n",
        "\n",
        "def squared_loss(output, target):\n",
        "    \"\"\"\n",
        "    Least squares loss function.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    output: torch.tenser\n",
        "        network output\n",
        "    target: torch.tenser\n",
        "        raw input\n",
        "    Returns\n",
        "    -------\n",
        "    : torch.tenser\n",
        "        loss value\n",
        "    \"\"\"\n",
        "    n = target.shape[0]\n",
        "    loss = 0.5 / n * torch.sum((output - target) ** 2)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc68593a",
      "metadata": {
        "id": "cc68593a"
      },
      "source": [
        "# NOTEARS_Nonlinear_SOB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cab9217e",
      "metadata": {
        "id": "cab9217e"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# 2021.03 added    (1) logging;\n",
        "#                  (2) BaseLearner;\n",
        "#                  (3) NotearsMLP, NotearsSob;\n",
        "# 2021.03 deleted  (1) __main__\n",
        "# Huawei Technologies Co., Ltd.\n",
        "#\n",
        "# Copyright (C) 2021. Huawei Technologies Co., Ltd. All rights reserved.\n",
        "#\n",
        "# Copyright (c) Xun Zheng (https://github.com/xunzheng/notears)\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from castle.algorithms.gradient.notears.torch.utils.locally_connected import LocallyConnected\n",
        "\n",
        "torch.set_default_dtype(torch.double)\n",
        "\n",
        "\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, dims, bias=True, device=None):\n",
        "        \"\"\"\n",
        "        Multilayer perceptron.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dims: tuple\n",
        "            Network shape parameters\n",
        "        bias:\n",
        "            Indicates whether to use weight deviation.\n",
        "        device: option, default: None\n",
        "            torch.device('cpu') or torch.device('cuda')\n",
        "        \"\"\"\n",
        "        super(MLPModel, self).__init__()\n",
        "        if len(dims) < 2:\n",
        "            raise ValueError(f\"The size of dims at least greater equal to 2, contains one \"\n",
        "                             f\"one hidden layer and one output_layer\")\n",
        "        if dims[-1] != 1:\n",
        "            raise ValueError(f\"The dimension of output layer must be 1, but got {dims[-1]}.\")\n",
        "        d = dims[0]\n",
        "        self.dims = dims\n",
        "        self.device = device\n",
        "        # fc1: variable splitting for l1\n",
        "        self.fc1_pos = nn.Linear(d, d * dims[1], bias=bias, device=self.device)\n",
        "        self.fc1_neg = nn.Linear(d, d * dims[1], bias=bias, device=self.device)\n",
        "        self.fc1_pos.weight.bounds = self._bounds()\n",
        "        self.fc1_neg.weight.bounds = self._bounds()\n",
        "        # fc2: local linear layers\n",
        "        layers = []\n",
        "        for l in range(len(dims) - 2):\n",
        "            layers.append(LocallyConnected(d, dims[l + 1], dims[l + 2], bias=bias))\n",
        "        self.fc2 = nn.ModuleList(layers).to(device=self.device)\n",
        "\n",
        "\n",
        "    def _bounds(self):\n",
        "        d = self.dims[0]\n",
        "        bounds = []\n",
        "        for j in range(d):\n",
        "            for m in range(self.dims[1]):\n",
        "                for i in range(d):\n",
        "                    if i == j:\n",
        "                        bound = (0, 0)\n",
        "                    else:\n",
        "                        bound = (0, None)\n",
        "                    bounds.append(bound)\n",
        "        return bounds\n",
        "\n",
        "    def forward(self, x):  # [n, d] -> [n, d]\n",
        "        x = self.fc1_pos(x) - self.fc1_neg(x)  # [n, d * m1]\n",
        "        x = x.view(-1, self.dims[0], self.dims[1])  # [n, d, m1]\n",
        "        for fc in self.fc2:\n",
        "            x = torch.sigmoid(x)  # [n, d, m1]\n",
        "            x = fc(x)  # [n, d, m2]\n",
        "        x = x.squeeze(dim=2)  # [n, d]\n",
        "        return x\n",
        "\n",
        "    def h_func(self):\n",
        "        \"\"\"\n",
        "        Constrain 2-norm-squared of fc1 weights along m1 dim to be a DAG.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "\n",
        "        \"\"\"\n",
        "        d = self.dims[0]\n",
        "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j * m1, i]\n",
        "        fc1_weight = fc1_weight.view(d, -1, d)  # [j, m1, i]\n",
        "        A = torch.sum(fc1_weight * fc1_weight, dim=1).t()  # [i, j]\n",
        "        # h = trace_expm(A) - d  # (Zheng et al. 2018)\n",
        "        init_e = torch.eye(d).to(self.device)\n",
        "        M = init_e + A / d  # (Yu et al. 2019)\n",
        "        E = torch.matrix_power(M, d - 1)\n",
        "        h = (E.t() * M).sum() - d\n",
        "        return h\n",
        "\n",
        "    def l2_reg(self):\n",
        "        \"\"\"\n",
        "        Take 2-norm-squared of all parameters.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "\n",
        "        \"\"\"\n",
        "        reg = 0.\n",
        "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j * m1, i]\n",
        "        reg += torch.sum(fc1_weight ** 2)\n",
        "        for fc in self.fc2:\n",
        "            reg += torch.sum(fc.weight ** 2)\n",
        "        return reg\n",
        "\n",
        "    def fc1_l1_reg(self):\n",
        "        \"\"\"\n",
        "        Take l1 norm of fc1 weight.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "\n",
        "        \"\"\"\n",
        "        reg = torch.sum(self.fc1_pos.weight + self.fc1_neg.weight)\n",
        "        return reg\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def fc1_to_adj(self) -> np.ndarray:  # [j * m1, i] -> [i, j]\n",
        "        \"\"\"\n",
        "        Get W from fc1 weights, take 2-norm over m1 dim.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "\n",
        "        \"\"\"\n",
        "        d = self.dims[0]\n",
        "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j * m1, i]\n",
        "        fc1_weight = fc1_weight.view(d, -1, d)  # [j, m1, i]\n",
        "        A = torch.sum(fc1_weight * fc1_weight, dim=1).t()  # [i, j]\n",
        "        W = torch.sqrt(A)  # [i, j]\n",
        "        W = W.cpu().detach().numpy()  # [i, j]\n",
        "        return W\n",
        "\n",
        "\n",
        "class SobolevModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Sobolev network.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    d: int\n",
        "        Num variables.\n",
        "    k: int\n",
        "        Num expansion of each variable.\n",
        "    bias:\n",
        "        Indicates whether to use weight deviation.\n",
        "    device: option, default: None\n",
        "        torch.device('cpu') or torch.device('cuda')\n",
        "    \"\"\"\n",
        "    def __init__(self, d, k, bias=False, device=None):\n",
        "        super(SobolevModel, self).__init__()\n",
        "        self.d, self.k = d, k\n",
        "        self.l2_reg_store = None\n",
        "        self.device = device\n",
        "        self.fc1_pos = nn.Linear(d * k, d, bias=bias, device=self.device)  # ik -> j\n",
        "        self.fc1_neg = nn.Linear(d * k, d, bias=bias, device=self.device)\n",
        "        self.fc1_pos.weight.bounds = self._bounds()\n",
        "        self.fc1_neg.weight.bounds = self._bounds()\n",
        "        nn.init.zeros_(self.fc1_pos.weight)\n",
        "        nn.init.zeros_(self.fc1_neg.weight)\n",
        "\n",
        "    def _bounds(self):\n",
        "        # weight shape [j, ik]\n",
        "        bounds = []\n",
        "        for j in range(self.d):\n",
        "            for i in range(self.d):\n",
        "                for _ in range(self.k):\n",
        "                    if i == j:\n",
        "                        bound = (0, 0)\n",
        "                    else:\n",
        "                        bound = (0, None)\n",
        "                    bounds.append(bound)\n",
        "        return bounds\n",
        "\n",
        "    def sobolev_basis(self, x):  # [n, d] -> [n, dk]\n",
        "        seq = []\n",
        "        for kk in range(self.k):\n",
        "            mu = 2.0 / (2 * kk + 1) / math.pi  # sobolev basis\n",
        "            psi = mu * torch.sin(x / mu)\n",
        "            seq.append(psi)  # [n, d] * k\n",
        "        bases = torch.stack(seq, dim=2)  # [n, d, k]\n",
        "        bases = bases.view(-1, self.d * self.k)  # [n, dk]\n",
        "        return bases\n",
        "\n",
        "    def forward(self, x):  # [n, d] -> [n, d]\n",
        "        bases = self.sobolev_basis(x)  # [n, dk]\n",
        "        x = self.fc1_pos(bases) - self.fc1_neg(bases)  # [n, d]\n",
        "        self.l2_reg_store = torch.sum(x ** 2) / x.shape[0]\n",
        "        return x\n",
        "\n",
        "    def h_func(self):\n",
        "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j, ik]\n",
        "        fc1_weight = fc1_weight.view(self.d, self.d, self.k)  # [j, i, k]\n",
        "        A = torch.sum(fc1_weight * fc1_weight, dim=2).t()  # [i, j]\n",
        "        # h = trace_expm(A) - d  # (Zheng et al. 2018)\n",
        "        init_e = torch.eye(self.d).to(self.device)\n",
        "        M = init_e + A / self.d  # (Yu et al. 2019)\n",
        "        E = torch.matrix_power(M, self.d - 1)\n",
        "        h = (E.t() * M).sum() - self.d\n",
        "        return h\n",
        "\n",
        "    def l2_reg(self):\n",
        "        reg = self.l2_reg_store\n",
        "        return reg\n",
        "\n",
        "    def fc1_l1_reg(self):\n",
        "        reg = torch.sum(self.fc1_pos.weight + self.fc1_neg.weight)\n",
        "        return reg\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def fc1_to_adj(self) -> np.ndarray:\n",
        "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j, ik]\n",
        "        fc1_weight = fc1_weight.view(self.d, self.d, self.k)  # [j, i, k]\n",
        "        A = torch.sum(fc1_weight * fc1_weight, dim=2).t()  # [i, j]\n",
        "        W = torch.sqrt(A)  # [i, j]\n",
        "        W = W.cpu().detach().numpy()  # [i, j]\n",
        "        return W\n",
        "\n",
        "\n",
        "def squared_loss(output, target):\n",
        "    \"\"\"\n",
        "    Least squares loss function.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    output: torch.tenser\n",
        "        network output\n",
        "    target: torch.tenser\n",
        "        raw input\n",
        "    Returns\n",
        "    -------\n",
        "    : torch.tenser\n",
        "        loss value\n",
        "    \"\"\"\n",
        "    n = target.shape[0]\n",
        "    loss = 0.5 / n * torch.sum((output - target) ** 2)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de84b235",
      "metadata": {
        "id": "de84b235"
      },
      "source": [
        "# NOTEARS_Nonlinear_MLP&SOB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95874fcb",
      "metadata": {
        "id": "95874fcb"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# 2021.03 added    (1) logging;\n",
        "#                  (2) BaseLearner;\n",
        "#                  (3) NotearsMLP, NotearsSob;\n",
        "# 2021.03 deleted  (1) __main__\n",
        "# 2021.11 added    (1) NotearsNonlinear\n",
        "#         deleted  (1) NotearsMLP, NotearsSob, MLPModel, SobolevModel\n",
        "# Huawei Technologies Co., Ltd.\n",
        "#\n",
        "# Copyright (C) 2021. Huawei Technologies Co., Ltd. All rights reserved.\n",
        "#\n",
        "# Copyright (c) Xun Zheng (https://github.com/xunzheng/notears)\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "from castle.common import BaseLearner, Tensor\n",
        "from castle.algorithms.gradient.notears.torch.models import MLPModel, SobolevModel, squared_loss\n",
        "from castle.algorithms.gradient.notears.torch.utils.lbfgsb_scipy import LBFGSBScipy\n",
        "from castle.common.consts import NONLINEAR_NOTEARS_VALID_PARAMS\n",
        "from castle.common.validator import check_args_value\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "\n",
        "class NotearsNonlinear(BaseLearner):\n",
        "    \"\"\"\n",
        "    Notears Nonlinear.\n",
        "    include notears-mlp and notears-sob.\n",
        "    A gradient-based algorithm using neural network or Sobolev space modeling for non-linear causal relationships.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lambda1: float\n",
        "        l1 penalty parameter\n",
        "    lambda2: float\n",
        "        l2 penalty parameter\n",
        "    max_iter: int\n",
        "        max num of dual ascent steps\n",
        "    h_tol: float\n",
        "        exit if |h(w_est)| <= htol\n",
        "    rho_max: float\n",
        "        exit if rho >= rho_max\n",
        "    w_threshold: float\n",
        "        drop edge if |weight| < threshold\n",
        "    hidden_layers: Iterrable\n",
        "        Dimension of per hidden layer, and the last element must be 1 as output dimension.\n",
        "        At least contains 2 elements. For example: hidden_layers=(5, 10, 1), denotes two hidden\n",
        "        layer has 5 and 10 dimension and output layer has 1 dimension.\n",
        "        It is effective when model_type='mlp'.\n",
        "    expansions: int\n",
        "        expansions of each variable, it is effective when model_type='sob'.\n",
        "    bias: bool\n",
        "        Indicates whether to use weight deviation.\n",
        "    model_type: str\n",
        "        The Choice of Two Nonlinear Network Models in a Notears Framework:\n",
        "        Multilayer perceptrons value is 'mlp', Basis expansions value is 'sob'.\n",
        "    device_type: str, default: cpu\n",
        "        ``cpu`` or ``gpu``\n",
        "    device_ids: int or str, default None\n",
        "        CUDA devices, it's effective when ``use_gpu`` is True.\n",
        "        For single-device modules, ``device_ids`` can be int or str, e.g. 0 or '0',\n",
        "        For multi-device modules, ``device_ids`` must be str, format like '0, 1'.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    causal_matrix : numpy.ndarray\n",
        "        Learned causal structure matrix\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    https://arxiv.org/abs/1909.13189\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from castle.algorithms import NotearsNonlinear\n",
        "    >>> from castle.datasets import load_dataset\n",
        "    >>> from castle.common import GraphDAG\n",
        "    >>> from castle.metrics import MetricsDAG\n",
        "    >>> X, true_dag, _ = load_dataset('IID_Test')\n",
        "    >>> n = NotearsNonlinear()\n",
        "    >>> n.learn(X)\n",
        "    >>> GraphDAG(n.causal_matrix, true_dag)\n",
        "    >>> met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "    >>> print(met.metrics)\n",
        "    \"\"\"\n",
        "\n",
        "    @check_args_value(NONLINEAR_NOTEARS_VALID_PARAMS)\n",
        "    def __init__(self, lambda1: float = 0.01,\n",
        "                 lambda2: float = 0.01,\n",
        "                 max_iter: int = 100,\n",
        "                 h_tol: float = 1e-8,\n",
        "                 rho_max: float = 1e+16,\n",
        "                 w_threshold: float = 0.3,\n",
        "                 hidden_layers: tuple = (10, 1),\n",
        "                 expansions: int = 10,\n",
        "                 bias: bool = True,\n",
        "                 model_type: str = \"mlp\",\n",
        "                 device_type: str = \"cpu\",\n",
        "                 device_ids=None):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.lambda1 = lambda1\n",
        "        self.lambda2 = lambda2\n",
        "        self.max_iter = max_iter\n",
        "        self.h_tol = h_tol\n",
        "        self.rho_max = rho_max\n",
        "        self.w_threshold = w_threshold\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.expansions = expansions\n",
        "        self.bias = bias\n",
        "        self.model_type = model_type\n",
        "        self.device_type = device_type\n",
        "        self.device_ids = device_ids\n",
        "        self.rho, self.alpha, self.h = 1.0, 0.0, np.inf\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            logging.info('GPU is available.')\n",
        "        else:\n",
        "            logging.info('GPU is unavailable.')\n",
        "            if self.device_type == 'gpu':\n",
        "                raise ValueError(\"GPU is unavailable, \"\n",
        "                                 \"please set device_type = 'cpu'.\")\n",
        "        if self.device_type == 'gpu':\n",
        "            if self.device_ids:\n",
        "                os.environ['CUDA_VISIBLE_DEVICES'] = str(self.device_ids)\n",
        "            device = torch.device('cuda')\n",
        "        else:\n",
        "            device = torch.device('cpu')\n",
        "        self.device = device\n",
        "\n",
        "    def learn(self, data, columns=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Set up and run the NotearsNonlinear algorithm.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: castle.Tensor or numpy.ndarray\n",
        "            The castle.Tensor or numpy.ndarray format data you want to learn.\n",
        "        columns : Index or array-like\n",
        "            Column labels to use for resulting tensor. Will default to\n",
        "            RangeIndex (0, 1, 2, ..., n) if no column labels are provided.\n",
        "        \"\"\"\n",
        "        X = Tensor(data, columns=columns)\n",
        "\n",
        "        input_dim = X.shape[1]\n",
        "        model = self.get_model(input_dim)\n",
        "        if model:\n",
        "            W_est = self.notears_nonlinear(model, X)\n",
        "\n",
        "            causal_matrix = (abs(W_est) > self.w_threshold).astype(int)\n",
        "            self.weight_causal_matrix = Tensor(W_est,\n",
        "                                               index=X.columns,\n",
        "                                               columns=X.columns)\n",
        "            self.causal_matrix = Tensor(causal_matrix, index=X.columns, columns=X.columns)\n",
        "\n",
        "    def dual_ascent_step(self, model, X):\n",
        "        \"\"\"\n",
        "        Perform one step of dual ascent in augmented Lagrangian.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        model: nn.Module\n",
        "            network model\n",
        "        X: torch.tenser\n",
        "            sample data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        :tuple\n",
        "            cycle control parameter\n",
        "        \"\"\"\n",
        "        h_new = None\n",
        "        optimizer = LBFGSBScipy(model.parameters())\n",
        "        X_torch = torch.from_numpy(X)\n",
        "        while self.rho < self.rho_max:\n",
        "            X_torch = X_torch.to(self.device)\n",
        "\n",
        "            def closure():\n",
        "                optimizer.zero_grad()\n",
        "                X_hat = model(X_torch)\n",
        "                loss = squared_loss(X_hat, X_torch)\n",
        "                h_val = model.h_func()\n",
        "                penalty = 0.5 * self.rho * h_val * h_val + self.alpha * h_val\n",
        "                l2_reg = 0.5 * self.lambda2 * model.l2_reg()\n",
        "                l1_reg = self.lambda1 * model.fc1_l1_reg()\n",
        "                primal_obj = loss + penalty + l2_reg + l1_reg\n",
        "                primal_obj.backward()\n",
        "                return primal_obj\n",
        "\n",
        "            optimizer.step(closure, self.device)  # NOTE: updates model in-place\n",
        "            with torch.no_grad():\n",
        "                model = model.to(self.device)\n",
        "                h_new = model.h_func().item()\n",
        "            if h_new > 0.25 * self.h:\n",
        "                self.rho *= 10\n",
        "            else:\n",
        "                break\n",
        "        self.alpha += self.rho * h_new\n",
        "        self.h = h_new\n",
        "\n",
        "    def notears_nonlinear(self,\n",
        "                          model: nn.Module,\n",
        "                          X: np.ndarray):\n",
        "        \"\"\"\n",
        "        notaears frame entrance.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        model: nn.Module\n",
        "            network model\n",
        "        X: castle.Tensor or numpy.ndarray\n",
        "            sample data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        :tuple\n",
        "            Prediction Graph Matrix Coefficients.\n",
        "        \"\"\"\n",
        "        logging.info('[start]: n={}, d={}, iter_={}, h_={}, rho_={}'.format(\n",
        "            X.shape[0], X.shape[1], self.max_iter, self.h_tol, self.rho_max))\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            self.dual_ascent_step(model, X)\n",
        "\n",
        "            logging.debug('[iter {}] h={:.3e}, rho={:.1e}'.format(_, self.h, self.rho))\n",
        "\n",
        "            if self.h <= self.h_tol or self.rho >= self.rho_max:\n",
        "                break\n",
        "        W_est = model.fc1_to_adj()\n",
        "\n",
        "        logging.info('FINISHED')\n",
        "\n",
        "        return W_est\n",
        "\n",
        "    def get_model(self, input_dim):\n",
        "        \"\"\"\n",
        "            Choose a different model.\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_dim: int\n",
        "            Enter the number of data dimensions.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "\n",
        "        \"\"\"\n",
        "        if self.model_type == \"mlp\":\n",
        "            model = MLPModel(dims=[input_dim, *self.hidden_layers],\n",
        "                             bias=self.bias, device=self.device)\n",
        "            return model\n",
        "        elif self.model_type == \"sob\":\n",
        "            model = SobolevModel(input_dim, k=self.expansions, bias=self.bias,\n",
        "                                 device=self.device)\n",
        "            return model\n",
        "        else:\n",
        "            logging.info(f'Unsupported model type {self.model_type}.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52690be3",
      "metadata": {
        "id": "52690be3"
      },
      "source": [
        "# NOTEARS_Nonlinear_MLP&SOB_Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "477b4a85",
      "metadata": {
        "id": "477b4a85",
        "outputId": "35f07713-89f4-4aaf-db5e-bc96c0338747"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-03 13:35:45,872 - C:\\Users\\notebook\\castle\\datasets\\simulator.py[line:270] - INFO: Finished synthetic dataset\n",
            "2023-04-03 13:35:45,873 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\nonlinear.py[line:137] - INFO: GPU is unavailable.\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "__init__() got an unexpected keyword argument 'device'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13436\\1611205089.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_dag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'IID_Test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNotearsNonlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mGraphDAG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcausal_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_dag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMetricsDAG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcausal_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_dag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\notears\\torch\\nonlinear.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, data, columns, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[0minput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mW_est\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotears_nonlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\notears\\torch\\nonlinear.py\u001b[0m in \u001b[0;36mget_model\u001b[1;34m(self, input_dim)\u001b[0m\n\u001b[0;32m    264\u001b[0m         \"\"\"\n\u001b[0;32m    265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"mlp\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m             model = MLPModel(dims=[input_dim, *self.hidden_layers],\n\u001b[0m\u001b[0;32m    267\u001b[0m                              bias=self.bias, device=self.device)\n\u001b[0;32m    268\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\notears\\torch\\models.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dims, bias, device)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;31m# fc1: variable splitting for l1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1_neg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1_pos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbounds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bounds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'device'"
          ]
        }
      ],
      "source": [
        "from castle.algorithms import NotearsNonlinear\n",
        "from castle.datasets import load_dataset\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "X, true_dag, _ = load_dataset('IID_Test')\n",
        "n = NotearsNonlinear()\n",
        "n.learn(X)\n",
        "GraphDAG(n.causal_matrix, true_dag)\n",
        "met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "print(met.metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb715906",
      "metadata": {
        "id": "eb715906"
      },
      "source": [
        "# NOTEARS-lOW-RANK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9920441",
      "metadata": {
        "id": "b9920441"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# Copyright (C) 2021. Huawei Technologies Co., Ltd. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import logging\n",
        "import numpy as np\n",
        "import scipy.linalg as slin\n",
        "import scipy.optimize as sopt\n",
        "\n",
        "from castle.common import BaseLearner, Tensor\n",
        "\n",
        "\n",
        "class NotearsLowRank(BaseLearner):\n",
        "    \"\"\"\n",
        "    NotearsLowRank Algorithm.\n",
        "    Adapting NOTEARS for large problems with low-rank causal graphs.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    w_init: None or numpy.ndarray\n",
        "        Initialized weight matrix\n",
        "    max_iter: int\n",
        "        Maximum number of iterations\n",
        "    h_tol: float\n",
        "        exit if |h(w)| <= h_tol\n",
        "    rho_max: float\n",
        "        maximum for rho\n",
        "    w_threshold : float,  default='0.3'\n",
        "        Drop edge if |weight| < threshold\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    causal_matrix : numpy.ndarray\n",
        "        Learned causal structure matrix\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    https://arxiv.org/abs/2006.05691\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> import numpy as np\n",
        "    >>> from castle.algorithms import NotearsLowRank\n",
        "    >>> from castle.datasets import load_dataset\n",
        "    >>> from castle.common import GraphDAG\n",
        "    >>> from castle.metrics import MetricsDAG\n",
        "    >>> X, true_dag, _ = load_dataset('IID_Test')\n",
        "    >>> rank = np.linalg.matrix_rank(true_dag)\n",
        "    >>> n = NotearsLowRank()\n",
        "    >>> n.learn(X, rank=rank)\n",
        "    >>> GraphDAG(n.causal_matrix, true_dag)\n",
        "    >>> met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "    >>> print(met.metrics)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, w_init=None, max_iter=15, h_tol=1e-6,\n",
        "                 rho_max=1e+20, w_threshold=0.3):\n",
        "\n",
        "        super(NotearsLowRank, self).__init__()\n",
        "\n",
        "        self.w_init = w_init\n",
        "        self.max_iter = max_iter\n",
        "        self.h_tol = h_tol\n",
        "        self.rho_max = rho_max\n",
        "        self.w_threshold = w_threshold\n",
        "\n",
        "    def learn(self, data, rank, columns=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Set up and run the NotearsLowRank algorithm.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: castle.Tensor or numpy.ndarray\n",
        "            The castle.Tensor or numpy.ndarray format data you want to learn.\n",
        "        columns : Index or array-like\n",
        "            Column labels to use for resulting tensor. Will default to\n",
        "            RangeIndex (0, 1, 2, ..., n) if no column labels are provided.\n",
        "        rank: int\n",
        "            The algebraic rank of the weighted adjacency matrix of a graph.\n",
        "        \"\"\"\n",
        "        X = Tensor(data, columns=columns)\n",
        "\n",
        "        n, d = X.shape\n",
        "        random_cnt = 0\n",
        "        total_cnt = 0\n",
        "        while total_cnt <= 20:\n",
        "            try:\n",
        "                if total_cnt == 0:\n",
        "                    w_init_ = np.zeros((d,d))\n",
        "                else:\n",
        "                    w_init_ = np.random.uniform(-0.3, 0.3, (d,d))\n",
        "\n",
        "                w_est2 = self.notears_low_rank(X, rank, w_init_)\n",
        "                causal_matrix = (abs(w_est2) > self.w_threshold).astype(int)\n",
        "\n",
        "                random_cnt += 1\n",
        "                total_cnt += 1\n",
        "                if random_cnt >= 1:\n",
        "                    break\n",
        "\n",
        "            except ValueError:\n",
        "                print(total_cnt, 'NAN error')\n",
        "                total_cnt += 1\n",
        "\n",
        "        self.weight_causal_matrix = Tensor(w_est2,\n",
        "                                           index=X.columns,\n",
        "                                           columns=X.columns)\n",
        "        self.causal_matrix = Tensor(causal_matrix, index=X.columns,\n",
        "                                    columns=X.columns)\n",
        "\n",
        "    def notears_low_rank(self, X, rank, w_init=None):\n",
        "        \"\"\"\n",
        "        Solve min_W ell(W; X) s.t. h(W) = 0 using augmented Lagrangian.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: [n,d] sample matrix\n",
        "            max_iter: max number of dual ascent steps.\n",
        "        rank: int\n",
        "            The rank of data.\n",
        "        w_init: None or numpy.ndarray\n",
        "            Initialized weight matrix\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        W_est: np.ndarray\n",
        "            estimate [d,d] dag matrix\n",
        "        \"\"\"\n",
        "        def _h(W):\n",
        "            return np.trace(slin.expm(W * W)) - d\n",
        "\n",
        "        def _func(uv):\n",
        "            # L = 0.5/n * || X (I - UV) ||_F^2 + rho/2*h^2 + alpha*h\n",
        "            nn = len(uv)\n",
        "            u = uv[0: nn // 2]\n",
        "            u = u.reshape((d, -1))\n",
        "            v = uv[nn // 2:]\n",
        "            v = v.reshape((d, -1))\n",
        "            W = np.matmul(u, v.transpose())\n",
        "            loss = 0.5 / n * np.square(np.linalg.norm(X.dot(np.eye(d, d) - W), 'fro'))\n",
        "            h = _h(W)\n",
        "            return loss + 0.5 * rho * h * h + alpha * h\n",
        "\n",
        "        def _grad(uv):\n",
        "            nn = len(uv)\n",
        "            u = uv[0: nn // 2]\n",
        "            v = uv[nn // 2:]\n",
        "            gd = np.zeros(nn)\n",
        "            gd[0: nn // 2] = _grad_u(u, v)\n",
        "            gd[nn // 2:] = _grad_v(v, u)\n",
        "            return gd\n",
        "\n",
        "        def _grad_u(u, v):\n",
        "            # -2⋅X⊤⋅(X−X⋅U⋅V⊤)⋅V\n",
        "            # ( expm(t2) .* 2(u*v') ) * v, t2 = vu' .* vu'\n",
        "            u = u.reshape((d, -1))\n",
        "            v = v.reshape((d, -1))\n",
        "            W = np.matmul(u, v.transpose())\n",
        "            loss_grad = - 1.0 / n * X.T.dot(X).dot(np.eye(d, d) - W).dot(v)\n",
        "            E = slin.expm(W * W)  # expm(t2)'\n",
        "            obj_grad = loss_grad + (rho * (np.trace(E) - d) + alpha) * 2 * \\\n",
        "                       np.matmul(E.T * W, v)\n",
        "            return obj_grad.flatten()\n",
        "\n",
        "        def _grad_v(v, u):\n",
        "            # −2⋅(X⊤−V⋅U⊤⋅X⊤)⋅X⋅U\n",
        "            # ( expm(t1) .* 2(v*u') ) * u, t1 = uv' .* uv'\n",
        "            u = u.reshape((d, -1))\n",
        "            v = v.reshape((d, -1))\n",
        "            W = np.matmul(v, u.transpose())\n",
        "            loss_grad = - 1.0 / n * (np.eye(d, d) - W).dot(X.T).dot(X).dot(u)\n",
        "            E = slin.expm(W * W)  # expm(t1)'\n",
        "            obj_grad = loss_grad + (rho * (np.trace(E) - d) + alpha) * 2 * \\\n",
        "                       np.matmul(E.T * W, u)\n",
        "            return obj_grad.flatten()\n",
        "\n",
        "        n, d = X.shape\n",
        "        r = rank\n",
        "        if w_init is None:\n",
        "            w_init = np.zeros((d,d))\n",
        "\n",
        "        u, s, vt = np.linalg.svd(w_init)\n",
        "        u_new = u[:, range(r)].dot(np.diag(s[range(r)])).reshape(d*r)\n",
        "        v_new = vt[range(r), :].transpose().reshape(d*r)\n",
        "\n",
        "        if np.sum(np.abs(u_new)) <= 1e-6 and np.sum(np.abs(v_new)) <= 1e-6:\n",
        "            raise ValueError('nearly zero gradient; input new initialized W')\n",
        "\n",
        "        rho, alpha, h, h_new = 1.0, 0.0, np.inf, np.inf\n",
        "        uv_new = np.hstack((u_new, v_new))\n",
        "        uv_est = np.copy(uv_new)\n",
        "        # bnds = [(0, 0) if i == j else (None, None) for i in range(d) for j in range(d)]\n",
        "\n",
        "        logging.info('[start]: n={}, d={}, iter_={}, h_={}, rho_={}'.format(\n",
        "                    n, d, self.max_iter, self.h_tol, self.rho_max))\n",
        "\n",
        "        for flag in range(-1, self.max_iter):\n",
        "            if flag >= 0:\n",
        "                while rho <= self.rho_max:\n",
        "                    sol = sopt.minimize(_func, uv_est, method='TNC',\n",
        "                                        jac=_grad, options={'disp': False})\n",
        "\n",
        "                    uv_new = sol.x\n",
        "                    h_new =_h(np.matmul(uv_new[0: d*r].reshape((d, r)),\n",
        "                                        uv_new[d*r:].reshape((d, r)).transpose()))\n",
        "\n",
        "                    logging.debug(\n",
        "                        '[iter {}] h={:.3e}, loss={:.3f}, rho={:.1e}'.format(\n",
        "                        flag, h_new, _func(uv_new), rho))\n",
        "\n",
        "                    if h_new > 0.25 * h:\n",
        "                        rho *= 10\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "            uv_est, h = uv_new, h_new\n",
        "\n",
        "            #############################\n",
        "            if flag >= 0:\n",
        "                alpha += rho * h\n",
        "\n",
        "            if flag >= 3 and h <= self.h_tol:\n",
        "                break\n",
        "\n",
        "        uv_new2 = np.copy(uv_new)\n",
        "        w_est2 = np.matmul(uv_new2[0: d*r].reshape((d, r)),\n",
        "                           uv_new2[d*r:].reshape((d, r)).transpose())\n",
        "        w_est2 = w_est2.reshape((d, d))\n",
        "\n",
        "        logging.info('FINISHED')\n",
        "\n",
        "        return w_est2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcee51d0",
      "metadata": {
        "id": "dcee51d0"
      },
      "source": [
        "# NOTEARS-lOW-RANK_Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb7e18d9",
      "metadata": {
        "id": "bb7e18d9",
        "outputId": "2ddea557-4131-4ee8-9859-9d26df215934"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-03-31 19:13:36,581 - C:\\Users\\notebook\\castle\\datasets\\simulator.py[line:270] - INFO: Finished synthetic dataset\n",
            "2023-03-31 19:13:36,582 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\low_rank.py[line:205] - INFO: [start]: n=2000, d=10, iter_=15, h_=1e-06, rho_=1e+20\n",
            "C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\low_rank.py:153: RuntimeWarning: overflow encountered in double_scalars\n",
            "  return loss + 0.5 * rho * h * h + alpha * h\n",
            "C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\low_rank.py:172: RuntimeWarning: overflow encountered in multiply\n",
            "  obj_grad = loss_grad + (rho * (np.trace(E) - d) + alpha) * 2 * \\\n",
            "C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\low_rank.py:184: RuntimeWarning: overflow encountered in multiply\n",
            "  obj_grad = loss_grad + (rho * (np.trace(E) - d) + alpha) * 2 * \\\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\_matfuncs.py:375: RuntimeWarning: overflow encountered in matmul\n",
            "  eAw = eAw @ eAw\n",
            "C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\low_rank.py:173: RuntimeWarning: invalid value encountered in matmul\n",
            "  np.matmul(E.T * W, v)\n",
            "C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\low_rank.py:185: RuntimeWarning: invalid value encountered in matmul\n",
            "  np.matmul(E.T * W, u)\n",
            "2023-03-31 19:13:44,257 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\low_rank.py[line:241] - INFO: FINISHED\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'fdr': 0.0, 'tpr': 1.0, 'fpr': 0.0, 'shd': 0, 'nnz': 20, 'precision': 1.0, 'recall': 1.0, 'F1': 1.0, 'gscore': 1.0}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from castle.algorithms import NotearsLowRank\n",
        "from castle.datasets import load_dataset\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "X, true_dag, _ = load_dataset('IID_Test')\n",
        "rank = np.linalg.matrix_rank(true_dag)\n",
        "n = NotearsLowRank()\n",
        "n.learn(X, rank=rank)\n",
        "GraphDAG(n.causal_matrix, true_dag)\n",
        "met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "print(met.metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aba1fa67",
      "metadata": {
        "id": "aba1fa67"
      },
      "source": [
        "# DAG-GNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b0c030b",
      "metadata": {
        "id": "7b0c030b"
      },
      "outputs": [],
      "source": [
        "# coding = utf-8\n",
        "# Copyright (C) 2022. Huawei Technologies Co., Ltd. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from castle.common import BaseLearner, Tensor\n",
        "from castle.common import consts\n",
        "from castle.common.validator import check_args_value\n",
        "\n",
        "from castle.algorithms.gradient.dag_gnn.torch.utils import functions as func\n",
        "from castle.algorithms.gradient.dag_gnn.torch.models.modules import Encoder, Decoder\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Referred from:\n",
        "    - https://stackoverflow.com/questions/38469632/tensorflow-non-repeatable-results\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    try:\n",
        "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "class DAG_GNN(BaseLearner):\n",
        "    \"\"\"DAG Structure Learning with Graph Neural Networks\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    https://arxiv.org/pdf/1904.10098.pdf\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    encoder_type: str, default: 'mlp'\n",
        "        choose an encoder, 'mlp' or 'sem'.\n",
        "    decoder_type: str, detault: 'mlp'\n",
        "        choose a decoder, 'mlp' or 'sem'.\n",
        "    encoder_hidden: int, default: 64\n",
        "        MLP encoder hidden layer dimension, just one hidden layer.\n",
        "    latent_dim: int, default equal to input dimension\n",
        "        encoder output dimension\n",
        "    decoder_hidden: int, default: 64\n",
        "        MLP decoder hidden layer dimension, just one hidden layer.\n",
        "    encoder_dropout: float, default: 0.0\n",
        "        Dropout rate (1 - keep probability).\n",
        "    decoder_dropout: float, default: 0.0\n",
        "        Dropout rate (1 - keep probability).\n",
        "    epochs: int, default: 300\n",
        "        train epochs\n",
        "    k_max_iter: int, default: 1e2\n",
        "        the max iteration number for searching lambda and c.\n",
        "    batch_size: int, default: 100\n",
        "        Sample size of each training batch\n",
        "    lr: float, default: 3e-3\n",
        "        learning rate\n",
        "    lr_decay: int, default: 200\n",
        "        Period of learning rate decay.\n",
        "    gamma: float, default: 1.0\n",
        "        Multiplicative factor of learning rate decay.\n",
        "    lambda_a: float, default: 0.0\n",
        "        coefficient for DAG constraint h(A).\n",
        "    c_a: float, default: 1.0\n",
        "        coefficient for absolute value h(A).\n",
        "    c_a_thresh: float, default: 1e20\n",
        "        control loop by c_a\n",
        "    eta: int, default: 10\n",
        "        use for update c_a, greater equal than 1.\n",
        "    multiply_h: float, default: 0.25\n",
        "        use for judge whether update c_a.\n",
        "    tau_a: float, default: 0.0\n",
        "        coefficient for L-1 norm of A.\n",
        "    h_tolerance: float, default: 1e-8\n",
        "        the tolerance of error of h(A) to zero.\n",
        "    use_a_connect_loss: bool, default: False\n",
        "        flag to use A connect loss\n",
        "    use_a_positiver_loss: bool, default: False\n",
        "        flag to enforce A must have positive values\n",
        "    graph_threshold: float, default: 0.3\n",
        "        threshold for learned adjacency matrix binarization.\n",
        "        greater equal to graph_threshold denotes has causal relationship.\n",
        "    optimizer: str, default: 'Adam'\n",
        "        choose optimizer, 'Adam' or 'SGD'\n",
        "    seed: int, default: 42\n",
        "        random seed\n",
        "    device_type: str, default: cpu\n",
        "        ``cpu`` or ``gpu``\n",
        "    device_ids: int or str, default None\n",
        "        CUDA devices, it's effective when ``use_gpu`` is True.\n",
        "        For single-device modules, ``device_ids`` can be int or str, e.g. 0 or '0',\n",
        "        For multi-device modules, ``device_ids`` must be str, format like '0, 1'.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from castle.algorithms.gradient.dag_gnn.torch import DAG_GNN\n",
        "    >>> from castle.datasets import load_dataset\n",
        "    >>> from castle.common import GraphDAG\n",
        "    >>> from castle.metrics import MetricsDAG\n",
        "    >>> X, true_dag, _ = load_dataset('IID_Test')\n",
        "    >>> m = DAG_GNN()\n",
        "    >>> m.learn(X)\n",
        "    >>> GraphDAG(m.causal_matrix, true_dag)\n",
        "    >>> met = MetricsDAG(m.causal_matrix, true_dag)\n",
        "    >>> print(met.metrics)\n",
        "    \"\"\"\n",
        "\n",
        "    @check_args_value(consts.GNN_VALID_PARAMS)\n",
        "    def __init__(self, encoder_type='mlp', decoder_type='mlp',\n",
        "                 encoder_hidden=64, latent_dim=None, decoder_hidden=64,\n",
        "                 encoder_dropout=0.0, decoder_dropout=0.0, epochs=300, k_max_iter=1e2, tau_a=0.0,\n",
        "                 batch_size=100, lr=3e-3, lr_decay=200, gamma=1.0, init_lambda_a=0.0, init_c_a=1.0,\n",
        "                 c_a_thresh=1e20, eta=10, multiply_h=0.25, h_tolerance=1e-8,\n",
        "                 use_a_connect_loss=False, use_a_positiver_loss=False, graph_threshold=0.3,\n",
        "                 optimizer='adam', seed=42, device_type='cpu', device_ids='0'):\n",
        "        super(DAG_GNN, self).__init__()\n",
        "        self.encoder_type = encoder_type\n",
        "        self.decoder_type = decoder_type\n",
        "        self.encoder_hidden = encoder_hidden\n",
        "        self.latent_dim = latent_dim\n",
        "        self.decoder_hidden = decoder_hidden\n",
        "        self.encoder_dropout = encoder_dropout\n",
        "        self.decoder_dropout = decoder_dropout\n",
        "        self.epochs = epochs\n",
        "        self.k_max_iter = int(k_max_iter)\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.lr_decay = lr_decay\n",
        "        self.gamma = gamma\n",
        "        self.init_lambda_a = init_lambda_a\n",
        "        self.init_c_a = init_c_a\n",
        "        self.c_a_thresh = c_a_thresh\n",
        "        self.eta = eta\n",
        "        self.multiply_h = multiply_h\n",
        "        self.tau_a = tau_a\n",
        "        self.h_tolerance = h_tolerance\n",
        "        self.use_a_connect_loss = use_a_connect_loss\n",
        "        self.use_a_positiver_loss = use_a_positiver_loss\n",
        "        self.graph_threshold = graph_threshold\n",
        "        self.optimizer = optimizer\n",
        "        self.seed = seed\n",
        "        self.device_type = device_type\n",
        "        self.device_ids = device_ids\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            logging.info('GPU is available.')\n",
        "        else:\n",
        "            logging.info('GPU is unavailable.')\n",
        "            if self.device_type == 'gpu':\n",
        "                raise ValueError(\"GPU is unavailable, \"\n",
        "                                 \"please set device_type = 'cpu'.\")\n",
        "        if self.device_type == 'gpu':\n",
        "            if self.device_ids:\n",
        "                os.environ['CUDA_VISIBLE_DEVICES'] = str(self.device_ids)\n",
        "            device = torch.device('cuda')\n",
        "        else:\n",
        "            device = torch.device('cpu')\n",
        "        self.device = device\n",
        "\n",
        "        self.input_dim = None\n",
        "\n",
        "    def learn(self, data, columns=None, **kwargs):\n",
        "\n",
        "        set_seed(self.seed)\n",
        "\n",
        "        if data.ndim == 2:\n",
        "            data = np.expand_dims(data, axis=2)\n",
        "        self.n_samples, self.n_nodes, self.input_dim = data.shape\n",
        "\n",
        "        if self.latent_dim is None:\n",
        "            self.latent_dim = self.input_dim\n",
        "        train_loader = func.get_dataloader(data, batch_size=self.batch_size, device=self.device)\n",
        "\n",
        "        # =====initialize encoder and decoder=====\n",
        "        adj_A = torch.zeros((self.n_nodes, self.n_nodes), requires_grad=True, device=self.device)\n",
        "        self.encoder = Encoder(input_dim=self.input_dim,\n",
        "                               hidden_dim=self.encoder_hidden,\n",
        "                               output_dim=self.latent_dim,\n",
        "                               adj_A=adj_A,\n",
        "                               device=self.device,\n",
        "                               encoder_type=self.encoder_type.lower()\n",
        "                               )\n",
        "        self.decoder = Decoder(input_dim=self.latent_dim,\n",
        "                               hidden_dim=self.decoder_hidden,\n",
        "                               output_dim=self.input_dim,\n",
        "                               device=self.device,\n",
        "                               decoder_type=self.decoder_type.lower()\n",
        "                               )\n",
        "        # =====initialize optimizer=====\n",
        "        if self.optimizer.lower() == 'adam':\n",
        "            optimizer = optim.Adam([{'params': self.encoder.parameters()},\n",
        "                                    {'params': self.decoder.parameters()}],\n",
        "                                   lr=self.lr)\n",
        "        elif self.optimizer.lower() == 'sgd':\n",
        "            optimizer = optim.SGD([{'params': self.encoder.parameters()},\n",
        "                                   {'params': self.decoder.parameters()}],\n",
        "                                  lr=self.lr)\n",
        "        else:\n",
        "            raise\n",
        "        self.scheduler = lr_scheduler.StepLR(optimizer, step_size=self.lr_decay, gamma=self.gamma)\n",
        "\n",
        "        ################################\n",
        "        # main training\n",
        "        ################################\n",
        "        c_a = self.init_c_a\n",
        "        lambda_a = self.init_lambda_a\n",
        "        h_a_new = torch.tensor(1.0)\n",
        "        h_a_old = np.inf\n",
        "        elbo_loss = np.inf\n",
        "        best_elbo_loss = np.inf\n",
        "        origin_a = adj_A\n",
        "        epoch = 0\n",
        "        for step_k in range(self.k_max_iter):\n",
        "            while c_a < self.c_a_thresh:\n",
        "                for epoch in range(self.epochs):\n",
        "                    elbo_loss, origin_a = self._train(train_loader=train_loader,\n",
        "                                                      optimizer=optimizer,\n",
        "                                                      lambda_a=lambda_a,\n",
        "                                                      c_a=c_a)\n",
        "                    if elbo_loss < best_elbo_loss:\n",
        "                        best_elbo_loss = elbo_loss\n",
        "                if elbo_loss > 2 * best_elbo_loss:\n",
        "                    break\n",
        "                # update parameters\n",
        "                a_new = origin_a.detach().clone()\n",
        "                h_a_new = func._h_A(a_new, self.n_nodes)\n",
        "                if h_a_new.item() > self.multiply_h * h_a_old:\n",
        "                    c_a *= self.eta  # eta\n",
        "                else:\n",
        "                    break\n",
        "            # update parameters\n",
        "            # h_A, adj_A are computed in loss anyway, so no need to store\n",
        "            h_a_old = h_a_new.item()\n",
        "            logging.info(f\"Iter: {step_k}, epoch: {epoch}, h_new: {h_a_old}\")\n",
        "            lambda_a += c_a * h_a_new.item()\n",
        "            if h_a_old <= self.h_tolerance:\n",
        "                break\n",
        "\n",
        "        origin_a = origin_a.detach().cpu().numpy()\n",
        "        origin_a[np.abs(origin_a) < self.graph_threshold] = 0\n",
        "        origin_a[np.abs(origin_a) >= self.graph_threshold] = 1\n",
        "\n",
        "        self.causal_matrix = Tensor(origin_a, index=columns, columns=columns)\n",
        "\n",
        "    def _train(self, train_loader, optimizer, lambda_a, c_a):\n",
        "\n",
        "        self.encoder.train()\n",
        "        self.decoder.train()\n",
        "\n",
        "        # update optimizer\n",
        "        optimizer, lr = func.update_optimizer(optimizer, self.lr, c_a)\n",
        "\n",
        "        nll_train = []\n",
        "        kl_train = []\n",
        "        origin_a = None\n",
        "        for batch_idx, (data, relations) in enumerate(train_loader):\n",
        "            x = Variable(data).double()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits, origin_a = self.encoder(x)\n",
        "            z_gap = self.encoder.z\n",
        "            z_positive = self.encoder.z_positive\n",
        "            wa = self.encoder.wa\n",
        "\n",
        "            x_pred = self.decoder(logits, adj_A=origin_a, wa=wa)    # X_hat\n",
        "\n",
        "            # reconstruction accuracy loss\n",
        "            loss_nll = func.nll_gaussian(x_pred, x)\n",
        "\n",
        "            # KL loss\n",
        "            loss_kl = func.kl_gaussian_sem(logits)\n",
        "\n",
        "            # ELBO loss:\n",
        "            loss = loss_kl + loss_nll\n",
        "\n",
        "            # add A loss\n",
        "            one_adj_a = origin_a  # torch.mean(adj_A_tilt_decoder, dim =0)\n",
        "            sparse_loss = self.tau_a * torch.sum(torch.abs(one_adj_a))\n",
        "\n",
        "            # other loss term\n",
        "            if self.use_a_connect_loss:\n",
        "                connect_gap = func.a_connect_loss(one_adj_a, self.graph_threshold, z_gap)\n",
        "                loss += lambda_a * connect_gap + 0.5 * c_a * connect_gap * connect_gap\n",
        "\n",
        "            if self.use_a_positiver_loss:\n",
        "                positive_gap = func.a_positive_loss(one_adj_a, z_positive)\n",
        "                loss += .1 * (lambda_a * positive_gap\n",
        "                              + 0.5 * c_a * positive_gap * positive_gap)\n",
        "            # compute h(A)\n",
        "            h_A = func._h_A(origin_a, self.n_nodes)\n",
        "            loss += (lambda_a * h_A\n",
        "                     + 0.5 * c_a * h_A * h_A\n",
        "                     + 100. * torch.trace(origin_a * origin_a)\n",
        "                     + sparse_loss)  # +  0.01 * torch.sum(variance * variance)\n",
        "            if np.isnan(loss.detach().cpu().numpy()):\n",
        "                raise ValueError(f\"The loss value is Nan, \"\n",
        "                                 f\"suggest to set optimizer='adam' to solve it. \"\n",
        "                                 f\"If you already set, please check your code whether has other problems.\")\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            self.scheduler.step()\n",
        "\n",
        "            nll_train.append(loss_nll.item())\n",
        "            kl_train.append(loss_kl.item())\n",
        "\n",
        "        return (np.mean(np.mean(kl_train) + np.mean(nll_train)), origin_a)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca9bdf5f",
      "metadata": {
        "id": "ca9bdf5f"
      },
      "source": [
        "# DAG-GNN_Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a63caa17",
      "metadata": {
        "id": "a63caa17",
        "outputId": "09d231e6-c4b6-4916-9330-524f0993a8a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-03-31 19:13:54,057 - C:\\Users\\notebook\\castle\\datasets\\simulator.py[line:270] - INFO: Finished synthetic dataset\n",
            "2023-03-31 19:13:54,058 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\dag_gnn\\torch\\dag_gnn.py[line:167] - INFO: GPU is unavailable.\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "__init__() got an unexpected keyword argument 'device'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10824\\2305085140.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_dag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'IID_Test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDAG_GNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mGraphDAG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcausal_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_dag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMetricsDAG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcausal_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_dag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\dag_gnn\\torch\\dag_gnn.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, data, columns, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[1;31m# =====initialize encoder and decoder=====\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0madj_A\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_nodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         self.encoder = Encoder(input_dim=self.input_dim,\n\u001b[0m\u001b[0;32m    196\u001b[0m                                \u001b[0mhidden_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder_hidden\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m                                \u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\dag_gnn\\torch\\models\\modules.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dim, hidden_dim, output_dim, adj_A, device, encoder_type)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'mlp'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;31m# =====initialize encoder hyper-parameters=====\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\dag_gnn\\torch\\models\\modules.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dim, hidden_dim, output_dim, device)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         self.w1 = nn.Linear(in_features=self.input_dim,\n\u001b[0m\u001b[0;32m     29\u001b[0m                             \u001b[0mout_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                             \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'device'"
          ]
        }
      ],
      "source": [
        "from castle.algorithms.gradient.dag_gnn.torch import DAG_GNN\n",
        "from castle.datasets import load_dataset\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "X, true_dag, _ = load_dataset('IID_Test')\n",
        "m = DAG_GNN()\n",
        "m.learn(X)\n",
        "GraphDAG(m.causal_matrix, true_dag)\n",
        "met = MetricsDAG(m.causal_matrix, true_dag)\n",
        "print(met.metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "927fc601",
      "metadata": {
        "id": "927fc601"
      },
      "source": [
        "# GOLEM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16dfd5c4",
      "metadata": {
        "id": "16dfd5c4"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# 2021.03 modified (1) golem(def) to GOLEM(class)\n",
        "#                  (2) replace tensorflow with pytorch\n",
        "# 2021.03 added    (1) get_args, set_seed;\n",
        "#                  (2) BaseLearner\n",
        "# 2021.03 deleted  (1) __main__\n",
        "# Huawei Technologies Co., Ltd.\n",
        "#\n",
        "# Copyright (C) 2021. Huawei Technologies Co., Ltd. All rights reserved.\n",
        "#\n",
        "# Copyright (c) Ignavier Ng (https://github.com/ignavier/golem)\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import logging\n",
        "import torch\n",
        "import os\n",
        "\n",
        "from castle.algorithms.gradient.notears.torch.golem_utils import GolemModel\n",
        "from castle.algorithms.gradient.notears.torch.golem_utils.train import postprocess\n",
        "from castle.algorithms.gradient.notears.torch.golem_utils.utils import set_seed\n",
        "\n",
        "from castle.common import BaseLearner, Tensor\n",
        "\n",
        "\n",
        "class GOLEM(BaseLearner):\n",
        "    \"\"\"\n",
        "    GOLEM Algorithm.\n",
        "    A more efficient version of NOTEARS that can reduce number of optimization iterations.\n",
        "\n",
        "    Paramaters\n",
        "    ----------\n",
        "    B_init: None\n",
        "        File of weighted matrix for initialization. Set to None to disable.\n",
        "    lambda_1: float\n",
        "        Coefficient of L1 penalty.\n",
        "    lambda_2: float\n",
        "        Coefficient of DAG penalty.\n",
        "    equal_variances: bool\n",
        "        Assume equal noise variances for likelibood objective.\n",
        "    non_equal_variances: bool\n",
        "        Assume non-equal noise variances for likelibood objective.\n",
        "    learning_rate: float\n",
        "        Learning rate of Adam optimizer.\n",
        "    num_iter: float\n",
        "        Number of iterations for training.\n",
        "    checkpoint_iter: int\n",
        "        Number of iterations between each checkpoint. Set to None to disable.\n",
        "    seed: int\n",
        "        Random seed.\n",
        "    graph_thres: float\n",
        "        Threshold for weighted matrix.\n",
        "    device_type: bool\n",
        "        whether to use GPU or not\n",
        "    device_ids: int\n",
        "        choose which gpu to use\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    causal_matrix : numpy.ndarray\n",
        "        Learned causal structure matrix\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    https://arxiv.org/abs/2006.10201\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from castle.algorithms import GOLEM\n",
        "    >>> from castle.datasets import load_dataset\n",
        "    >>> from castle.common import GraphDAG\n",
        "    >>> from castle.metrics import MetricsDAG\n",
        "    >>> X, true_dag, topology_matrix = load_dataset(name='IID_Test')\n",
        "    >>> n = GOLEM()\n",
        "    >>> n.learn(X)\n",
        "    >>> GraphDAG(n.causal_matrix, true_dag)\n",
        "    >>> met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "    >>> print(met.metrics)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, B_init=None,\n",
        "                 lambda_1=2e-2,\n",
        "                 lambda_2=5.0,\n",
        "                 equal_variances=True,\n",
        "                 non_equal_variances=True,\n",
        "                 learning_rate=1e-3,\n",
        "                 num_iter=1e+5,\n",
        "                 checkpoint_iter=5000,\n",
        "                 seed=1,\n",
        "                 graph_thres=0.3,\n",
        "                 device_type='cpu',\n",
        "                 device_ids=0):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.B_init = B_init\n",
        "        self.lambda_1 = lambda_1\n",
        "        self.lambda_2 = lambda_2\n",
        "        self.equal_variances = equal_variances\n",
        "        self.non_equal_variances = non_equal_variances\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iter = num_iter\n",
        "        self.checkpoint_iter = checkpoint_iter\n",
        "        self.seed = seed\n",
        "        self.graph_thres = graph_thres\n",
        "        self.device_type = device_type\n",
        "        self.device_ids = device_ids\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            logging.info('GPU is available.')\n",
        "        else:\n",
        "            logging.info('GPU is unavailable.')\n",
        "            if self.device_type == 'gpu':\n",
        "                raise ValueError(\"GPU is unavailable, \"\n",
        "                                 \"please set device_type = 'cpu'.\")\n",
        "        if self.device_type == 'gpu':\n",
        "            if self.device_ids:\n",
        "                os.environ['CUDA_VISIBLE_DEVICES'] = str(self.device_ids)\n",
        "            device = torch.device('cuda')\n",
        "        else:\n",
        "            device = torch.device('cpu')\n",
        "        self.device = device\n",
        "\n",
        "    def learn(self, data, columns=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Set up and run the GOLEM algorithm.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: castle.Tensor or numpy.ndarray\n",
        "            The castle.Tensor or numpy.ndarray format data you want to learn.\n",
        "        X: numpy.ndarray\n",
        "            [n, d] data matrix.\n",
        "        columns : Index or array-like\n",
        "            Column labels to use for resulting tensor. Will default to\n",
        "            RangeIndex (0, 1, 2, ..., n) if no column labels are provided.\n",
        "        lambda_1: float\n",
        "            Coefficient of L1 penalty.\n",
        "        lambda_2: float\n",
        "            Coefficient of DAG penalty.\n",
        "        equal_variances: bool\n",
        "            Whether to assume equal noise variances\n",
        "            for likelibood objective. Default: True.\n",
        "        num_iter:int\n",
        "            Number of iterations for training.\n",
        "        learning_rate: float\n",
        "            Learning rate of Adam optimizer. Default: 1e-3.\n",
        "        seed: int\n",
        "            Random seed. Default: 1.\n",
        "        checkpoint_iter: int\n",
        "            Number of iterations between each checkpoint.\n",
        "            Set to None to disable. Default: None.\n",
        "        B_init: numpy.ndarray or None\n",
        "            [d, d] weighted matrix for initialization.\n",
        "            Set to None to disable. Default: None.\n",
        "        \"\"\"\n",
        "\n",
        "        X = Tensor(data, columns=columns)\n",
        "\n",
        "        causal_matrix = self._golem(X)\n",
        "        self.causal_matrix = Tensor(causal_matrix, index=X.columns,\n",
        "                                    columns=X.columns)\n",
        "\n",
        "    def _golem(self, X):\n",
        "        \"\"\"\n",
        "        Solve the unconstrained optimization problem of GOLEM, which involves\n",
        "        GolemModel and GolemTrainer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray\n",
        "            [n, d] data matrix.\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        B_result: np.ndarray\n",
        "            [d, d] estimated weighted matrix.\n",
        "\n",
        "        Hyperparameters\n",
        "        ---------------\n",
        "        (1) GOLEM-NV: equal_variances=False, lambda_1=2e-3, lambda_2=5.0.\n",
        "        (2) GOLEM-EV: equal_variances=True, lambda_1=2e-2, lambda_2=5.0.\n",
        "        \"\"\"\n",
        "        set_seed(self.seed)\n",
        "        n, d = X.shape\n",
        "        X = torch.Tensor(X).to(self.device)\n",
        "\n",
        "        # Set up model\n",
        "        model = GolemModel(n=n, d=d, lambda_1=self.lambda_1,\n",
        "                           lambda_2=self.lambda_2,\n",
        "                           equal_variances=self.equal_variances,\n",
        "                           B_init=self.B_init,\n",
        "                           device=self.device)\n",
        "\n",
        "        self.train_op = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        logging.info(\"Started training for {} iterations.\".format(int(self.num_iter)))\n",
        "        for i in range(0, int(self.num_iter) + 1):\n",
        "            model(X)\n",
        "            score, likelihood, h, B_est = model.score, model.likelihood, model.h, model.B\n",
        "\n",
        "            if i > 0:  # Do not train here, only perform evaluation\n",
        "                # Optimizer\n",
        "                self.loss = score\n",
        "                self.train_op.zero_grad()\n",
        "                self.loss.backward()\n",
        "                self.train_op.step()\n",
        "\n",
        "            if self.checkpoint_iter is not None and i % self.checkpoint_iter == 0:\n",
        "                logging.info(\"[Iter {}] score={:.3f}, likelihood={:.3f}, h={:.1e}\".format( \\\n",
        "                    i, score, likelihood, h))\n",
        "\n",
        "        # Post-process estimated solution and compute results\n",
        "        B_processed = postprocess(B_est.cpu().detach().numpy(), graph_thres=0.3)\n",
        "        B_result = (B_processed != 0).astype(int)\n",
        "\n",
        "        return B_result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06ab858a",
      "metadata": {
        "id": "06ab858a"
      },
      "source": [
        "# GOLEM_Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "259185a9",
      "metadata": {
        "id": "259185a9",
        "outputId": "a73e39ad-9f71-46fe-c908-296eaa4c65fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-03 09:46:47,712 - C:\\Users\\notebook\\castle\\datasets\\simulator.py[line:270] - INFO: Finished synthetic dataset\n",
            "2023-04-03 09:46:47,712 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:122] - INFO: GPU is unavailable.\n",
            "2023-04-03 09:46:47,720 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:207] - INFO: Started training for 100000 iterations.\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'torch.linalg' has no attribute 'slogdet'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10824\\1402134284.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_dag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopology_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'IID_Test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGOLEM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mGraphDAG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcausal_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_dag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMetricsDAG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcausal_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_dag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\notears\\torch\\golem.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, data, columns, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[0mcausal_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_golem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m         self.causal_matrix = Tensor(causal_matrix, index=X.columns,\n\u001b[0;32m    172\u001b[0m                                     columns=X.columns)\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\notears\\torch\\golem.py\u001b[0m in \u001b[0;36m_golem\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Started training for {} iterations.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m             \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB_est\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\notears\\torch\\golem_utils\\golem_model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;31m# Likelihood, penalty terms and score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlikelihood\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL1_penalty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_L1_penalty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_h\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\notears\\torch\\golem_utils\\golem_model.py\u001b[0m in \u001b[0;36m_compute_likelihood\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m             return (0.5 * self.d\n\u001b[0;32m    114\u001b[0m                     \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m                     - torch.linalg.slogdet(torch.eye(self.d).to(self.device) - self.B)[1])\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Assuming non-equal noise variances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             return (0.5\n",
            "\u001b[1;31mAttributeError\u001b[0m: module 'torch.linalg' has no attribute 'slogdet'"
          ]
        }
      ],
      "source": [
        "from castle.algorithms import GOLEM\n",
        "from castle.datasets import load_dataset\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "X, true_dag, topology_matrix = load_dataset(name='IID_Test')\n",
        "n = GOLEM()\n",
        "n.learn(X)\n",
        "GraphDAG(n.causal_matrix, true_dag)\n",
        "met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "print(met.metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "771a8c01",
      "metadata": {
        "id": "771a8c01"
      },
      "source": [
        "# GraNDAG_Mindspore_Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48aff001",
      "metadata": {
        "id": "48aff001",
        "outputId": "af626dd9-2079-41a7-bbe7-c1c27a2c9999"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-03 14:52:44,322 - C:\\Users\\notebook\\castle\\datasets\\simulator.py[line:270] - INFO: Finished synthetic dataset\n",
            "2023-04-03 14:52:44,324 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gran_dag\\torch\\gran_dag.py[line:271] - INFO: GPU is unavailable.\n",
            "Training Iterations:   0%|                                                                   | 0/10000 [00:00<?, ?it/s]C:\\Users\\notebook\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\container.py:435: UserWarning: Setting attributes on ParameterList is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n",
            "Training Iterations: 100%|██████████████████████████████████████████████████████| 10000/10000 [01:01<00:00, 161.34it/s]\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'fdr': 0.0, 'tpr': 0.1, 'fpr': 0.0, 'shd': 18, 'nnz': 2, 'precision': 1.0, 'recall': 0.1, 'F1': 0.1818, 'gscore': 0.1}\n"
          ]
        }
      ],
      "source": [
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "from castle.datasets import DAG, IIDSimulation\n",
        "from castle.algorithms import GraNDAG\n",
        "\n",
        "# load data\n",
        "weighted_random_dag = DAG.erdos_renyi(n_nodes=10, n_edges=20,\n",
        "                                      weight_range=(0.5, 2.0), seed=1)\n",
        "dataset = IIDSimulation(W=weighted_random_dag, n=2000, method='nonlinear',\n",
        "                        sem_type='mlp')\n",
        "dag, x = dataset.B, dataset.X\n",
        "\n",
        "# Instantiation algorithm\n",
        "d = {'model_name': 'NonLinGauss', 'nonlinear': 'leaky-relu', 'optimizer': 'sgd', 'norm_prod': 'paths', 'device_type': 'gpu'}\n",
        "gnd = GraNDAG(input_dim=x.shape[1], )\n",
        "gnd.learn(data=x)\n",
        "\n",
        "# plot predict_dag and true_dag\n",
        "GraphDAG(gnd.causal_matrix, dag)\n",
        "mm = MetricsDAG(gnd.causal_matrix, dag)\n",
        "print(mm.metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17e47c60",
      "metadata": {
        "id": "17e47c60"
      },
      "source": [
        "# GraNDAG_Torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "085aab71",
      "metadata": {
        "id": "085aab71"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# Copyright (C) 2021. Huawei Technologies Co., Ltd. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "from castle.algorithms.gradient.gran_dag.torch.base import NonlinearGauss\n",
        "from castle.algorithms.gradient.gran_dag.torch.base import NonlinearGaussANM\n",
        "from castle.algorithms.gradient.gran_dag.torch.base import compute_constraint\n",
        "from castle.algorithms.gradient.gran_dag.torch.base import compute_jacobian_avg\n",
        "from castle.algorithms.gradient.gran_dag.torch.base import is_acyclic\n",
        "\n",
        "from castle.common import BaseLearner, Tensor\n",
        "from castle.common.validator import check_args_value\n",
        "from castle.common.consts import GRANDAG_VALID_PARAMS\n",
        "#gcastle/castle/algorithms/gradient/gran_dag/torch/base/base_model.py\n",
        "\n",
        "class NormalizationData(object):\n",
        "    \"\"\"\n",
        "    Create Normalization Data object\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : numpy.ndarray\n",
        "        train x\n",
        "    normalize : bool, default False\n",
        "        whether normalization\n",
        "    mean : float or None default None\n",
        "        Mean value of normalization\n",
        "    std : float or None default None\n",
        "        Standard Deviation of normalization\n",
        "    shuffle : bool\n",
        "        whether shuffle\n",
        "    train_size : float, default 0.8\n",
        "        ratio of train data for training\n",
        "    train : bool, default True\n",
        "        whether training\n",
        "    random_seed : int\n",
        "        for set random seed\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, normalize=False, mean=None, std=None,\n",
        "                 shuffle=False, train_size=0.8, train=True, random_seed=42):\n",
        "        self.random = np.random.RandomState(random_seed)\n",
        "\n",
        "        shuffle_idx = np.arange(data.shape[0])\n",
        "        if shuffle:\n",
        "            self.random.shuffle(shuffle_idx)\n",
        "\n",
        "        if isinstance(train_size, float):\n",
        "            train_samples = int(data.shape[0] * train_size)\n",
        "        else:\n",
        "            raise TypeError(\"The param train_size must be float < 1\")\n",
        "        if train:\n",
        "            data = data[shuffle_idx[: train_samples]]\n",
        "        else:\n",
        "            data = data[shuffle_idx[train_samples:]]\n",
        "        # as tensor\n",
        "        self.data_set = torch.as_tensor(data).type(torch.Tensor)\n",
        "\n",
        "        # Normalize data\n",
        "        self.mean, self.std = mean, std\n",
        "        if normalize:\n",
        "            if mean is None or std is None:\n",
        "                self.mean = torch.mean(self.data_set, 0, keepdim=True)\n",
        "                self.std = torch.std(self.data_set, 0, keepdim=True)\n",
        "            self.data_set = (self.data_set - self.mean) / self.std\n",
        "        self.n_samples = self.data_set.size(0)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"sampling from self.dataset\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size : int\n",
        "            batch size of sample\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        samples : torch.Tensor\n",
        "            sample data after sampling\n",
        "        torch.ones_like(samples): torch.Tensor\n",
        "        \"\"\"\n",
        "        sample_idxs = self.random.choice(np.arange(int(self.n_samples)),\n",
        "                                         size=(int(batch_size),),\n",
        "                                         replace=False)\n",
        "        samples = self.data_set[torch.as_tensor(sample_idxs).long()]\n",
        "\n",
        "        return samples, torch.ones_like(samples)\n",
        "\n",
        "\n",
        "class GraNDAG(BaseLearner):\n",
        "    \"\"\"\n",
        "    Gradient Based Neural DAG Learner\n",
        "\n",
        "    A gradient-based algorithm using neural network modeling for\n",
        "    non-linear additive noise data\n",
        "\n",
        "    References: https://arxiv.org/pdf/1906.02226.pdf\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_dim : int\n",
        "        number of input layer, must be int\n",
        "    hidden_num : int, default 2\n",
        "        number of hidden layers\n",
        "    hidden_dim : int, default 10\n",
        "        number of dimension per hidden layer\n",
        "    batch_size : int, default 64\n",
        "        batch size of per training of NN\n",
        "    lr : float, default 0.001\n",
        "        learning rate\n",
        "    iterations : int, default 10000\n",
        "        times of iteration\n",
        "    model_name : str, default 'NonLinGaussANM'\n",
        "        name of model, 'NonLinGauss' or 'NonLinGaussANM'\n",
        "    nonlinear : str, default 'leaky-relu'\n",
        "        name of Nonlinear activation function, 'sigmoid' or 'leaky-relu'\n",
        "    optimizer : str, default 'rmsprop'\n",
        "        Method of optimize, `rmsprop` or `sgd`\n",
        "    h_threshold : float, default 1e-8\n",
        "        constrained threshold\n",
        "    device_type : str, default 'cpu'\n",
        "        use gpu or cpu\n",
        "    use_pns : bool, default False\n",
        "        whether use pns before training, if nodes > 50, use it.\n",
        "    pns_thresh : float, default 0.75\n",
        "        threshold for feature importance score in pns\n",
        "    num_neighbors : int, default None\n",
        "        number of potential parents for each variables\n",
        "    normalize : bool, default False\n",
        "        whether normalize data\n",
        "    precision : bool, default False\n",
        "        whether use Double precision\n",
        "        if True, use torch.FloatTensor; if False, use torch.DoubleTensor\n",
        "    random_seed : int, default 42\n",
        "        random seed\n",
        "    norm_prod : str, default 'paths'\n",
        "        use norm of product of paths, 'none' or 'paths'\n",
        "        'paths': use norm, 'none': with no norm\n",
        "    square_prod : bool, default False\n",
        "        use squared product of paths\n",
        "    jac_thresh : bool, default True\n",
        "        get the average Jacobian with the trained model\n",
        "    lambda_init : float, default 0.0\n",
        "        initialization of Lagrangian coefficient in the optimization of\n",
        "        augmented Lagrangian\n",
        "    mu_init : float, default 0.001\n",
        "        initialization of penalty coefficient in the optimization of\n",
        "        augmented Lagrangian\n",
        "    omega_lambda : float, default 0.0001\n",
        "        tolerance on the delta lambda, to find saddle points\n",
        "    omega_mu : float, default 0.9\n",
        "        check whether the constraint decreases sufficiently if it decreases\n",
        "        at least (1-omega_mu) * h_prev\n",
        "    stop_crit_win : int, default 100\n",
        "        number of iterations for updating values\n",
        "    edge_clamp_range : float, default 0.0001\n",
        "        threshold for keeping the edge (if during training)\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "        Load data\n",
        "    >>> from castle.datasets import load_dataset\n",
        "    >>> data, true_dag, _ = load_dataset('IID_Test')\n",
        "\n",
        "    >>> gnd = GraNDAG(input_dim=data.shape[1])\n",
        "    >>> gnd.learn(data=data)\n",
        "\n",
        "        Also print GraN_DAG.model.adjacency with torch.Tensor type\n",
        "        or print GranN_DAG.causal_matrix with numpy.ndarray.\n",
        "    >>> print(gnd.causal_matrix)\n",
        "    >>> print(gnd.model.adjacency)\n",
        "    \"\"\"\n",
        "\n",
        "    @check_args_value(GRANDAG_VALID_PARAMS)\n",
        "    def __init__(self, input_dim,\n",
        "                 hidden_num=2,\n",
        "                 hidden_dim=10,\n",
        "                 batch_size=64,\n",
        "                 lr=0.001,\n",
        "                 iterations=10000,\n",
        "                 model_name='NonLinGaussANM',\n",
        "                 nonlinear='leaky-relu',\n",
        "                 optimizer='rmsprop',\n",
        "                 h_threshold=1e-8,\n",
        "                 device_type='cpu',\n",
        "                 device_ids='0',\n",
        "                 use_pns=False,\n",
        "                 pns_thresh=0.75,\n",
        "                 num_neighbors=None,\n",
        "                 normalize=False,\n",
        "                 precision=False,\n",
        "                 random_seed=42,\n",
        "                 jac_thresh=True,\n",
        "                 lambda_init=0.0,\n",
        "                 mu_init=0.001,\n",
        "                 omega_lambda=0.0001,\n",
        "                 omega_mu=0.9,\n",
        "                 stop_crit_win=100,\n",
        "                 edge_clamp_range=0.0001,\n",
        "                 norm_prod='paths',\n",
        "                 square_prod=False):\n",
        "        super(GraNDAG, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_num = hidden_num\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.iterations = iterations\n",
        "        self.model_name = model_name\n",
        "        self.nonlinear = nonlinear\n",
        "        self.optimizer = optimizer\n",
        "        self.h_threshold = h_threshold\n",
        "        self.device_type = device_type\n",
        "        self.device_ids = device_ids\n",
        "        self.use_pns = use_pns\n",
        "        self.pns_thresh = pns_thresh\n",
        "        self.num_neighbors = num_neighbors\n",
        "        self.normalize = normalize\n",
        "        self.precision = precision\n",
        "        self.random_seed = random_seed\n",
        "        self.jac_thresh = jac_thresh\n",
        "        self.lambda_init = lambda_init\n",
        "        self.mu_init = mu_init\n",
        "        self.omega_lambda = omega_lambda\n",
        "        self.omega_mu = omega_mu\n",
        "        self.stop_crit_win = stop_crit_win\n",
        "        self.edge_clamp_range = edge_clamp_range\n",
        "        self.norm_prod = norm_prod\n",
        "        self.square_prod = square_prod\n",
        "\n",
        "    def learn(self, data, columns=None, **kwargs):\n",
        "        \"\"\"Set up and run the Gran-DAG algorithm\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: numpy.ndarray or Tensor\n",
        "            include Tensor.data\n",
        "        columns : Index or array-like\n",
        "            Column labels to use for resulting tensor. Will default to\n",
        "            RangeIndex (0, 1, 2, ..., n) if no column labels are provided.\n",
        "        \"\"\"\n",
        "\n",
        "        # Control as much randomness as possible\n",
        "        torch.manual_seed(self.random_seed)\n",
        "        np.random.seed(self.random_seed)\n",
        "\n",
        "        # Use gpu\n",
        "        if torch.cuda.is_available():\n",
        "            logging.info('GPU is available.')\n",
        "        else:\n",
        "            logging.info('GPU is unavailable.')\n",
        "            if self.device_type == 'gpu':\n",
        "                raise ValueError(\"GPU is unavailable, \"\n",
        "                                 \"please set device_type = 'cpu'.\")\n",
        "        if self.device_type == 'gpu':\n",
        "            if self.precision:\n",
        "                torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "            else:\n",
        "                torch.set_default_tensor_type('torch.cuda.DoubleTensor')\n",
        "            if self.device_ids:\n",
        "                os.environ['CUDA_VISIBLE_DEVICES'] = str(self.device_ids)\n",
        "            device = torch.device('cuda')\n",
        "        else:\n",
        "            if self.precision:\n",
        "                torch.set_default_tensor_type('torch.FloatTensor')\n",
        "            else:\n",
        "                torch.set_default_tensor_type('torch.DoubleTensor')\n",
        "            device = torch.device('cpu')\n",
        "        self.device = device\n",
        "\n",
        "        # create learning model and ground truth model\n",
        "        data = Tensor(data, columns=columns)\n",
        "\n",
        "        if data.shape[1] != self.input_dim:\n",
        "            raise ValueError(\"The number of variables is `{}`, \"\n",
        "                             \"the param input_dim is `{}`, \"\n",
        "                             \"they must be consistent\"\n",
        "                             \".\".format(data.shape[1], self.input_dim))\n",
        "\n",
        "        if self.model_name == \"NonLinGauss\":\n",
        "            self.model = NonlinearGauss(input_dim=self.input_dim,\n",
        "                                        hidden_num=self.hidden_num,\n",
        "                                        hidden_dim=self.hidden_dim,\n",
        "                                        output_dim=2,\n",
        "                                        nonlinear=self.nonlinear,\n",
        "                                        norm_prod=self.norm_prod,\n",
        "                                        square_prod=self.square_prod)\n",
        "        elif self.model_name == \"NonLinGaussANM\":\n",
        "            self.model = NonlinearGaussANM(input_dim=self.input_dim,\n",
        "                                           hidden_num=self.hidden_num,\n",
        "                                           hidden_dim=self.hidden_dim,\n",
        "                                           output_dim=1,\n",
        "                                           nonlinear=self.nonlinear,\n",
        "                                           norm_prod=self.norm_prod,\n",
        "                                           square_prod=self.square_prod)\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"self.model has to be in {NonLinGauss, NonLinGaussANM}\")\n",
        "\n",
        "        # create NormalizationData\n",
        "        train_data = NormalizationData(data, train=True,\n",
        "                                       normalize=self.normalize)\n",
        "        test_data = NormalizationData(data, train=False,\n",
        "                                      normalize=self.normalize,\n",
        "                                      mean=train_data.mean,\n",
        "                                      std=train_data.std)\n",
        "\n",
        "        # apply preliminary neighborhood selection if input_dim > 50\n",
        "        if self.use_pns:\n",
        "            if self.num_neighbors is None:\n",
        "                num_neighbors = self.input_dim\n",
        "            else:\n",
        "                num_neighbors = self.num_neighbors\n",
        "\n",
        "            self.model = neighbors_selection(model=self.model, all_samples=data,\n",
        "                                             num_neighbors=num_neighbors,\n",
        "                                             thresh=self.pns_thresh)\n",
        "\n",
        "        # update self.model by train\n",
        "        self._train(train_data=train_data, test_data=test_data)\n",
        "\n",
        "        # update self.model by run _to_dag\n",
        "        self._to_dag(train_data)\n",
        "\n",
        "        self._causal_matrix = Tensor(self.model.adjacency.detach().cpu().numpy(),\n",
        "                                     index=data.columns,\n",
        "                                     columns=data.columns)\n",
        "\n",
        "    def _train(self, train_data, test_data):\n",
        "        \"\"\"\n",
        "        Applying augmented Lagrangian to solve the continuous constrained problem.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        train_data: NormalizationData\n",
        "            train samples\n",
        "        test_data: NormalizationData object\n",
        "            test samples for validation\n",
        "        \"\"\"\n",
        "\n",
        "        # initialize stuff for learning loop\n",
        "        aug_lagrangians = []\n",
        "        aug_lagrangian_ma = [0.0] * (self.iterations + 1)\n",
        "        aug_lagrangians_val = []\n",
        "        grad_norms = []\n",
        "        grad_norm_ma = [0.0] * (self.iterations + 1)\n",
        "\n",
        "        w_adjs = np.zeros((self.iterations,\n",
        "                           self.input_dim,\n",
        "                           self.input_dim), dtype=np.float32)\n",
        "\n",
        "        hs = []\n",
        "        not_nlls = []  # Augmented Lagrangian minus (pseudo) NLL\n",
        "        nlls = []  # NLL on train\n",
        "        nlls_val = []  # NLL on validation\n",
        "\n",
        "        # Augmented Lagrangian stuff\n",
        "        mu = self.mu_init\n",
        "        lamb = self.lambda_init\n",
        "        mus = []\n",
        "        lambdas = []\n",
        "\n",
        "        if self.optimizer == \"sgd\":\n",
        "            optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr)\n",
        "        elif self.optimizer == \"rmsprop\":\n",
        "            optimizer = torch.optim.RMSprop(self.model.parameters(), lr=self.lr)\n",
        "        else:\n",
        "            raise NotImplementedError(\"optimizer {} is not implemented\"\n",
        "                                      .format(self.optimizer))\n",
        "\n",
        "        # Learning loop:\n",
        "        for iter in tqdm(range(self.iterations), desc='Training Iterations'):\n",
        "            # compute loss\n",
        "            self.model.train()\n",
        "            x, _ = train_data.sample(self.batch_size)\n",
        "            # Initialize weights and bias\n",
        "            weights, biases, extra_params = self.model.get_parameters(mode=\"wbx\")\n",
        "            loss = - torch.mean(\n",
        "                self.model.compute_log_likelihood(x, weights, biases, extra_params))\n",
        "            nlls.append(loss.item())\n",
        "            self.model.eval()\n",
        "\n",
        "            # constraint related\n",
        "            w_adj = self.model.get_w_adj()\n",
        "            h = compute_constraint(self.model, w_adj)\n",
        "\n",
        "            # compute augmented Lagrangian\n",
        "            aug_lagrangian = loss + 0.5 * mu * h ** 2 + lamb * h\n",
        "\n",
        "            # optimization step on augmented lagrangian\n",
        "            optimizer.zero_grad()\n",
        "            aug_lagrangian.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # clamp edges\n",
        "            if self.edge_clamp_range != 0:\n",
        "                with torch.no_grad():\n",
        "                    to_keep = (w_adj > self.edge_clamp_range) * 1\n",
        "                    self.model.adjacency *= to_keep\n",
        "\n",
        "            # logging\n",
        "            w_adjs[iter, :, :] = w_adj.detach().cpu().numpy().astype(np.float32)\n",
        "            mus.append(mu)\n",
        "            lambdas.append(lamb)\n",
        "            not_nlls.append(0.5 * mu * h.item() ** 2 + lamb * h.item())\n",
        "\n",
        "            # compute augmented lagrangian moving average\n",
        "            aug_lagrangians.append(aug_lagrangian.item())\n",
        "            aug_lagrangian_ma[iter + 1] = aug_lagrangian_ma[iter] + \\\n",
        "                                          0.01 * (aug_lagrangian.item() -\n",
        "                                                  aug_lagrangian_ma[iter])\n",
        "            grad_norms.append(self.model.get_grad_norm(\"wbx\").item())\n",
        "            grad_norm_ma[iter + 1] = grad_norm_ma[iter] + \\\n",
        "                                     0.01 * (grad_norms[-1] - grad_norm_ma[iter])\n",
        "\n",
        "            # compute loss on whole validation set\n",
        "            if iter % self.stop_crit_win == 0:\n",
        "                with torch.no_grad():\n",
        "                    x, _ = test_data.sample(test_data.n_samples)\n",
        "                    loss_val = - torch.mean(self.model.compute_log_likelihood(x,\n",
        "                                                                 weights,\n",
        "                                                                 biases,\n",
        "                                                                 extra_params))\n",
        "                    nlls_val.append(loss_val.item())\n",
        "                    aug_lagrangians_val.append([iter, loss_val + not_nlls[-1]])\n",
        "\n",
        "            # compute delta for lambda\n",
        "            if iter >= 2 * self.stop_crit_win \\\n",
        "                    and iter % (2 * self.stop_crit_win) == 0:\n",
        "                t0 = aug_lagrangians_val[-3][1]\n",
        "                t_half = aug_lagrangians_val[-2][1]\n",
        "                t1 = aug_lagrangians_val[-1][1]\n",
        "\n",
        "                # if the validation loss went up and down,\n",
        "                # do not update lagrangian and penalty coefficients.\n",
        "                if not (min(t0, t1) < t_half < max(t0, t1)):\n",
        "                    delta_lambda = -np.inf\n",
        "                else:\n",
        "                    delta_lambda = (t1 - t0) / self.stop_crit_win\n",
        "            else:\n",
        "                delta_lambda = -np.inf  # do not update lambda nor mu\n",
        "\n",
        "            # Does the augmented lagrangian converged?\n",
        "            if h > self.h_threshold:\n",
        "                # if we have found a stationary point of the augmented loss\n",
        "                if abs(delta_lambda) < self.omega_lambda or delta_lambda > 0:\n",
        "                    lamb += mu * h.item()\n",
        "\n",
        "                    # Did the constraint improve sufficiently?\n",
        "                    hs.append(h.item())\n",
        "                    if len(hs) >= 2:\n",
        "                        if hs[-1] > hs[-2] * self.omega_mu:\n",
        "                            mu *= 10\n",
        "\n",
        "                    # little hack to make sure the moving average is going down.\n",
        "                    with torch.no_grad():\n",
        "                        gap_in_not_nll = 0.5 * mu * h.item() ** 2 + \\\n",
        "                                         lamb * h.item() - not_nlls[-1]\n",
        "                        aug_lagrangian_ma[iter + 1] += gap_in_not_nll\n",
        "                        aug_lagrangians_val[-1][1] += gap_in_not_nll\n",
        "\n",
        "                    if self.optimizer == \"rmsprop\":\n",
        "                        optimizer = torch.optim.RMSprop(self.model.parameters(),\n",
        "                                                        lr=self.lr)\n",
        "                    else:\n",
        "                        optimizer = torch.optim.SGD(self.model.parameters(),\n",
        "                                                    lr=self.lr)\n",
        "            else:\n",
        "                # Final clamping of all edges == 0\n",
        "                with torch.no_grad():\n",
        "                    to_keep = (w_adj > 0).type(torch.Tensor)\n",
        "                    self.model.adjacency *= to_keep\n",
        "\n",
        "                return self.model\n",
        "\n",
        "    def _to_dag(self, train_data):\n",
        "        \"\"\"\n",
        "        1- If some entries of A_\\phi == 0, also mask them\n",
        "        (This can happen with stochastic proximal gradient descent)\n",
        "        2- Remove edges (from weaker to stronger) until a DAG is obtained.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        train_data : NormalizationData\n",
        "            train samples\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        if self.jac_thresh:\n",
        "            A = compute_jacobian_avg(self.model, train_data,\n",
        "                                     train_data.n_samples).t()\n",
        "        else:\n",
        "            A = self.model.get_w_adj()\n",
        "        A = A.detach().cpu().numpy()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Find the smallest threshold that removes all cycle-inducing edges\n",
        "            thresholds = np.unique(A)\n",
        "            epsilon = 1e-8\n",
        "            for step, t in enumerate(thresholds):\n",
        "                to_keep = torch.Tensor(A > t + epsilon)\n",
        "                new_adj = self.model.adjacency * to_keep\n",
        "                if is_acyclic(new_adj, device=self.device):\n",
        "                    self.model.adjacency.copy_(new_adj)\n",
        "                    break\n",
        "\n",
        "        return self.model\n",
        "\n",
        "\n",
        "def neighbors_selection(model, all_samples, num_neighbors, thresh):\n",
        "    \"\"\"\n",
        "    Preliminary neighborhood selection\n",
        "    After pns, just model.adjacency is changed. if nodes > 50, use it.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: model object\n",
        "    all_samples: array-like\n",
        "        2 dimensional array include all samples\n",
        "    num_neighbors: integer\n",
        "        variable number or neighbors number you want\n",
        "    thresh: float\n",
        "        apply for sklearn.feature_selection.SelectFromModel\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    out: model\n",
        "    \"\"\"\n",
        "\n",
        "    model_adj = model.adjacency.detach().cpu().numpy()\n",
        "    model_adj = _pns(model_adj, all_samples, num_neighbors, thresh)\n",
        "    with torch.no_grad():\n",
        "        model.adjacency.copy_(torch.Tensor(model_adj))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def _pns(model_adj, all_samples, num_neighbors, thresh):\n",
        "    \"\"\"Preliminary neighborhood selection\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model_adj : numpy.ndarray\n",
        "        adjacency matrix, all element is 1\n",
        "    all_samples: numpy.ndarray\n",
        "        2 dimensional array include all samples\n",
        "    num_neighbors: integer\n",
        "        variable number or neighbors number you want\n",
        "    thresh: float\n",
        "        apply for sklearn.feature_selection.SelectFromModel\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model_adj : numpy.ndarray\n",
        "        adjacency matrix, after pns process\n",
        "    \"\"\"\n",
        "\n",
        "    num_nodes = all_samples.shape[1]\n",
        "\n",
        "    for node in tqdm(range(num_nodes), desc='Preliminary neighborhood selection'):\n",
        "        x_other = np.copy(all_samples)\n",
        "        x_other[:, node] = 0\n",
        "        extraTree = ExtraTreesRegressor(n_estimators=500)\n",
        "        extraTree.fit(x_other, all_samples[:, node])\n",
        "        selected_reg = SelectFromModel(extraTree,\n",
        "                                       threshold=\"{}*mean\".format(thresh),\n",
        "                                       prefit=True,\n",
        "                                       max_features=num_neighbors)\n",
        "        mask_selected = selected_reg.get_support(indices=False)\n",
        "        model_adj[:, node] *= mask_selected\n",
        "\n",
        "    return model_adj\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fc29982",
      "metadata": {
        "id": "1fc29982"
      },
      "source": [
        "# GraNDAG_Torch_Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d2fc4b8",
      "metadata": {
        "id": "5d2fc4b8",
        "outputId": "5fde779a-8cb8-4426-ffcc-1203e9f1e68b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-03 12:05:37,800 - C:\\Users\\notebook\\castle\\datasets\\simulator.py[line:270] - INFO: Finished synthetic dataset\n",
            "2023-04-03 12:05:37,800 - C:\\Users\\notebook\\AppData\\Local\\Temp\\ipykernel_10824\\4249829128.py[line:271] - INFO: GPU is unavailable.\n",
            "Training Iterations:   0%|                                                                   | 0/10000 [00:00<?, ?it/s]C:\\Users\\notebook\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\container.py:435: UserWarning: Setting attributes on ParameterList is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n",
            "Training Iterations: 100%|██████████████████████████████████████████████████████| 10000/10000 [01:10<00:00, 140.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "from castle.datasets import load_dataset\n",
        "data, true_dag, _ = load_dataset('IID_Test')\n",
        "\n",
        "gnd = GraNDAG(input_dim=data.shape[1])\n",
        "gnd.learn(data=data)\n",
        "\n",
        "print(gnd.causal_matrix)\n",
        "print(gnd.model.adjacency)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfae3c00",
      "metadata": {
        "id": "dfae3c00"
      },
      "source": [
        "# MCSL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5da535c3",
      "metadata": {
        "id": "5da535c3"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# Copyright (C) 2021. Huawei Technologies Co., Ltd. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from castle.common.base import BaseLearner, Tensor\n",
        "from castle.algorithms.gradient.mcsl.torch.trainers.al_trainer import Trainer\n",
        "from castle.algorithms.gradient.mcsl.torch.models.masked_model import MaskedModel\n",
        "from castle.algorithms.gradient.mcsl.torch.helpers.utils import callback_after_training\n",
        "from castle.common.consts import MCSL_VALID_PARAMS\n",
        "from castle.common.validator import check_args_value\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Referred from:\n",
        "    - https://stackoverflow.com/questions/38469632/tensorflow-non-repeatable-results\n",
        "    \"\"\"\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    try:\n",
        "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    finally:\n",
        "        pass\n",
        "\n",
        "\n",
        "class MCSL(BaseLearner):\n",
        "    \"\"\"\n",
        "    Masked Gradient-Based Causal Structure Learning\n",
        "\n",
        "    A gradient-based algorithm for non-linear additive noise data by learning\n",
        "    the binary adjacency matrix.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model_type: str, default: 'nn'\n",
        "        `nn` denotes neural network, `qr` denotes quatratic regression.\n",
        "    num_hidden_layers: int, default: 4\n",
        "        Number of hidden layer in neural network when `model_type` is 'nn'.\n",
        "    hidden_dim: int, default: 16\n",
        "        Number of hidden dimension in hidden layer, when `model_type` is 'nn'.\n",
        "    graph_thresh: float, default: 0.5\n",
        "        Threshold used to determine whether has edge in graph, element greater\n",
        "        than the `graph_thresh` means has a directed edge, otherwise has not.\n",
        "    l1_graph_penalty: float, default: 2e-3\n",
        "        Penalty weight for L1 normalization\n",
        "    learning_rate: float, default: 3e-2\n",
        "        learning rate for opitimizer\n",
        "    max_iter: int, default: 25\n",
        "        Number of iterations for optimization problem\n",
        "    iter_step: int, default: 1000\n",
        "        Number of steps for each iteration\n",
        "    init_iter: int, default: 2\n",
        "        Initial iteration to disallow early stopping\n",
        "    h_tol: float, default: 1e-10\n",
        "        Tolerance of optimization problem\n",
        "    init_rho: float, default: 1e-5\n",
        "        Initial value for penalty parameter.\n",
        "    rho_thresh: float, default: 1e14\n",
        "        Threshold for penalty parameter.\n",
        "    h_thresh: float, default: 0.25\n",
        "        Threshold for h\n",
        "    rho_multiply: float, default: 10.0\n",
        "        Multiplication to amplify rho each time\n",
        "    temperature: float, default: 0.2\n",
        "        Temperature for gumbel sigmoid\n",
        "    device_type: str, default: 'cpu'\n",
        "        'cpu' or 'gpu'\n",
        "    device_ids: int or str, default '0'\n",
        "        CUDA devices, it's effective when ``use_gpu`` is True.\n",
        "        For single-device modules, ``device_ids`` can be int or str, e.g. 0 or '0',\n",
        "        For multi-device modules, ``device_ids`` must be str, format like '0, 1'.\n",
        "    random_seed: int, default: 1230\n",
        "        random seed for every random value\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    https://arxiv.org/abs/1910.08527\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from castle.algorithms import MCSL\n",
        "    >>> from castle.datasets import load_dataset\n",
        "    >>> from castle.common import GraphDAG\n",
        "    >>> from castle.metrics import MetricsDAG\n",
        "    >>> true_dag, X = load_dataset(name='iid_test')\n",
        "    >>> n = MCSL(iter_step=1000, rho_thres=1e14, init_rho=1e-5,\n",
        "    ...          rho_multiply=10, graph_thres=0.5, l1_graph_penalty=2e-3)\n",
        "    >>> n.learn(X)\n",
        "    >>> GraphDAG(n.causal_matrix, true_dag)\n",
        "    >>> met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "    >>> print(met.metrics)\n",
        "    \"\"\"\n",
        "\n",
        "    @check_args_value(MCSL_VALID_PARAMS)\n",
        "    def __init__(self, model_type='nn', num_hidden_layers=4, hidden_dim=16,\n",
        "                 graph_thresh=0.5, l1_graph_penalty=2e-3, learning_rate=3e-2,\n",
        "                 max_iter=25, iter_step=1000, init_iter=2, h_tol=1e-10,\n",
        "                 init_rho=1e-5, rho_thresh=1e14, h_thresh=0.25,\n",
        "                 rho_multiply=10, temperature=0.2, device_type='cpu',\n",
        "                 device_ids='0', random_seed=1230) -> None:\n",
        "        super(MCSL, self).__init__()\n",
        "\n",
        "        self.model_type = model_type\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.graph_thresh = graph_thresh\n",
        "        self.l1_graph_penalty = l1_graph_penalty\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iter = max_iter\n",
        "        self.iter_step = iter_step\n",
        "        self.init_iter = init_iter\n",
        "        self.h_tol = h_tol\n",
        "        self.init_rho = init_rho\n",
        "        self.rho_thresh = rho_thresh\n",
        "        self.h_thresh = h_thresh\n",
        "        self.rho_multiply = rho_multiply\n",
        "        self.temperature = temperature\n",
        "        self.device_type = device_type\n",
        "        self.device_ids = device_ids\n",
        "        self.random_seed = random_seed\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            logging.info('GPU is available.')\n",
        "        else:\n",
        "            logging.info('GPU is unavailable.')\n",
        "            if self.device_type == 'gpu':\n",
        "                raise ValueError(\"GPU is unavailable, \"\n",
        "                                 \"please set device_type = 'cpu'.\")\n",
        "        if self.device_type == 'gpu':\n",
        "            if self.device_ids:\n",
        "                os.environ['CUDA_VISIBLE_DEVICES'] = str(self.device_ids)\n",
        "            device = torch.device('cuda')\n",
        "        else:\n",
        "            device = torch.device('cpu')\n",
        "        self.device = device\n",
        "\n",
        "    def learn(self, data, columns=None, pns_mask=None, **kwargs) -> None:\n",
        "        \"\"\"\n",
        "        Set up and run the MCSL algorithm.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: castle.Tensor or numpy.ndarray\n",
        "            The castle.Tensor or numpy.ndarray format data you want to learn.\n",
        "        columns: Index or array-like\n",
        "            Column labels to use for resulting tensor. Will default to\n",
        "            RangeIndex (0, 1, 2, ..., n) if no column labels are provided.\n",
        "        pns_mask: array_like or None\n",
        "            The mask matrix.\n",
        "            array with element in {0, 1}, ``0`` denotes has no edge in i -> j,\n",
        "            ``1`` denotes maybe has edge in i -> j or not.\n",
        "        \"\"\"\n",
        "\n",
        "        x = Tensor(data, columns=columns)\n",
        "\n",
        "        self.n_samples, self.n_nodes = x.shape\n",
        "        if pns_mask is None:\n",
        "            pns_mask = torch.ones([x.shape[1], x.shape[1]], device=self.device)\n",
        "        else:\n",
        "            pns_mask = torch.tensor(pns_mask, device=self.device)\n",
        "\n",
        "        causal_matrix, causal_matrix_weight = self._mcsl(x, pns_mask)\n",
        "\n",
        "        self.causal_matrix_weight = Tensor(causal_matrix_weight,\n",
        "                                           index=x.columns,\n",
        "                                           columns=x.columns\n",
        "                                           )\n",
        "        self.causal_matrix = Tensor(causal_matrix,\n",
        "                                    index=x.columns,\n",
        "                                    columns=x.columns\n",
        "                                    )\n",
        "\n",
        "    def _mcsl(self, x, pns_mask) -> tuple:\n",
        "        \"\"\"\n",
        "        Starting model of MCSL.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: torch.Tensor\n",
        "            The torch.Tensor data you want to learn.\n",
        "        pns_mask: torch.Tensor\n",
        "            The mask matrix.\n",
        "        \"\"\"\n",
        "\n",
        "        set_seed(self.random_seed)\n",
        "\n",
        "        model = MaskedModel(model_type=self.model_type,\n",
        "                            n_samples=self.n_samples,\n",
        "                            n_nodes=self.n_nodes,\n",
        "                            pns_mask=pns_mask,\n",
        "                            num_hidden_layers=self.num_hidden_layers,\n",
        "                            hidden_dim=self.hidden_dim,\n",
        "                            l1_graph_penalty=self.l1_graph_penalty,\n",
        "                            seed=self.random_seed,\n",
        "                            device=self.device)\n",
        "        trainer = Trainer(model=model,\n",
        "                          learning_rate=self.learning_rate,\n",
        "                          init_rho=self.init_rho,\n",
        "                          rho_thresh=self.rho_thresh,\n",
        "                          h_thresh=self.h_thresh,\n",
        "                          rho_multiply=self.rho_multiply,\n",
        "                          init_iter=self.init_iter,\n",
        "                          h_tol=self.h_tol,\n",
        "                          temperature=self.temperature,\n",
        "                          device=self.device)\n",
        "\n",
        "        if not isinstance(x, torch.Tensor):\n",
        "            x = torch.tensor(x, device=self.device)\n",
        "        w_logits = trainer.train(x, self.max_iter, self.iter_step)\n",
        "\n",
        "        w_est, w_est_weight = callback_after_training(w_logits,\n",
        "                                                      self.temperature,\n",
        "                                                      self.graph_thresh)\n",
        "        return w_est.detach().cpu().numpy(), w_est_weight.detach().cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7170aaee",
      "metadata": {
        "id": "7170aaee"
      },
      "source": [
        "# MCSL_Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faacde44",
      "metadata": {
        "id": "faacde44",
        "outputId": "f37c3524-9394-4195-e970-cb6aacf91d4f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-03 12:39:56,246 - C:\\Users\\notebook\\castle\\datasets\\simulator.py[line:270] - INFO: Finished synthetic dataset\n",
            "2023-04-03 12:39:56,247 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\mcsl.py[line:144] - INFO: GPU is unavailable.\n",
            "2023-04-03 12:39:56,261 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 1==================\n",
            "2023-04-03 12:39:56,314 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 172.21960756363447\n",
            "2023-04-03 12:40:04,322 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 2.0705390402262447\n",
            "2023-04-03 12:40:12,116 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.756680084647887\n",
            "2023-04-03 12:40:19,920 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.6667374272769206\n",
            "2023-04-03 12:40:27,678 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.6184198320065792\n",
            "2023-04-03 12:40:35,388 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 14.612440766258683\n",
            "2023-04-03 12:40:35,388 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 30.275212386045027\n",
            "2023-04-03 12:40:35,388 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 2==================\n",
            "2023-04-03 12:40:35,440 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.5807159895817755\n",
            "2023-04-03 12:40:43,553 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.5458887798272134\n",
            "2023-04-03 12:40:52,169 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.5130923219609467\n",
            "2023-04-03 12:41:00,936 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.506705689656188\n",
            "2023-04-03 12:41:09,070 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.5604003720813322\n",
            "2023-04-03 12:41:17,423 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.518087448933749\n",
            "2023-04-03 12:41:25,643 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.5298201861180967\n",
            "2023-04-03 12:41:33,895 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.4466128896598012\n",
            "2023-04-03 12:41:42,523 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.4058026102518244\n",
            "2023-04-03 12:41:51,434 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.3749506512236302\n",
            "2023-04-03 12:41:59,803 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.3830310670574082\n",
            "2023-04-03 12:42:07,903 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.3495845294340878\n",
            "2023-04-03 12:42:15,988 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.3526826555131335\n",
            "2023-04-03 12:42:24,014 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.3425991582521977\n",
            "2023-04-03 12:42:32,088 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.2989619956835354\n",
            "2023-04-03 12:42:40,066 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 2.0148059691714444\n",
            "2023-04-03 12:42:40,066 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 24.293849003898544\n",
            "2023-04-03 12:42:40,066 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 3==================\n",
            "2023-04-03 12:42:40,143 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.311475332408541\n",
            "2023-04-03 12:42:48,553 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.2812017029344036\n",
            "2023-04-03 12:42:56,902 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.2864250398156116\n",
            "2023-04-03 12:43:05,266 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.3126732860785195\n",
            "2023-04-03 12:43:13,396 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.2545727867953274\n",
            "2023-04-03 12:43:21,719 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.583558840465297\n",
            "2023-04-03 12:43:30,283 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.354065147955072\n",
            "2023-04-03 12:43:39,065 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.2695162830768278\n",
            "2023-04-03 12:43:48,623 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.2444688381895108\n",
            "2023-04-03 12:43:58,667 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.2215719978375603\n",
            "2023-04-03 12:44:09,289 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.2376815151882092\n",
            "2023-04-03 12:44:20,586 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.1965036521139742\n",
            "2023-04-03 12:44:32,307 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.1841456198084033\n",
            "2023-04-03 12:44:45,541 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.198043652340504\n",
            "2023-04-03 12:44:58,595 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.184331290779044\n",
            "2023-04-03 12:45:09,944 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 0.21507671584605603\n",
            "2023-04-03 12:45:09,944 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 16.468165330531697\n",
            "2023-04-03 12:45:09,944 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 4==================\n",
            "2023-04-03 12:45:09,993 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.1712305303169612\n",
            "2023-04-03 12:45:21,772 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.1999678156655438\n",
            "2023-04-03 12:45:33,269 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.1663252619151006\n",
            "2023-04-03 12:45:43,869 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.103643648027766\n",
            "2023-04-03 12:45:54,016 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.1020331791224833\n",
            "2023-04-03 12:46:03,786 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.106581806969012\n",
            "2023-04-03 12:46:13,442 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.2166124893347647\n",
            "2023-04-03 12:46:21,963 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.1474754732174741\n",
            "2023-04-03 12:46:29,484 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.1139217470987808\n",
            "2023-04-03 12:46:37,039 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.5487465165769358\n",
            "2023-04-03 12:46:44,580 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 0.05322761133114895\n",
            "2023-04-03 12:46:44,580 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 13.292174026609366\n",
            "2023-04-03 12:46:44,580 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 5==================\n",
            "2023-04-03 12:46:44,626 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.1603818612509365\n",
            "2023-04-03 12:46:52,169 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.1085466343120003\n",
            "2023-04-03 12:46:59,715 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.1839831068981015\n",
            "2023-04-03 12:47:07,298 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.16005640463496\n",
            "2023-04-03 12:47:14,958 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.1216654203214254\n",
            "2023-04-03 12:47:22,964 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.1313175972842378\n",
            "2023-04-03 12:47:31,121 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.1047317515950885\n",
            "2023-04-03 12:47:39,615 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.0923847515252607\n",
            "2023-04-03 12:47:48,391 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.0921034125722644\n",
            "2023-04-03 12:47:57,964 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.0549921703917318\n",
            "2023-04-03 12:48:09,738 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.0800556490017355\n",
            "2023-04-03 12:48:21,617 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.0995437517748872\n",
            "2023-04-03 12:48:33,443 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.164045143218508\n",
            "2023-04-03 12:48:48,058 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.0347331131287119\n",
            "2023-04-03 12:49:05,481 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.0576820582064685\n",
            "2023-04-03 12:49:22,687 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 0.008013265075899056\n",
            "2023-04-03 12:49:22,687 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 9.256776188028088\n",
            "2023-04-03 12:49:22,696 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 6==================\n",
            "2023-04-03 12:49:22,788 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.0953363498077666\n",
            "2023-04-03 12:49:37,412 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.0224019446048604\n",
            "2023-04-03 12:49:49,275 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.0564212342470167\n",
            "2023-04-03 12:50:00,902 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.0321342293132036\n",
            "2023-04-03 12:50:11,683 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.0966505527697845\n",
            "2023-04-03 12:50:21,002 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.1021688633269988\n",
            "2023-04-03 12:50:29,817 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.0223571494850765\n",
            "2023-04-03 12:50:38,275 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.034189704916487\n",
            "2023-04-03 12:50:46,439 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.054969412745555\n",
            "2023-04-03 12:50:54,411 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.0206343533360183\n",
            "2023-04-03 12:51:02,219 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.0678304725657346\n",
            "2023-04-03 12:51:09,825 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.026220549152489\n",
            "2023-04-03 12:51:17,327 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.005390328819679\n",
            "2023-04-03 12:51:24,813 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.0439136950373198\n",
            "2023-04-03 12:51:32,432 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.0194924841112452\n",
            "2023-04-03 12:51:40,063 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 0.0009675705444340821\n",
            "2023-04-03 12:51:40,063 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 5.87755064117812\n",
            "2023-04-03 12:51:40,063 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 7==================\n",
            "2023-04-03 12:51:40,106 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.0744826336075808\n",
            "2023-04-03 12:51:47,954 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.2074835985192922\n",
            "2023-04-03 12:51:56,047 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.0177144207722275\n",
            "2023-04-03 12:52:05,158 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.0677168198578022\n",
            "2023-04-03 12:52:14,580 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.051432291826676\n",
            "2023-04-03 12:52:24,819 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.073159703295811\n",
            "2023-04-03 12:52:34,711 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.0556807035084292\n",
            "2023-04-03 12:52:45,740 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 0.9880833031455373\n",
            "2023-04-03 12:52:57,200 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 0.9567749514175276\n",
            "2023-04-03 12:53:10,815 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 0.9814158806896922\n",
            "2023-04-03 12:53:26,506 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.0833208209398144\n",
            "2023-04-03 12:53:43,352 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.0928390129487386\n",
            "2023-04-03 12:53:59,885 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.0475160478948\n",
            "2023-04-03 12:54:13,179 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 0.9571189941786714\n",
            "2023-04-03 12:54:24,646 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.008603475041625\n",
            "2023-04-03 12:54:35,129 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 0.00011634858939402193\n",
            "2023-04-03 12:54:35,129 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 3.9612114019225295\n",
            "2023-04-03 12:54:35,129 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 8==================\n",
            "2023-04-03 12:54:35,193 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 0.9900077899179242\n",
            "2023-04-03 12:54:45,084 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.0327241313841031\n",
            "2023-04-03 12:54:54,474 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.0131505276497659\n",
            "2023-04-03 12:55:03,434 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 0.9467272197611095\n",
            "2023-04-03 12:55:11,937 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 0.9801182936714373\n",
            "2023-04-03 12:55:20,229 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 0.9854831449417669\n",
            "2023-04-03 12:55:28,298 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 0.9583304605525461\n",
            "2023-04-03 12:55:36,115 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.0298974441032995\n",
            "2023-04-03 12:55:43,799 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.0025865888193268\n",
            "2023-04-03 12:55:51,443 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 0.9586380888355656\n",
            "2023-04-03 12:55:59,169 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 0.9946704095891908\n",
            "2023-04-03 12:56:06,958 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.0652079011271303\n",
            "2023-04-03 12:56:14,859 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 0.9858893190620042\n",
            "2023-04-03 12:56:22,976 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 0.965533049843913\n",
            "2023-04-03 12:56:30,934 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 0.9852854681722283\n",
            "2023-04-03 12:56:39,034 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 1.3334510741813688e-05\n",
            "2023-04-03 12:56:39,034 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 3.0124698350162245\n",
            "2023-04-03 12:56:39,034 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 9==================\n",
            "2023-04-03 12:56:39,083 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 0.9557159435579088\n",
            "2023-04-03 12:56:47,411 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 0.9756925177480793\n",
            "2023-04-03 12:56:56,136 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.0096483009130905\n",
            "2023-04-03 12:57:05,425 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 0.9725686287662292\n",
            "2023-04-03 12:57:15,126 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.0629068501898349\n",
            "2023-04-03 12:57:25,307 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.0116407547758226\n",
            "2023-04-03 12:57:36,068 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.0265773342193836\n",
            "2023-04-03 12:57:48,215 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 0.9627255424450575\n",
            "2023-04-03 12:58:02,901 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 0.9713448704459346\n",
            "2023-04-03 12:58:20,037 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.0317331759895616\n",
            "2023-04-03 12:58:36,541 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.0494775952310096\n",
            "2023-04-03 12:58:52,242 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.0338698443139336\n",
            "2023-04-03 12:59:06,387 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.0285551774876098\n",
            "2023-04-03 12:59:18,144 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.08752061371429\n",
            "2023-04-03 12:59:28,623 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.0406068022995172\n",
            "2023-04-03 12:59:40,807 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 1.4110290820212867e-06\n",
            "2023-04-03 12:59:40,807 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 2.5543306990433194\n",
            "2023-04-03 12:59:40,807 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 10==================\n",
            "2023-04-03 12:59:40,873 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 0.9889562482170019\n",
            "2023-04-03 12:59:50,719 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 0.9856685568569447\n"
          ]
        }
      ],
      "source": [
        "from castle.algorithms import MCSL\n",
        "from castle.datasets import load_dataset\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "\n",
        "X, true_dag, _ = load_dataset(name='IID_Test')\n",
        "\n",
        "n = MCSL(iter_step=1000, init_rho=1e-5,rho_multiply=10, l1_graph_penalty=2e-3)\n",
        "n.learn(X)\n",
        "GraphDAG(n.causal_matrix, true_dag)\n",
        "met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "print(met.metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdfd5dfa",
      "metadata": {
        "id": "bdfd5dfa"
      },
      "source": [
        "# GAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "698243a2",
      "metadata": {
        "id": "698243a2",
        "outputId": "76358011-64f5-44f0-e835-5b1afbc897fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-03 13:07:02,319 - C:\\Users\\notebook\\castle\\backend\\__init__.py[line:36] - INFO: You can use `os.environ['CASTLE_BACKEND'] = backend` to set the backend(`pytorch` or `mindspore`).\n",
            "2023-04-03 13:07:03,485 - C:\\Users\\notebook\\castle\\algorithms\\__init__.py[line:36] - INFO: You are using ``pytorch`` as the backend.\n"
          ]
        }
      ],
      "source": [
        "# coding=utf-8\n",
        "# Copyright (C) 2022. Huawei Technologies Co., Ltd. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import logging\n",
        "import numpy as np\n",
        "\n",
        "from castle.common import BaseLearner, Tensor\n",
        "from castle.algorithms.gradient.gae.torch.trainers.al_trainer import ALTrainer\n",
        "from castle.algorithms.gradient.gae.torch.models.model import AutoEncoder\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Referred from:\n",
        "    - https://stackoverflow.com/questions/38469632/tensorflow-non-repeatable-results\n",
        "    \"\"\"\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    try:\n",
        "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "class GAE(BaseLearner):\n",
        "    \"\"\"\n",
        "    GAE Algorithm.\n",
        "    A gradient-based algorithm using graph autoencoder to model non-linear\n",
        "    causal relationships.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_dim: int, default: 1\n",
        "        dimension of vector for x\n",
        "    hidden_layers: int, default: 1\n",
        "        number of hidden layers for encoder and decoder\n",
        "    hidden_dim: int, default: 4\n",
        "        hidden size for mlp layer\n",
        "    activation: callable, default: nn.LeakyReLU(0.05)\n",
        "        nonlinear functional\n",
        "    epochs: int, default: 10\n",
        "        Number of iterations for optimization problem\n",
        "    update_freq: int, default: 3000\n",
        "        Number of steps for each iteration\n",
        "    init_iter: int, default: 3\n",
        "        Initial iteration to disallow early stopping\n",
        "    lr: float, default: 1e-3\n",
        "        learning rate\n",
        "    alpha: float, default: 0.0\n",
        "        Lagrange multiplier\n",
        "    beta: float, default: 2.0\n",
        "        Multiplication to amplify rho each time\n",
        "    init_rho: float, default: 1.0\n",
        "        Initial value for rho\n",
        "    rho_thresh: float, default: 1e30\n",
        "        Threshold for rho\n",
        "    gamma: float, default: 0.25\n",
        "        Threshold for h\n",
        "    penalty_lambda: float, default: 0.0\n",
        "        L1 penalty for sparse graph. Set to 0.0 to disable\n",
        "    h_thresh: float, default: 1e-8\n",
        "        Tolerance of optimization problem\n",
        "    graph_thresh: float, default: 0.3\n",
        "        Threshold to filter out small values in graph\n",
        "    early_stopping: bool, default: False\n",
        "        Whether to use early stopping\n",
        "    early_stopping_thresh: float, default: 1.0\n",
        "        Threshold ratio for early stopping\n",
        "    seed: int, default: 1230\n",
        "        Reproducibility, must be int\n",
        "    device_type: str, default: 'cpu'\n",
        "        'cpu' or 'gpu'\n",
        "    device_ids: int or str, default '0'\n",
        "        CUDA devices, it's effective when ``use_gpu`` is True.\n",
        "        For single-device modules, ``device_ids`` can be int or str,\n",
        "        e.g. 0 or '0', For multi-device modules, ``device_ids`` must be str,\n",
        "        format like '0, 1'.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_dim=1,\n",
        "                 hidden_layers=1,\n",
        "                 hidden_dim=4,\n",
        "                 activation=torch.nn.LeakyReLU(0.05),\n",
        "                 epochs=10,\n",
        "                 update_freq=3000,\n",
        "                 init_iter=3,\n",
        "                 lr=1e-3,\n",
        "                 alpha=0.0,\n",
        "                 beta=2.0,\n",
        "                 init_rho=1.0,\n",
        "                 rho_thresh=1e30,\n",
        "                 gamma=0.25,\n",
        "                 penalty_lambda=0.0,\n",
        "                 h_thresh=1e-8,\n",
        "                 graph_thresh=0.3,\n",
        "                 early_stopping=False,\n",
        "                 early_stopping_thresh=1.0,\n",
        "                 seed=1230,\n",
        "                 device_type='cpu',\n",
        "                 device_ids='0'):\n",
        "\n",
        "        super(GAE, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.activation = activation\n",
        "        self.epochs = epochs\n",
        "        self.update_freq = update_freq\n",
        "        self.init_iter = init_iter\n",
        "        self.lr = lr\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.init_rho = init_rho\n",
        "        self.rho_thresh = rho_thresh\n",
        "        self.gamma = gamma\n",
        "        self.penalty_lambda = penalty_lambda\n",
        "        self.h_thresh = h_thresh\n",
        "        self.graph_thresh = graph_thresh\n",
        "        self.early_stopping = early_stopping\n",
        "        self.early_stopping_thresh = early_stopping_thresh\n",
        "        self.seed = seed\n",
        "        self.device_type = device_type\n",
        "        self.device_ids = device_ids\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            logging.info('GPU is available.')\n",
        "        else:\n",
        "            logging.info('GPU is unavailable.')\n",
        "            if self.device_type == 'gpu':\n",
        "                raise ValueError(\"GPU is unavailable, \"\n",
        "                                 \"please set device_type = 'cpu'.\")\n",
        "        if self.device_type == 'gpu':\n",
        "            if self.device_ids:\n",
        "                os.environ['CUDA_VISIBLE_DEVICES'] = str(self.device_ids)\n",
        "            device = torch.device('cuda')\n",
        "        else:\n",
        "            device = torch.device('cpu')\n",
        "        self.device = device\n",
        "\n",
        "    def learn(self, data, columns=None, **kwargs):\n",
        "\n",
        "        x = torch.from_numpy(data)\n",
        "\n",
        "        self.n, self.d = x.shape[:2]\n",
        "        if x.ndim == 2:\n",
        "            x = x.reshape((self.n, self.d, 1))\n",
        "            self.input_dim = 1\n",
        "        elif x.ndim == 3:\n",
        "            self.input_dim = x.shape[2]\n",
        "\n",
        "        w_est = self._gae(x).detach().cpu().numpy()\n",
        "\n",
        "        self.weight_causal_matrix = Tensor(w_est,\n",
        "                                           index=columns,\n",
        "                                           columns=columns)\n",
        "        causal_matrix = (abs(w_est) > self.graph_thresh).astype(int)\n",
        "        self.causal_matrix = Tensor(causal_matrix,\n",
        "                                    index=columns,\n",
        "                                    columns=columns)\n",
        "\n",
        "    def _gae(self, x):\n",
        "\n",
        "        set_seed(self.seed)\n",
        "        model = AutoEncoder(d=self.d,\n",
        "                            input_dim=self.input_dim,\n",
        "                            hidden_layers=self.hidden_layers,\n",
        "                            hidden_dim=self.hidden_dim,\n",
        "                            activation=self.activation,\n",
        "                            device=self.device,\n",
        "                            )\n",
        "        trainer = ALTrainer(n=self.n,\n",
        "                            d=self.d,\n",
        "                            model=model,\n",
        "                            lr=self.lr,\n",
        "                            init_iter=self.init_iter,\n",
        "                            alpha=self.alpha,\n",
        "                            beta=self.beta,\n",
        "                            rho=self.init_rho,\n",
        "                            l1_penalty=self.penalty_lambda,\n",
        "                            rho_thresh=self.rho_thresh,\n",
        "                            h_thresh=self.h_thresh,  # 1e-8\n",
        "                            early_stopping=self.early_stopping,\n",
        "                            early_stopping_thresh=self.early_stopping_thresh,\n",
        "                            gamma=self.gamma,\n",
        "                            seed=self.seed,\n",
        "                            device=self.device)\n",
        "        w_est = trainer.train(x=x,\n",
        "                              epochs=self.epochs,\n",
        "                              update_freq=self.update_freq)\n",
        "        w_est = w_est / torch.max(abs(w_est))\n",
        "\n",
        "        return w_est\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1160d1a6",
      "metadata": {
        "id": "1160d1a6"
      },
      "source": [
        "# GAE_Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50f80f92",
      "metadata": {
        "id": "50f80f92",
        "outputId": "0f7cffcd-06c3-4939-c172-9d1d795c5134"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-03 13:10:18,139 - C:\\Users\\notebook\\castle\\datasets\\simulator.py[line:270] - INFO: Finished synthetic dataset\n",
            "2023-04-03 13:10:18,141 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\gae.py[line:146] - INFO: GPU is unavailable.\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "__init__() got an unexpected keyword argument 'device'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13436\\3928578374.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mga\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGAE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mga\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# plot est_dag and true_dag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\gae\\torch\\gae.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, data, columns, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mw_est\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         self.weight_causal_matrix = Tensor(w_est,\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\gae\\torch\\gae.py\u001b[0m in \u001b[0;36m_gae\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[0mset_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         model = AutoEncoder(d=self.d,\n\u001b[0m\u001b[0;32m    183\u001b[0m                             \u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                             \u001b[0mhidden_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\gae\\torch\\models\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, d, input_dim, hidden_layers, hidden_dim, activation, device)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         self.encoder = MLP(input_dim=self.input_dim,\n\u001b[0m\u001b[0;32m     78\u001b[0m                            \u001b[0mlayers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                            \u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\gae\\torch\\models\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dim, layers, units, output_dim, activation, device)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0minput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             weight = nn.Linear(in_features=input_size,\n\u001b[0m\u001b[0;32m     43\u001b[0m                                \u001b[0mout_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                                \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'device'"
          ]
        }
      ],
      "source": [
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "from castle.datasets import DAG, IIDSimulation\n",
        "from castle.algorithms import GAE\n",
        "\n",
        "\n",
        "#######################################\n",
        "# graph_auto_encoder used simulate data\n",
        "#######################################\n",
        "# simulate data for graph-auto-encoder\n",
        "weighted_random_dag = DAG.erdos_renyi(n_nodes=10, n_edges=20, weight_range=(0.5, 2.0), seed=1)\n",
        "dataset = IIDSimulation(W=weighted_random_dag, n=2000, method='linear', sem_type='gauss')\n",
        "true_dag, X = dataset.B, dataset.X\n",
        "\n",
        "ga = GAE(input_dim=10)\n",
        "ga.learn(X)\n",
        "\n",
        "# plot est_dag and true_dag\n",
        "GraphDAG(ga.causal_matrix, true_dag)\n",
        "\n",
        "# calculate accuracy\n",
        "met = MetricsDAG(ga.causal_matrix, true_dag)\n",
        "print(met.metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b349c7bf",
      "metadata": {
        "id": "b349c7bf"
      },
      "source": [
        "# RL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77d87562",
      "metadata": {
        "id": "77d87562"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# Copyright (C) 2021. Huawei Technologies Co., Ltd. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import os\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import platform\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from castle.algorithms.gradient.rl.torch.data_loader import DataGenerator_read_data\n",
        "from castle.algorithms.gradient.rl.torch.models import Actor\n",
        "from castle.algorithms.gradient.rl.torch.rewards import get_Reward\n",
        "from castle.algorithms.gradient.rl.torch.helpers.torch_utils import set_seed\n",
        "from castle.algorithms.gradient.rl.torch.helpers.lambda_utils import BIC_lambdas\n",
        "from castle.algorithms.gradient.rl.torch.helpers.analyze_utils import convert_graph_int_to_adj_mat,graph_prunned_by_coef, graph_prunned_by_coef_2nd\n",
        "\n",
        "from castle.common import BaseLearner, Tensor\n",
        "from castle.metrics import MetricsDAG\n",
        "from castle.common.consts import RL_VALID_PARAMS\n",
        "from castle.common.validator import check_args_value\n",
        "\n",
        "\n",
        "class RL(BaseLearner):\n",
        "    \"\"\"\n",
        "    RL Algorithm.\n",
        "    A RL-based algorithm that can work with flexible score functions (including non-smooth ones).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    encoder_type: str\n",
        "        type of encoder used\n",
        "    hidden_dim: int\n",
        "        actor LSTM num_neurons\n",
        "    num_heads: int\n",
        "        actor input embedding\n",
        "    num_stacks: int\n",
        "        actor LSTM num_neurons\n",
        "    residual: bool\n",
        "        whether to use residual for gat encoder\n",
        "    decoder_type: str\n",
        "        type of decoder used\n",
        "    decoder_activation: str\n",
        "        activation for decoder\n",
        "    decoder_hidden_dim: int\n",
        "        hidden dimension for decoder\n",
        "    use_bias: bool\n",
        "        Whether to add bias term when calculating decoder logits\n",
        "    use_bias_constant: bool\n",
        "        Whether to add bias term as CONSTANT when calculating decoder logits\n",
        "    bias_initial_value: float\n",
        "        Initial value for bias term when calculating decoder logits\n",
        "    batch_size: int\n",
        "        batch size for training\n",
        "    input_dimension: int\n",
        "        dimension of reshaped vector\n",
        "    normalize: bool\n",
        "        whether the inputdata shall be normalized\n",
        "    transpose: bool\n",
        "        whether the true graph needs transposed\n",
        "    score_type: str\n",
        "        score functions\n",
        "    reg_type: str\n",
        "        regressor type (in combination wth score_type)\n",
        "    lambda_iter_num: int\n",
        "        how often to update lambdas\n",
        "    lambda_flag_default: bool\n",
        "        with set lambda parameters; true with default strategy and ignore input bounds\n",
        "    score_bd_tight: bool\n",
        "        if bound is tight, then simply use a fixed value, rather than the adaptive one\n",
        "    lambda1_update: float\n",
        "        increasing additive lambda1\n",
        "    lambda2_update: float\n",
        "        increasing  multiplying lambda2\n",
        "    score_lower: float\n",
        "        lower bound on lambda1\n",
        "    score_upper: float\n",
        "        upper bound on lambda1\n",
        "    lambda2_lower: float\n",
        "        lower bound on lambda2\n",
        "    lambda2_upper: float\n",
        "        upper bound on lambda2\n",
        "    seed: int\n",
        "        seed\n",
        "    nb_epoch: int\n",
        "        nb epoch\n",
        "    lr1_start: float\n",
        "        actor learning rate\n",
        "    lr1_decay_step: int\n",
        "        lr1 decay step\n",
        "    lr1_decay_rate: float\n",
        "        lr1 decay rate\n",
        "    alpha: float\n",
        "        update factor moving average baseline\n",
        "    init_baseline: float\n",
        "        initial baseline - REINFORCE\n",
        "    temperature: float\n",
        "        pointer_net initial temperature\n",
        "    C: float\n",
        "        pointer_net tan clipping\n",
        "    l1_graph_reg: float\n",
        "        L1 graph regularization to encourage sparsity\n",
        "    inference_mode: bool\n",
        "        switch to inference mode when model is trained\n",
        "    verbose: bool\n",
        "        print detailed logging or not\n",
        "    device_type: str\n",
        "        whether to use GPU or not\n",
        "    device_ids: int\n",
        "        choose which gpu to use\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    causal_matrix : numpy.ndarray\n",
        "        Learned causal structure matrix\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    https://arxiv.org/abs/1906.04477\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from castle.algorithms import RL\n",
        "    >>> from castle.datasets import load_dataset\n",
        "    >>> from castle.common import GraphDAG\n",
        "    >>> from castle.metrics import MetricsDAG\n",
        "    >>> X, true_dag, _ = load_dataset('IID_Test')\n",
        "    >>> n = RL()\n",
        "    >>> n.learn(X, dag=true_dag)\n",
        "    >>> GraphDAG(n.causal_matrix, true_dag)\n",
        "    >>> met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "    >>> print(met.metrics)\n",
        "    \"\"\"\n",
        "\n",
        "    @check_args_value(RL_VALID_PARAMS)\n",
        "    def __init__(self, encoder_type='TransformerEncoder',\n",
        "                 hidden_dim=64,\n",
        "                 num_heads=16,\n",
        "                 num_stacks=6,\n",
        "                 residual=False,\n",
        "                 decoder_type='SingleLayerDecoder',\n",
        "                 decoder_activation='tanh',\n",
        "                 decoder_hidden_dim=16,\n",
        "                 use_bias=False,\n",
        "                 use_bias_constant=False,\n",
        "                 bias_initial_value=False,\n",
        "                 batch_size=64,\n",
        "                 input_dimension=64,\n",
        "                 normalize=False,\n",
        "                 transpose=False,\n",
        "                 score_type='BIC',\n",
        "                 reg_type='LR',\n",
        "                 lambda_iter_num=1000,\n",
        "                 lambda_flag_default=True,\n",
        "                 score_bd_tight=False,\n",
        "                 lambda2_update=10,\n",
        "                 score_lower=0.0,\n",
        "                 score_upper=0.0,\n",
        "                 seed=8,\n",
        "                 nb_epoch=20000,\n",
        "                 lr1_start=0.001,\n",
        "                 lr1_decay_step=5000,\n",
        "                 lr1_decay_rate=0.96,\n",
        "                 alpha=0.99,\n",
        "                 init_baseline=-1.0,\n",
        "                 l1_graph_reg=0.0,\n",
        "                 verbose=False,\n",
        "                 device_type='cpu',\n",
        "                 device_ids=0):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder_type = encoder_type\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_stacks = num_stacks\n",
        "        self.residual = residual\n",
        "        self.decoder_type = decoder_type\n",
        "        self.decoder_activation = decoder_activation\n",
        "        self.decoder_hidden_dim = decoder_hidden_dim\n",
        "        self.use_bias = use_bias\n",
        "        self.use_bias_constant = use_bias_constant\n",
        "        self.bias_initial_value = bias_initial_value\n",
        "        self.batch_size = batch_size\n",
        "        self.input_dimension = input_dimension\n",
        "        self.normalize = normalize\n",
        "        self.transpose = transpose\n",
        "        self.score_type = score_type\n",
        "        self.reg_type = reg_type\n",
        "        self.lambda_iter_num = lambda_iter_num\n",
        "        self.lambda_flag_default = lambda_flag_default\n",
        "        self.score_bd_tight = score_bd_tight\n",
        "        self.lambda2_update = lambda2_update\n",
        "        self.score_lower = score_lower\n",
        "        self.score_upper = score_upper\n",
        "        self.seed = seed\n",
        "        self.nb_epoch = nb_epoch\n",
        "        self.lr1_start = lr1_start\n",
        "        self.lr1_decay_step = lr1_decay_step\n",
        "        self.lr1_decay_rate = lr1_decay_rate\n",
        "        self.alpha = alpha\n",
        "        self.init_baseline = init_baseline\n",
        "        self.l1_graph_reg = l1_graph_reg\n",
        "        self.verbose = verbose\n",
        "        self.device_type = device_type\n",
        "        self.device_ids = device_ids\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            logging.info('GPU is available.')\n",
        "        else:\n",
        "            logging.info('GPU is unavailable.')\n",
        "            if self.device_type == 'gpu':\n",
        "                raise ValueError(\"GPU is unavailable, \"\n",
        "                                 \"please set device_type = 'cpu'.\")\n",
        "        if self.device_type == 'gpu':\n",
        "            if self.device_ids:\n",
        "                os.environ['CUDA_VISIBLE_DEVICES'] = str(self.device_ids)\n",
        "            device = torch.device('cuda')\n",
        "        else:\n",
        "            device = torch.device('cpu')\n",
        "        self.device = device\n",
        "\n",
        "    def learn(self, data, columns=None, dag=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Set up and run the RL algorithm.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: castle.Tensor or numpy.ndarray\n",
        "            The castle.Tensor or numpy.ndarray format data you want to learn.\n",
        "        columns : Index or array-like\n",
        "            Column labels to use for resulting tensor. Will default to\n",
        "            RangeIndex (0, 1, 2, ..., n) if no column labels are provided.\n",
        "        dag : ndarray\n",
        "            two-dimensional, prior matrix\n",
        "        \"\"\"\n",
        "\n",
        "        self.dag = dag\n",
        "        X = Tensor(data, columns=columns)\n",
        "\n",
        "        self.data_size = X.shape[0]\n",
        "        self.max_length = X.shape[1]\n",
        "\n",
        "        causal_matrix = self._rl(X)\n",
        "        self.causal_matrix = causal_matrix\n",
        "\n",
        "    def _rl(self, X):\n",
        "        # Reproducibility\n",
        "        set_seed(self.seed)\n",
        "\n",
        "        logging.info('Python version is {}'.format(platform.python_version()))\n",
        "\n",
        "        # input data\n",
        "        if self.dag :\n",
        "            training_set = DataGenerator_read_data(\n",
        "                X, self.dag, self.normalize, self.transpose)\n",
        "        else:\n",
        "            training_set = DataGenerator_read_data(\n",
        "                X, None, self.normalize, self.transpose)\n",
        "\n",
        "        # set penalty weights\n",
        "        score_type = self.score_type\n",
        "        reg_type = self.reg_type\n",
        "\n",
        "        if self.lambda_flag_default:\n",
        "            sl, su, strue = BIC_lambdas(training_set.inputdata, None, None, None, reg_type, score_type)\n",
        "            lambda1 = 0\n",
        "            lambda1_upper = 5\n",
        "            lambda1_update_add = 1\n",
        "            lambda2 = 1/(10**(np.round(self.max_length/3)))\n",
        "            lambda2_upper = 0.01\n",
        "            lambda2_update_mul = 10\n",
        "            lambda_iter_num = self.lambda_iter_num\n",
        "\n",
        "            # test initialized score\n",
        "            logging.info('Original sl: {}, su: {}, strue: {}'.format(sl, su, strue))\n",
        "            logging.info('Transfomed sl: {}, su: {}, lambda2: {}, true: {}'.format(sl, su, lambda2,\n",
        "                        (strue-sl)/(su-sl)*lambda1_upper))\n",
        "        else:\n",
        "            # test choices for the case with mannualy provided bounds\n",
        "            # not fully tested\n",
        "            sl = self.score_lower\n",
        "            su = self.score_upper\n",
        "            if self.score_bd_tight:\n",
        "                lambda1 = 2\n",
        "                lambda1_upper = 2\n",
        "            else:\n",
        "                lambda1 = 0\n",
        "                lambda1_upper = 5\n",
        "                lambda1_update_add = 1\n",
        "            lambda2 = 1/(10**(np.round(self.max_length/3)))\n",
        "            lambda2_upper = 0.01\n",
        "            lambda2_update_mul = self.lambda2_update\n",
        "            lambda_iter_num = self.lambda_iter_num\n",
        "\n",
        "        # actor\n",
        "        actor = Actor(encoder_type=self.encoder_type,\n",
        "                      hidden_dim=self.hidden_dim,\n",
        "                      max_length=self.max_length,\n",
        "                      num_heads=self.num_heads,\n",
        "                      num_stacks=self.num_stacks,\n",
        "                      residual=self.residual,\n",
        "                      decoder_type=self.decoder_type,\n",
        "                      decoder_activation=self.decoder_activation,\n",
        "                      decoder_hidden_dim=self.decoder_hidden_dim,\n",
        "                      use_bias=self.use_bias,\n",
        "                      use_bias_constant=self.use_bias_constant,\n",
        "                      bias_initial_value=self.bias_initial_value,\n",
        "                      batch_size=self.batch_size,\n",
        "                      input_dimension=self.input_dimension,\n",
        "                      lr1_start=self.lr1_start,\n",
        "                      lr1_decay_step=self.lr1_decay_step,\n",
        "                      lr1_decay_rate=self.lr1_decay_rate,\n",
        "                      alpha=self.alpha,\n",
        "                      init_baseline=self.init_baseline,\n",
        "                      device=self.device)\n",
        "        callreward = get_Reward(self.batch_size, self.max_length,\n",
        "                                self.input_dimension, training_set.inputdata,\n",
        "                                sl, su, lambda1_upper, score_type, reg_type,\n",
        "                                self.l1_graph_reg, False)\n",
        "        logging.info('Finished creating training dataset and reward class')\n",
        "\n",
        "        # Initialize useful variables\n",
        "        rewards_avg_baseline = []\n",
        "        rewards_batches = []\n",
        "        reward_max_per_batch = []\n",
        "\n",
        "        lambda1s = []\n",
        "        lambda2s = []\n",
        "\n",
        "        graphss = []\n",
        "        probsss = []\n",
        "        max_rewards = []\n",
        "        max_reward = float('-inf')\n",
        "        max_reward_score_cyc = (lambda1_upper+1, 0)\n",
        "\n",
        "        logging.info('Starting training.')\n",
        "\n",
        "        for i in tqdm(range(1, self.nb_epoch + 1)):\n",
        "\n",
        "            if self.verbose:\n",
        "                logging.info('Start training for {}-th epoch'.format(i))\n",
        "\n",
        "            input_batch = training_set.train_batch(self.batch_size, self.max_length, self.input_dimension)\n",
        "            inputs = torch.from_numpy(np.array(input_batch)).to(self.device)\n",
        "\n",
        "            # Test tensor shape\n",
        "            if i == 1:\n",
        "                logging.info('Shape of actor.input: {}'.format(inputs.shape))\n",
        "\n",
        "            # actor\n",
        "            actor.build_permutation(inputs)\n",
        "            graphs_feed = actor.graphs_\n",
        "\n",
        "            reward_feed = callreward.cal_rewards(graphs_feed.cpu().detach().numpy(), lambda1, lambda2)  # np.array\n",
        "            actor.build_reward(reward_ = -torch.from_numpy(reward_feed)[:,0].to(self.device))\n",
        "\n",
        "\n",
        "            # max reward, max reward per batch\n",
        "            max_reward = -callreward.update_scores([max_reward_score_cyc], lambda1, lambda2)[0]\n",
        "            max_reward_batch = float('inf')\n",
        "            max_reward_batch_score_cyc = (0, 0)\n",
        "\n",
        "            for reward_, score_, cyc_ in reward_feed:\n",
        "                if reward_ < max_reward_batch:\n",
        "                    max_reward_batch = reward_\n",
        "                    max_reward_batch_score_cyc = (score_, cyc_)\n",
        "\n",
        "            max_reward_batch = -max_reward_batch\n",
        "\n",
        "            if max_reward < max_reward_batch:\n",
        "                max_reward = max_reward_batch\n",
        "                max_reward_score_cyc = max_reward_batch_score_cyc\n",
        "\n",
        "            # for average reward per batch\n",
        "            reward_batch_score_cyc = np.mean(reward_feed[:,1:], axis=0)\n",
        "\n",
        "            if self.verbose:\n",
        "                logging.info('Finish calculating reward for current batch of graph')\n",
        "\n",
        "            score_test, probs, graph_batch, \\\n",
        "            reward_batch, reward_avg_baseline = \\\n",
        "                    actor.test_scores, actor.log_softmax, actor.graph_batch, \\\n",
        "                    actor.reward_batch, actor.avg_baseline\n",
        "\n",
        "            if self.verbose:\n",
        "                logging.info('Finish updating actor and critic network using reward calculated')\n",
        "\n",
        "            lambda1s.append(lambda1)\n",
        "            lambda2s.append(lambda2)\n",
        "\n",
        "            rewards_avg_baseline.append(reward_avg_baseline)\n",
        "            rewards_batches.append(reward_batch_score_cyc)\n",
        "            reward_max_per_batch.append(max_reward_batch_score_cyc)\n",
        "\n",
        "            graphss.append(graph_batch)\n",
        "            probsss.append(probs)\n",
        "            max_rewards.append(max_reward_score_cyc)\n",
        "\n",
        "            # logging\n",
        "            if i == 1 or i % 500 == 0:\n",
        "                logging.info('[iter {}] reward_batch: {:.4}, max_reward: {:.4}, max_reward_batch: {:.4}'.format(i,\n",
        "                            reward_batch, max_reward, max_reward_batch))\n",
        "\n",
        "            # update lambda1, lamda2\n",
        "            if i == 1 or i % lambda_iter_num == 0:\n",
        "                ls_kv = callreward.update_all_scores(lambda1, lambda2)\n",
        "\n",
        "                graph_int, score_min, cyc_min = np.int64(ls_kv[0][0]), ls_kv[0][1][1], ls_kv[0][1][-1]\n",
        "\n",
        "                if cyc_min < 1e-5:\n",
        "                    lambda1_upper = score_min\n",
        "                lambda1 = min(lambda1+lambda1_update_add, lambda1_upper)\n",
        "                lambda2 = min(lambda2*lambda2_update_mul, lambda2_upper)\n",
        "                logging.info('[iter {}] lambda1 {:.4}, upper {:.4}, lambda2 {:.4}, upper {:.4}, score_min {:.4}, cyc_min {:.4}'.format(i,\n",
        "                            lambda1*1.0, lambda1_upper*1.0, lambda2*1.0, lambda2_upper*1.0, score_min*1.0, cyc_min*1.0))\n",
        "\n",
        "                graph_batch = convert_graph_int_to_adj_mat(graph_int)\n",
        "\n",
        "                if reg_type == 'LR':\n",
        "                    graph_batch_pruned = np.array(graph_prunned_by_coef(graph_batch, training_set.inputdata))\n",
        "                elif reg_type == 'QR':\n",
        "                    graph_batch_pruned = np.array(graph_prunned_by_coef_2nd(graph_batch, training_set.inputdata))\n",
        "\n",
        "                if self.dag:\n",
        "                    met = MetricsDAG(graph_batch.T, training_set.true_graph)\n",
        "                    met2 = MetricsDAG(graph_batch_pruned.T, training_set.true_graph)\n",
        "                    acc_est = met.metrics\n",
        "                    acc_est2 = met2.metrics\n",
        "\n",
        "                    fdr, tpr, fpr, shd, nnz = \\\n",
        "                        acc_est['fdr'], acc_est['tpr'], acc_est['fpr'], \\\n",
        "                        acc_est['shd'], acc_est['nnz']\n",
        "                    fdr2, tpr2, fpr2, shd2, nnz2 = \\\n",
        "                        acc_est2['fdr'], acc_est2['tpr'], acc_est2['fpr'], \\\n",
        "                        acc_est2['shd'], acc_est2['nnz']\n",
        "\n",
        "                    logging.info('before pruning: fdr {}, tpr {}, fpr {}, shd {}, nnz {}'.format(fdr, tpr, fpr, shd, nnz))\n",
        "                    logging.info('after  pruning: fdr {}, tpr {}, fpr {}, shd {}, nnz {}'.format(fdr2, tpr2, fpr2, shd2, nnz2))\n",
        "\n",
        "        logging.info('Training COMPLETED !')\n",
        "\n",
        "        return graph_batch_pruned.T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b8a754b",
      "metadata": {
        "id": "6b8a754b"
      },
      "source": [
        "# RL_Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5cf703d",
      "metadata": {
        "id": "d5cf703d",
        "outputId": "2ff09558-5237-4fcc-f65b-77843ba003d2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-03 13:11:53,290 - C:\\Users\\notebook\\castle\\datasets\\simulator.py[line:270] - INFO: Finished synthetic dataset\n",
            "2023-04-03 13:11:53,291 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\rl\\torch\\rl.py[line:224] - INFO: GPU is unavailable.\n",
            "2023-04-03 13:11:53,300 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\rl\\torch\\rl.py[line:264] - INFO: Python version is 3.9.13\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13436\\2373037575.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_dag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'IID_Test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrue_dag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mGraphDAG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcausal_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_dag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMetricsDAG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcausal_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_dag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\rl\\torch\\rl.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, data, columns, dag, **kwargs)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[0mcausal_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcausal_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcausal_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\rl\\torch\\rl.py\u001b[0m in \u001b[0;36m_rl\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[1;31m# input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdag\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m             training_set = DataGenerator_read_data(\n\u001b[0;32m    269\u001b[0m                 X, self.dag, self.normalize, self.transpose)\n",
            "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ],
      "source": [
        "from castle.algorithms import RL\n",
        "from castle.datasets import load_dataset\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "X, true_dag, _ = load_dataset('IID_Test')\n",
        "n = RL()\n",
        "n.learn(X, dag=true_dag)\n",
        "GraphDAG(n.causal_matrix, true_dag)\n",
        "met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "print(met.metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e52c158",
      "metadata": {
        "id": "0e52c158"
      },
      "source": [
        "# CORL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f8d43b6",
      "metadata": {
        "id": "5f8d43b6"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# Copyright (C) 2021. Huawei Technologies Co., Ltd. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import platform\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from castle.common import BaseLearner, Tensor, consts\n",
        "from castle.algorithms.gradient.corl.torch.frame import Actor, EpisodicCritic, Reward, DenseCritic\n",
        "from castle.algorithms.gradient.corl.torch.frame import score_function as Score_Func\n",
        "from castle.algorithms.gradient.corl.torch.utils.data_loader import DataGenerator\n",
        "from castle.algorithms.gradient.corl.torch.utils.graph_analysis import get_graph_from_order, pruning_by_coef\n",
        "from castle.algorithms.gradient.corl.torch.utils.graph_analysis import pruning_by_coef_2nd\n",
        "from castle.common.validator import check_args_value\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Referred from:\n",
        "    - https://stackoverflow.com/questions/38469632/tensorflow-non-repeatable-results\n",
        "    \"\"\"\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    try:\n",
        "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "class CORL(BaseLearner):\n",
        "    \"\"\"\n",
        "    Causal discovery with Ordering-based Reinforcement Learning\n",
        "\n",
        "    A RL- and order-based algorithm that improves the efficiency and scalability\n",
        "    of previous RL-based approach, contains CORL1 with ``episodic`` reward type\n",
        "    and CORL2 with ``dense`` reward type``.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    https://arxiv.org/abs/2105.06631\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    batch_size: int, default: 64\n",
        "        training batch size\n",
        "    input_dim: int, default: 64\n",
        "        dimension of input data\n",
        "    embed_dim: int, default: 256\n",
        "        dimension of embedding layer output\n",
        "    normalize: bool, default: False\n",
        "        whether normalization for input data\n",
        "    encoder_name: str, default: 'transformer'\n",
        "        Encoder name, must be one of ['transformer', 'lstm', 'mlp']\n",
        "    encoder_heads: int, default: 8\n",
        "        number of multi-head of `transformer` Encoder.\n",
        "    encoder_blocks: int, default: 3\n",
        "        blocks number of Encoder\n",
        "    encoder_dropout_rate: float, default: 0.1\n",
        "        dropout rate for encoder\n",
        "    decoder_name: str, default: 'lstm'\n",
        "        Decoder name, must be one of ['lstm', 'mlp']\n",
        "    reward_mode: str, default: 'episodic'\n",
        "        reward mode, 'episodic' or 'dense',\n",
        "        'episodic' denotes ``episodic-reward``, 'dense' denotes ``dense-reward``.\n",
        "    reward_score_type: str, default: 'BIC'\n",
        "        type of score function\n",
        "    reward_regression_type: str, default: 'LR'\n",
        "        type of regression function, must be one of ['LR', 'QR']\n",
        "    reward_gpr_alpha: float, default: 1.0\n",
        "        alpha of GPR\n",
        "    iteration: int, default: 5000\n",
        "        training times\n",
        "    actor_lr: float, default: 1e-4\n",
        "        learning rate of Actor network, includes ``encoder`` and ``decoder``.\n",
        "    critic_lr: float, default: 1e-3\n",
        "        learning rate of Critic network\n",
        "    alpha: float, default: 0.99\n",
        "        alpha for score function, includes ``dense_actor_loss`` and\n",
        "        ``dense_critic_loss``.\n",
        "    init_baseline: float, default: -1.0\n",
        "        initilization baseline for score function, includes ``dense_actor_loss``\n",
        "        and ``dense_critic_loss``.\n",
        "    random_seed: int, default: 0\n",
        "        random seed for all random process\n",
        "    device_type: str, default: cpu\n",
        "        ``cpu`` or ``gpu``\n",
        "    device_ids: int or str, default None\n",
        "        CUDA devices, it's effective when ``use_gpu`` is True.\n",
        "        For single-device modules, ``device_ids`` can be int or str, e.g. 0 or '0',\n",
        "        For multi-device modules, ``device_ids`` must be str, format like '0, 1'.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from castle.algorithms.gradient.corl.torch import CORL\n",
        "    >>> from castle.datasets import load_dataset\n",
        "    >>> from castle.common import GraphDAG\n",
        "    >>> from castle.metrics import MetricsDAG\n",
        "    >>> X, true_dag, _ = load_dataset('IID_Test')\n",
        "    >>> n = CORL()\n",
        "    >>> n.learn(X)\n",
        "    >>> GraphDAG(n.causal_matrix, true_dag)\n",
        "    >>> met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "    >>> print(met.metrics)\n",
        "    \"\"\"\n",
        "\n",
        "    @check_args_value(consts.CORL_VALID_PARAMS)\n",
        "    def __init__(self, batch_size=64, input_dim=100, embed_dim=256,\n",
        "                 normalize=False,\n",
        "                 encoder_name='transformer',\n",
        "                 encoder_heads=8,\n",
        "                 encoder_blocks=3,\n",
        "                 encoder_dropout_rate=0.1,\n",
        "                 decoder_name='lstm',\n",
        "                 reward_mode='episodic',\n",
        "                 reward_score_type='BIC',\n",
        "                 reward_regression_type='LR',\n",
        "                 reward_gpr_alpha=1.0,\n",
        "                 iteration=10000,\n",
        "                 lambda_iter_num=500,\n",
        "                 actor_lr=1e-4,\n",
        "                 critic_lr=1e-3,\n",
        "                 alpha=0.99,  # for score function\n",
        "                 init_baseline=-1.0,\n",
        "                 random_seed=0,\n",
        "                 device_type='cpu',\n",
        "                 device_ids=0\n",
        "                 ):\n",
        "        super(CORL, self).__init__()\n",
        "        self.batch_size             = batch_size\n",
        "        self.input_dim              = input_dim\n",
        "        self.embed_dim              = embed_dim\n",
        "        self.normalize              = normalize\n",
        "        self.encoder_name           = encoder_name\n",
        "        self.encoder_heads          = encoder_heads\n",
        "        self.encoder_blocks         = encoder_blocks\n",
        "        self.encoder_dropout_rate   = encoder_dropout_rate\n",
        "        self.decoder_name           = decoder_name\n",
        "        self.reward_mode            = reward_mode\n",
        "        self.reward_score_type      = reward_score_type\n",
        "        self.reward_regression_type = reward_regression_type\n",
        "        self.reward_gpr_alpha       = reward_gpr_alpha\n",
        "        self.iteration              = iteration\n",
        "        self.lambda_iter_num        = lambda_iter_num\n",
        "        self.actor_lr               = actor_lr\n",
        "        self.critic_lr              = critic_lr\n",
        "        self.alpha                  = alpha\n",
        "        self.init_baseline          = init_baseline\n",
        "        self.random_seed            = random_seed\n",
        "        self.device_type            = device_type\n",
        "        self.device_ids             = device_ids\n",
        "        if reward_mode == 'dense':\n",
        "            self.avg_baseline = torch.tensor(init_baseline, requires_grad=False)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            logging.info('GPU is available.')\n",
        "        else:\n",
        "            logging.info('GPU is unavailable.')\n",
        "            if self.device_type == 'gpu':\n",
        "                raise ValueError(\"GPU is unavailable, \"\n",
        "                                 \"please set device_type = 'cpu'.\")\n",
        "        if self.device_type == 'gpu':\n",
        "            if self.device_ids:\n",
        "                os.environ['CUDA_VISIBLE_DEVICES'] = str(self.device_ids)\n",
        "            device = torch.device('cuda')\n",
        "        else:\n",
        "            device = torch.device('cpu')\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "    def learn(self, data, columns=None, **kwargs) -> None:\n",
        "        \"\"\"\n",
        "        Set up and run the Causal discovery with Ordering-based Reinforcement\n",
        "        Learning algorithm.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: castle.Tensor or numpy.ndarray\n",
        "            The castle.Tensor or numpy.ndarray format data you want to learn.\n",
        "        columns : Index or array-like\n",
        "            Column labels to use for resulting tensor. Will default to\n",
        "            RangeIndex (0, 1, 2, ..., n) if no column labels are provided.\n",
        "        Other Parameters:\n",
        "            dag_mask : ndarray\n",
        "                two-dimensional array with [0, 1], shape = [n_nodes, n_nodes].\n",
        "                (i, j) indicated element `0` denotes there must be no edge\n",
        "                between nodes `i` and `j` , the element `1` indicates that\n",
        "                there may or may not be an edge.\n",
        "        \"\"\"\n",
        "\n",
        "        X = Tensor(data, columns=columns)\n",
        "        self.n_samples = X.shape[0]\n",
        "        self.seq_length = X.shape[1] # seq_length == n_nodes\n",
        "        if X.shape[1] > self.batch_size:\n",
        "            raise ValueError(f'The `batch_size` must greater than or equal to '\n",
        "                             f'`n_nodes`, but got '\n",
        "                             f'batch_size: {self.batch_size}, '\n",
        "                             f'n_nodes: {self.seq_length}.')\n",
        "        self.dag_mask = getattr(kwargs, 'dag_mask', None)\n",
        "        causal_matrix = self._rl_search(X)\n",
        "        self.causal_matrix = Tensor(causal_matrix,\n",
        "                                    index=X.columns,\n",
        "                                    columns=X.columns)\n",
        "\n",
        "    def _rl_search(self, X) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Search DAG with ordering-based reinforcement learning\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray\n",
        "            The numpy.ndarray format data you want to learn.\n",
        "        \"\"\"\n",
        "\n",
        "        set_seed(self.random_seed)\n",
        "        logging.info('Python version is {}'.format(platform.python_version()))\n",
        "\n",
        "        # generate observed data\n",
        "        data_generator = DataGenerator(dataset=X,\n",
        "                                       normalize=self.normalize,\n",
        "                                       device=self.device)\n",
        "        # Instantiating an Actor\n",
        "        actor = Actor(input_dim=self.input_dim,\n",
        "                      embed_dim=self.embed_dim,\n",
        "                      encoder_blocks=self.encoder_blocks,\n",
        "                      encoder_heads=self.encoder_heads,\n",
        "                      encoder_name=self.encoder_name,\n",
        "                      decoder_name=self.decoder_name,\n",
        "                      device=self.device)\n",
        "        # Instantiating an Critic\n",
        "        if self.reward_mode == 'episodic':\n",
        "            critic = EpisodicCritic(input_dim=self.embed_dim,\n",
        "                                    device=self.device)\n",
        "        else:\n",
        "            critic = DenseCritic(input_dim=self.embed_dim,\n",
        "                                 output_dim=self.embed_dim,\n",
        "                                 device=self.device)\n",
        "        # Instantiating an Reward\n",
        "        reward =Reward(input_data=data_generator.dataset.cpu().detach().numpy(),\n",
        "                       reward_mode=self.reward_mode,\n",
        "                       score_type=self.reward_score_type,\n",
        "                       regression_type=self.reward_regression_type,\n",
        "                       alpha=self.reward_gpr_alpha)\n",
        "        # Instantiating an Optimizer\n",
        "        optimizer = torch.optim.Adam([\n",
        "            {\n",
        "                'params': actor.encoder.parameters(), 'lr': self.actor_lr\n",
        "            },\n",
        "            {\n",
        "                'params': actor.decoder.parameters(), 'lr': self.actor_lr\n",
        "            },\n",
        "            {\n",
        "                'params': critic.parameters(), 'lr': self.critic_lr\n",
        "            }\n",
        "        ])\n",
        "\n",
        "        # initial max_reward\n",
        "        max_reward = float('-inf')\n",
        "\n",
        "        logging.info(f'Shape of input batch: {self.batch_size}, '\n",
        "                     f'{self.seq_length}, {self.input_dim}')\n",
        "        logging.info(f'Shape of input batch: {self.batch_size}, '\n",
        "                     f'{self.seq_length}, {self.embed_dim}')\n",
        "        logging.info('Starting training.')\n",
        "\n",
        "        graph_batch_pruned = Tensor(np.ones((self.seq_length,\n",
        "                                             self.seq_length)) -\n",
        "                                            np.eye(self.seq_length))\n",
        "        for i in tqdm(range(1, self.iteration + 1)):\n",
        "            # generate one batch input\n",
        "            input_batch = data_generator.draw_batch(batch_size=self.batch_size,\n",
        "                                                    dimension=self.input_dim)\n",
        "            # (batch_size, n_nodes, input_dim)\n",
        "            encoder_output = actor.encode(input=input_batch)\n",
        "            decoder_output = actor.decode(input=encoder_output)\n",
        "            actions, mask_scores, s_list, h_list, c_list = decoder_output\n",
        "\n",
        "            batch_graphs = []\n",
        "            action_mask_s = []\n",
        "            for m in range(actions.shape[0]):\n",
        "                zero_matrix = get_graph_from_order(actions[m].cpu())\n",
        "                action_mask = np.zeros(zero_matrix.shape[0])\n",
        "                for act in actions[m]:\n",
        "                    action_mask_s.append(action_mask.copy())\n",
        "                    action_mask += np.eye(zero_matrix.shape[0])[act]\n",
        "                batch_graphs.append(zero_matrix)\n",
        "            batch_graphs = np.stack(batch_graphs) # 64*10*10\n",
        "            action_mask_s = np.stack(action_mask_s)\n",
        "\n",
        "            # Reward\n",
        "            reward_output = reward.cal_rewards(batch_graphs, actions.cpu())\n",
        "            reward_list, normal_batch_reward, max_reward_batch, td_target = reward_output\n",
        "\n",
        "            if max_reward < max_reward_batch:\n",
        "                max_reward = max_reward_batch\n",
        "\n",
        "            # Critic\n",
        "            prev_input = s_list.reshape((-1, self.embed_dim))\n",
        "            prev_state_0 = h_list.reshape((-1, self.embed_dim))\n",
        "            prev_state_1 = c_list.reshape((-1, self.embed_dim))\n",
        "\n",
        "            action_mask_ =  action_mask_s.reshape((-1, self.seq_length))\n",
        "            log_softmax = actor.decoder.log_softmax(input=prev_input,\n",
        "                                                    position=actions,\n",
        "                                                    mask=action_mask_,\n",
        "                                                    state_0=prev_state_0,\n",
        "                                                    state_1=prev_state_1)\n",
        "            log_softmax = log_softmax.reshape((self.batch_size,\n",
        "                                               self.seq_length)).T\n",
        "            if self.reward_mode == 'episodic':\n",
        "                critic.predict_env(stats_x=s_list[:, :-1, :])\n",
        "                critic.predict_tgt(stats_y=s_list[:, 1:, :])\n",
        "                critic.soft_replacement()\n",
        "                td_target = td_target[::-1][:-1]\n",
        "\n",
        "                actor_loss = Score_Func.episodic_actor_loss(\n",
        "                    td_target=torch.tensor(td_target),\n",
        "                    prediction_env=critic.prediction_env,\n",
        "                    log_softmax=log_softmax,\n",
        "                    device=self.device\n",
        "                )\n",
        "                critic_loss = Score_Func.episodic_critic_loss(\n",
        "                    td_target=torch.tensor(td_target),\n",
        "                    prediction_env=critic.prediction_env,\n",
        "                    device=self.device\n",
        "                )\n",
        "            elif self.reward_mode == 'dense':\n",
        "                log_softmax = torch.sum(log_softmax, 0)\n",
        "                reward_mean = np.mean(normal_batch_reward)\n",
        "                self.avg_baseline = self.alpha * self.avg_baseline + \\\n",
        "                                    (1.0 - self.alpha) * reward_mean\n",
        "                predict_reward = critic.predict_reward(encoder_output=encoder_output)\n",
        "\n",
        "                actor_loss = Score_Func.dense_actor_loss(normal_batch_reward,\n",
        "                                                         self.avg_baseline,\n",
        "                                                         predict_reward,\n",
        "                                                         log_softmax,\n",
        "                                                         device=self.device)\n",
        "                critic_loss = Score_Func.dense_critic_loss(normal_batch_reward,\n",
        "                                                           self.avg_baseline,\n",
        "                                                           predict_reward,\n",
        "                                                           device=self.device)\n",
        "            else:\n",
        "                raise ValueError(f\"reward_mode must be one of ['episodic', \"\n",
        "                                 f\"'dense'], but got {self.reward_mode}.\")\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            critic_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # logging\n",
        "            if i == 1 or i % consts.LOG_FREQUENCY == 0:\n",
        "                logging.info('[iter {}] max_reward: {:.4}, '\n",
        "                             'max_reward_batch: {:.4}'.format(i, max_reward,\n",
        "                                                              max_reward_batch))\n",
        "            if i == 1 or i % self.lambda_iter_num == 0:\n",
        "                ls_kv = reward.update_all_scores()\n",
        "                score_min, graph_int_key = ls_kv[0][1][0], ls_kv[0][0]\n",
        "                logging.info('[iter {}] score_min {:.4}'.format(i, score_min * 1.0))\n",
        "                graph_batch = get_graph_from_order(graph_int_key,\n",
        "                                                   dag_mask=self.dag_mask)\n",
        "\n",
        "                if self.reward_regression_type == 'LR':\n",
        "                    graph_batch_pruned = pruning_by_coef(\n",
        "                        graph_batch, data_generator.dataset.cpu().detach().numpy()\n",
        "                    )\n",
        "                elif self.reward_regression_type == 'QR':\n",
        "                    graph_batch_pruned = pruning_by_coef_2nd(\n",
        "                        graph_batch, data_generator.dataset.cpu().detach().numpy()\n",
        "                    )\n",
        "                else:\n",
        "                    raise ValueError(f\"reward_regression_type must be one of \"\n",
        "                                     f\"['LR', 'QR'], but got \"\n",
        "                                     f\"{self.reward_regression_type}.\")\n",
        "\n",
        "        return graph_batch_pruned.T\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c26296f",
      "metadata": {
        "id": "2c26296f"
      },
      "source": [
        "# CORL_Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab4c725b",
      "metadata": {
        "id": "ab4c725b",
        "outputId": "83fd7693-b736-4ac9-e7e4-1c0844a41774"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-03 13:13:14,213 - C:\\Users\\notebook\\castle\\datasets\\simulator.py[line:270] - INFO: Finished synthetic dataset\n",
            "2023-04-03 13:13:14,215 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:175] - INFO: GPU is unavailable.\n",
            "2023-04-03 13:13:14,221 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:233] - INFO: Python version is 3.9.13\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "__init__() got an unexpected keyword argument 'device'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13436\\1959254598.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_dag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'IID_Test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCORL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mGraphDAG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcausal_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_dag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMetricsDAG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcausal_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_dag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\corl\\torch\\corl.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, data, columns, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m                              f'n_nodes: {self.seq_length}.')\n\u001b[0;32m    216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdag_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dag_mask'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m         \u001b[0mcausal_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rl_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m         self.causal_matrix = Tensor(causal_matrix,\n\u001b[0;32m    219\u001b[0m                                     \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\corl\\torch\\corl.py\u001b[0m in \u001b[0;36m_rl_search\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    238\u001b[0m                                        device=self.device)\n\u001b[0;32m    239\u001b[0m         \u001b[1;31m# Instantiating an Actor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         actor = Actor(input_dim=self.input_dim,\n\u001b[0m\u001b[0;32m    241\u001b[0m                       \u001b[0membed_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m                       \u001b[0mencoder_blocks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder_blocks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\corl\\torch\\frame\\_actor.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dim, embed_dim, encoder_name, encoder_blocks, encoder_heads, decoder_name, device)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_instantiation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_instantiation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\corl\\torch\\frame\\_actor.py\u001b[0m in \u001b[0;36m_instantiation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_instantiation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'transformer'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             self.encoder = TransformerEncoder(input_dim=self.input_dim,\n\u001b[0m\u001b[0;32m     74\u001b[0m                                               \u001b[0membed_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                                               \u001b[0mhidden_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENCODER_HIDDEN_DIM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\corl\\torch\\models\\encoders.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dim, embed_dim, hidden_dim, heads, blocks, device)\u001b[0m\n\u001b[0;32m     93\u001b[0m     def __init__(self, input_dim, embed_dim, hidden_dim,\n\u001b[0;32m     94\u001b[0m                  heads=8, blocks=3, device=None) -> None:\n\u001b[1;32m---> 95\u001b[1;33m         super(TransformerEncoder, self).__init__(input_dim=input_dim,\n\u001b[0m\u001b[0;32m     96\u001b[0m                                                  \u001b[0membed_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                                                  \u001b[0mhidden_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\corl\\torch\\models\\_base_network.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dim, embed_dim, hidden_dim, device)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# this layer just for Encoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         self.embedding = nn.Sequential(\n\u001b[1;32m---> 34\u001b[1;33m             nn.Conv1d(in_channels=input_dim,\n\u001b[0m\u001b[0;32m     35\u001b[0m                       \u001b[0mout_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                       \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'device'"
          ]
        }
      ],
      "source": [
        "from castle.algorithms.gradient.corl.torch import CORL\n",
        "from castle.datasets import load_dataset\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "X, true_dag, _ = load_dataset('IID_Test')\n",
        "n = CORL()\n",
        "n.learn(X)\n",
        "GraphDAG(n.causal_matrix, true_dag)\n",
        "met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "print(met.metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c55a6686",
      "metadata": {
        "id": "c55a6686"
      },
      "source": [
        "# TTPM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9e7245e",
      "metadata": {
        "id": "f9e7245e"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# Copyright (C) 2021. Huawei Technologies Co., Ltd. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from itertools import product\n",
        "\n",
        "from castle.common import BaseLearner, Tensor\n",
        "\n",
        "\n",
        "class TTPM(BaseLearner):\n",
        "    \"\"\"\n",
        "    TTPM Algorithm.\n",
        "\n",
        "    A causal structure learning algorithm based on Topological Hawkes process\n",
        "     for spatio-temporal event sequences.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    topology_matrix: np.matrix\n",
        "        Interpreted as an adjacency matrix to generate the graph.\n",
        "        It should have two dimensions, and should be square.\n",
        "\n",
        "    delta: float, default=0.1\n",
        "            Time decaying coefficient for the exponential kernel.\n",
        "\n",
        "    epsilon: int, default=1\n",
        "        BIC penalty coefficient.\n",
        "\n",
        "    max_hop: positive int, default=6\n",
        "        The maximum considered hops in the topology,\n",
        "        when ``max_hop=0``, it is divided by nodes, regardless of topology.\n",
        "\n",
        "    penalty: str, default=BIC\n",
        "        Two optional values: 'BIC' or 'AIC'.\n",
        "\n",
        "    max_iter: int\n",
        "        Maximum number of iterations.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from castle.common import GraphDAG\n",
        "    >>> from castle.metrics import MetricsDAG\n",
        "    >>> from castle.datasets import load_dataset\n",
        "    >>> from castle.algorithms import TTPM\n",
        "    # Data Simulation for TTPM\n",
        "    >>> X, true_causal_matrix, topology_matrix = load_dataset('THP_Test')\n",
        "    >>> ttpm = TTPM(topology_matrix, max_hop=2)\n",
        "    >>> ttpm.learn(X)\n",
        "    >>> causal_matrix = ttpm.causal_matrix\n",
        "    # plot est_dag and true_dag\n",
        "    >>> GraphDAG(ttpm.causal_matrix, true_causal_matrix)\n",
        "    # calculate accuracy\n",
        "    >>> ret_metrix = MetricsDAG(ttpm.causal_matrix, true_causal_matrix)\n",
        "    >>> ret_metrix.metrics\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, topology_matrix, delta=0.1, epsilon=1,\n",
        "                 max_hop=0, penalty='BIC', max_iter=20):\n",
        "        BaseLearner.__init__(self)\n",
        "        assert isinstance(topology_matrix, np.ndarray),\\\n",
        "            'topology_matrix should be np.matrix object'\n",
        "        assert topology_matrix.ndim == 2,\\\n",
        "            'topology_matrix should be two dimension'\n",
        "        assert topology_matrix.shape[0] == topology_matrix.shape[1],\\\n",
        "            'The topology_matrix should be square.'\n",
        "        self._topo = nx.from_numpy_matrix(topology_matrix,\n",
        "                                          create_using=nx.Graph)\n",
        "        # initialize instance variables\n",
        "        self._penalty = penalty\n",
        "        self._delta = delta\n",
        "        self._max_hop = max_hop\n",
        "        self._epsilon = epsilon\n",
        "        self._max_iter = max_iter\n",
        "\n",
        "    def learn(self, tensor, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Set up and run the TTPM algorithm.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        tensor:  pandas.DataFrame\n",
        "            (V 1.0.0, we'll eliminate this constraint in the next version)\n",
        "            The tensor is supposed to contain three cols:\n",
        "                ['event', 'timestamp', 'node']\n",
        "\n",
        "            Description of the three columns:\n",
        "                event: event name (type).\n",
        "                timestamp: occurrence timestamp of event, i.e., '1615962101.0'.\n",
        "                node: topological node where the event happened.\n",
        "        \"\"\"\n",
        "\n",
        "        # data type judgment\n",
        "        if not isinstance(tensor, pd.DataFrame):\n",
        "            raise TypeError('The tensor type is not correct,'\n",
        "                            'only receive pd.DataFrame type currently.')\n",
        "\n",
        "        cols_list = ['event', 'timestamp', 'node']\n",
        "        for col in cols_list:\n",
        "            if col not in tensor.columns:\n",
        "                raise ValueError(\n",
        "                    \"The data tensor should contain column with name {}\".format(\n",
        "                        col))\n",
        "\n",
        "        # initialize needed values\n",
        "        self._start_init(tensor)\n",
        "\n",
        "        # Generate causal matrix (DAG)\n",
        "        _, raw_causal_matrix = self._hill_climb()\n",
        "        self._causal_matrix = Tensor(raw_causal_matrix,\n",
        "                                     index=self._matrix_names,\n",
        "                                     columns=self._matrix_names)\n",
        "\n",
        "    def _start_init(self, tensor):\n",
        "        \"\"\"\n",
        "        Generates some required initial values.\n",
        "        \"\"\"\n",
        "        tensor.dropna(axis=0, how='any', inplace=True)\n",
        "        tensor['timestamp'] = tensor['timestamp'].astype(float)\n",
        "\n",
        "        tensor = tensor.groupby(\n",
        "            ['event', 'timestamp', 'node']).apply(len).reset_index()\n",
        "        tensor.columns = ['event', 'timestamp', 'node', 'times']\n",
        "        tensor = tensor.reindex(columns=['node', 'timestamp', 'event', 'times'])\n",
        "\n",
        "        tensor = tensor.sort_values(['node', 'timestamp'])\n",
        "        self.tensor = tensor[tensor['node'].isin(self._topo.nodes)]\n",
        "\n",
        "        # calculate considered events\n",
        "        self._event_names = np.array(list(set(self.tensor['event'])))\n",
        "        self._event_names.sort()\n",
        "        self._N = len(self._event_names)\n",
        "        self._matrix_names = list(self._event_names.astype(str))\n",
        "\n",
        "        # map event name to corresponding index value\n",
        "        self._event_indexes = self._map_event_to_index(\n",
        "            self.tensor['event'].values, self._event_names)\n",
        "        self.tensor['event'] = self._event_indexes\n",
        "\n",
        "        self._g = self._topo.subgraph(self.tensor['node'].unique())\n",
        "        self._ne_grouped = self.tensor.groupby('node')\n",
        "\n",
        "        self._decay_effects = np.zeros(\n",
        "            [len(self._event_names), self._max_hop+1])  # will be used in EM.\n",
        "\n",
        "        self._max_s_t = tensor['timestamp'].max()\n",
        "        self._min_s_t = tensor['timestamp'].min()\n",
        "\n",
        "        for k in range(self._max_hop+1):\n",
        "            self._decay_effects[:, k] = tensor.groupby('event').apply(\n",
        "                lambda i: ((((1 - np.exp(\n",
        "                    -self._delta * (self._max_s_t - i['timestamp']))) / self._delta)\n",
        "                            * i['times']) * i['node'].apply(\n",
        "                    lambda j: len(self._k_hop_neibors(j, k)))).sum())\n",
        "        # |V|x|T|\n",
        "        self._T = (self._max_s_t - self._min_s_t) * len(tensor['node'].unique())\n",
        "\n",
        "    def _k_hop_neibors(self, node, k):\n",
        "\n",
        "        if k == 0:\n",
        "            return {node}\n",
        "        else:\n",
        "            return set(nx.single_source_dijkstra_path_length(\n",
        "                self._g, node, k).keys()) - set(\n",
        "                nx.single_source_dijkstra_path_length(\n",
        "                    self._g, node, k - 1).keys())\n",
        "\n",
        "    @staticmethod\n",
        "    def _map_event_to_index(event_names, base_event_names):\n",
        "        \"\"\"\n",
        "        Maps the event name to the corresponding index value.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        event_names: np.ndarray, shape like (52622,)\n",
        "            All occurred event names sorted by node and timestamp.\n",
        "        base_event_names: np.ndarray, shape like (10,)\n",
        "            All deduplicated and sorted event names\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray: All occurred event names mapped to their corresponding index\n",
        "         in base_event_names.\n",
        "        \"\"\"\n",
        "        return np.array(list(map(lambda event_name:\n",
        "                                 np.where(base_event_names == event_name)[0][0],\n",
        "                                 event_names)))\n",
        "\n",
        "    def _hill_climb(self):\n",
        "        \"\"\"\n",
        "        Search the best causal graph, then generate the causal matrix (DAG).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        result: tuple, (likelihood, alpha matrix, events vector)\n",
        "            likelihood: used as the score criteria for searching the\n",
        "                causal structure.\n",
        "            alpha matrix: the intensity of causal effect from event v’ to v.\n",
        "            events vector: the exogenous base intensity of each event.\n",
        "        edge_mat: np.ndarray\n",
        "            Causal matrix.\n",
        "        \"\"\"\n",
        "        self._get_effect_tensor_decays()\n",
        "        # Initialize the adjacency matrix\n",
        "        edge_mat = np.eye(self._N, self._N)\n",
        "        result = self._em(edge_mat)\n",
        "        l_ret = result[0]\n",
        "\n",
        "        for num_iter in range(self._max_iter):\n",
        "\n",
        "            logging.info('[iter {}]: likelihood_score = {}'.format(num_iter, l_ret))\n",
        "\n",
        "            stop_tag = True\n",
        "            for new_edge_mat in list(\n",
        "                    self._one_step_change_iterator(edge_mat)):\n",
        "                new_result = self._em(new_edge_mat)\n",
        "                new_l = new_result[0]\n",
        "                # Termination condition:\n",
        "                #   no adjacency matrix with higher likelihood appears\n",
        "                if new_l > l_ret:\n",
        "                    result = new_result\n",
        "                    l_ret = new_l\n",
        "                    stop_tag = False\n",
        "                    edge_mat = new_edge_mat\n",
        "\n",
        "            if stop_tag:\n",
        "                return result, edge_mat\n",
        "\n",
        "        return result, edge_mat\n",
        "\n",
        "    def _get_effect_tensor_decays(self):\n",
        "\n",
        "        self._effect_tensor_decays = np.zeros([self._max_hop+1,\n",
        "                                               len(self.tensor),\n",
        "                                               len(self._event_names)])\n",
        "        for k in range(self._max_hop+1):\n",
        "            self._get_effect_tensor_decays_each_hop(k)\n",
        "\n",
        "    def _get_effect_tensor_decays_each_hop(self, k):\n",
        "\n",
        "        j = 0\n",
        "        pre_effect = np.zeros(self._N)\n",
        "        tensor_array = self.tensor.values\n",
        "        for item_ind in range(len(self.tensor)):\n",
        "            sub_n, start_t, ala_i, times = tensor_array[\n",
        "                item_ind, [0, 1, 2, 3]]\n",
        "            last_sub_n, last_start_t, last_ala_i, last_times = \\\n",
        "                tensor_array[item_ind - 1, [0, 1, 2, 3]]\n",
        "            if (last_sub_n != sub_n) or (last_start_t > start_t):\n",
        "                j = 0\n",
        "                pre_effect = np.zeros(self._N)\n",
        "                try:\n",
        "                    k_hop_neighbors_ne = self._k_hop_neibors(sub_n, k)\n",
        "                    neighbors_table = pd.concat(\n",
        "                        [self._ne_grouped.get_group(i)\n",
        "                         for i in k_hop_neighbors_ne])\n",
        "                    neighbors_table = neighbors_table.sort_values(\n",
        "                        'timestamp')\n",
        "                    neighbors_table_value = neighbors_table.values\n",
        "                except ValueError as e:\n",
        "                    k_hop_neighbors_ne = []\n",
        "\n",
        "                if len(k_hop_neighbors_ne) == 0:\n",
        "                    continue\n",
        "\n",
        "            cur_effect = pre_effect * np.exp(\n",
        "                (np.min((last_start_t - start_t, 0))) * self._delta)\n",
        "            while 1:\n",
        "                try:\n",
        "                    nei_sub_n, nei_start_t, nei_ala_i, nei_times \\\n",
        "                        = neighbors_table_value[j, :]\n",
        "                except:\n",
        "                    break\n",
        "                if nei_start_t < start_t:\n",
        "                    cur_effect[int(nei_ala_i)] += nei_times * np.exp(\n",
        "                        (nei_start_t - start_t) * self._delta)\n",
        "                    j += 1\n",
        "                else:\n",
        "                    break\n",
        "            pre_effect = cur_effect\n",
        "\n",
        "            self._effect_tensor_decays[k, item_ind] = pre_effect\n",
        "\n",
        "    def _em(self, edge_mat):\n",
        "        \"\"\"\n",
        "        E-M module, used to find the optimal parameters.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        edge_mat： np.ndarray\n",
        "            Adjacency matrix.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        likelihood: used as the score criteria for searching the\n",
        "            causal structure.\n",
        "        alpha matrix: the intensity of causal effect from event v’ to v.\n",
        "        events vector: the exogenous base intensity of each event.\n",
        "        \"\"\"\n",
        "\n",
        "        causal_g = nx.from_numpy_matrix((edge_mat - np.eye(self._N, self._N)),\n",
        "                                        create_using=nx.DiGraph)\n",
        "\n",
        "        if not nx.is_directed_acyclic_graph(causal_g):\n",
        "            return -100000000000000, \\\n",
        "                   np.zeros([len(self._event_names), len(self._event_names)]), \\\n",
        "                   np.zeros(len(self._event_names))\n",
        "\n",
        "        # Initialize alpha:(nxn)，mu:(nx1) and L\n",
        "        alpha = np.ones([self._max_hop+1, len(self._event_names),\n",
        "                         len(self._event_names)])\n",
        "        alpha = alpha * edge_mat\n",
        "        mu = np.ones(len(self._event_names))\n",
        "        l_init = 0\n",
        "\n",
        "        for i in range(len(self._event_names)):\n",
        "            pa_i = set(np.where(edge_mat[:, i] == 1)[0])\n",
        "            li = -100000000000\n",
        "            ind = np.where(self._event_indexes == i)\n",
        "            x_i = self.tensor['times'].values[ind]\n",
        "            x_i_all = np.zeros_like(self.tensor['times'].values)\n",
        "            x_i_all[ind] = x_i\n",
        "            while 1:\n",
        "                # Calculate the first part of the likelihood\n",
        "                lambda_i_sum = (self._decay_effects\n",
        "                                * alpha[:, :, i].T).sum() + mu[i] * self._T\n",
        "\n",
        "                # Calculate the second part of the likelihood\n",
        "                lambda_for_i = np.zeros(len(self.tensor)) + mu[i]\n",
        "                for k in range(self._max_hop+1):\n",
        "                    lambda_for_i += np.matmul(\n",
        "                        self._effect_tensor_decays[k, :],\n",
        "                        alpha[k, :, i].T)\n",
        "                lambda_for_i = lambda_for_i[ind]\n",
        "                x_log_lambda = (x_i * np.log(lambda_for_i)).sum()\n",
        "                new_li = -lambda_i_sum + x_log_lambda\n",
        "\n",
        "                # Iteration termination condition\n",
        "                delta = new_li - li\n",
        "                if delta < 0.1:\n",
        "                    li = new_li\n",
        "                    l_init += li\n",
        "                    pa_i_alpha = dict()\n",
        "                    for j in pa_i:\n",
        "                        pa_i_alpha[j] = alpha[:, j, i]\n",
        "                    break\n",
        "                li = new_li\n",
        "                # update mu\n",
        "                mu[i] = ((mu[i] / lambda_for_i) * x_i).sum() / self._T\n",
        "                # update alpha\n",
        "                for j in pa_i:\n",
        "                    for k in range(self._max_hop+1):\n",
        "                        upper = ((alpha[k, j, i] * (\n",
        "                            self._effect_tensor_decays[k, :, j])[ind]\n",
        "                                  / lambda_for_i) * x_i).sum()\n",
        "                        lower = self._decay_effects[j, k]\n",
        "                        if lower == 0:\n",
        "                            alpha[k, j, i] = 0\n",
        "                            continue\n",
        "                        alpha[k, j, i] = upper / lower\n",
        "            i += 1\n",
        "\n",
        "        if self._penalty == 'AIC':\n",
        "            return l_init - (len(self._event_names)\n",
        "                             + self._epsilon * edge_mat.sum()\n",
        "                             * (self._max_hop+1)), alpha, mu\n",
        "        elif self._penalty == 'BIC':\n",
        "            return l_init - (len(self._event_names)\n",
        "                             + self._epsilon * edge_mat.sum()\n",
        "                             * (self._max_hop+1)) * np.log(\n",
        "                self.tensor['times'].sum()) / 2, alpha, mu\n",
        "        else:\n",
        "            raise ValueError(\"The penalty's value should be BIC or AIC.\")\n",
        "\n",
        "    def _one_step_change_iterator(self, edge_mat):\n",
        "\n",
        "        return map(lambda e: self._one_step_change(edge_mat, e),\n",
        "                   product(range(len(self._event_names)),\n",
        "                           range(len(self._event_names))))\n",
        "\n",
        "    @staticmethod\n",
        "    def _one_step_change(edge_mat, e):\n",
        "        \"\"\"\n",
        "        Changes the edge value in the edge_mat.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        edge_mat: np.ndarray\n",
        "            Adjacency matrix.\n",
        "        e: tuple_like (j,i)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        new_edge_mat: np.ndarray\n",
        "            new value of edge\n",
        "        \"\"\"\n",
        "        j, i = e\n",
        "        if j == i:\n",
        "            return edge_mat\n",
        "        new_edge_mat = edge_mat.copy()\n",
        "\n",
        "        if new_edge_mat[j, i] == 1:\n",
        "            new_edge_mat[j, i] = 0\n",
        "            return new_edge_mat\n",
        "        else:\n",
        "            new_edge_mat[j, i] = 1\n",
        "            new_edge_mat[i, j] = 0\n",
        "            return new_edge_mat\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4079bb8",
      "metadata": {
        "id": "c4079bb8"
      },
      "source": [
        "# TTPM_Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "927b495d",
      "metadata": {
        "id": "927b495d",
        "outputId": "89874eba-5cdd-4ccd-8f9b-526491d532f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 810.65it/s]\n",
            "100%|████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 2337.31it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 10006.69it/s]\n",
            "2023-04-03 13:13:39,313 - C:\\Users\\notebook\\castle\\algorithms\\ttpm\\ttpm.py[line:225] - INFO: [iter 0]: likelihood_score = -4745.714626482855\n",
            "2023-04-03 13:13:39,802 - C:\\Users\\notebook\\castle\\algorithms\\ttpm\\ttpm.py[line:225] - INFO: [iter 1]: likelihood_score = -4670.178490689865\n",
            "2023-04-03 13:13:40,388 - C:\\Users\\notebook\\castle\\algorithms\\ttpm\\ttpm.py[line:225] - INFO: [iter 2]: likelihood_score = -4645.976581524495\n",
            "2023-04-03 13:13:41,034 - C:\\Users\\notebook\\castle\\algorithms\\ttpm\\ttpm.py[line:225] - INFO: [iter 3]: likelihood_score = -4625.696760996663\n",
            "2023-04-03 13:13:41,665 - C:\\Users\\notebook\\castle\\algorithms\\ttpm\\ttpm.py[line:225] - INFO: [iter 4]: likelihood_score = -4617.056738533341\n",
            "2023-04-03 13:13:42,278 - C:\\Users\\notebook\\castle\\algorithms\\ttpm\\ttpm.py[line:225] - INFO: [iter 5]: likelihood_score = -4610.224521534543\n",
            "2023-04-03 13:13:43,024 - C:\\Users\\notebook\\castle\\algorithms\\ttpm\\ttpm.py[line:225] - INFO: [iter 6]: likelihood_score = -4606.5080413199985\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'fdr': 0.0,\n",
              " 'tpr': 0.8571,\n",
              " 'fpr': 0.0,\n",
              " 'shd': 1,\n",
              " 'nnz': 6,\n",
              " 'precision': 1.0,\n",
              " 'recall': 0.8571,\n",
              " 'F1': 0.9231,\n",
              " 'gscore': 0.8571}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "from castle.datasets import load_dataset\n",
        "from castle.algorithms import TTPM\n",
        "    # Data Simulation for TTPM\n",
        "X, true_causal_matrix, topology_matrix = load_dataset('THP_Test')\n",
        "ttpm = TTPM(topology_matrix, max_hop=2)\n",
        "ttpm.learn(X)\n",
        "causal_matrix = ttpm.causal_matrix\n",
        "    # plot est_dag and true_dag\n",
        "GraphDAG(ttpm.causal_matrix, true_causal_matrix)\n",
        "    # calculate accuracy\n",
        "ret_metrix = MetricsDAG(ttpm.causal_matrix, true_causal_matrix)\n",
        "ret_metrix.metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd39f98d",
      "metadata": {
        "id": "dd39f98d"
      },
      "source": [
        "# Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c90bc0e3",
      "metadata": {
        "id": "c90bc0e3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from castle.algorithms.anm import ANMNonlinear\n",
        "# np.random.seed(1)\n",
        "# x = np.random.rand(500, 2)\n",
        "data = np.load('dataset/linear_gauss_6nodes_15edges.npz', allow_pickle=True)\n",
        "x = data['x']\n",
        "true_dag = data['y']\n",
        "rank = np.linalg.matrix_rank(true_dag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ea5b718",
      "metadata": {
        "id": "5ea5b718",
        "outputId": "eaf30bbd-66e2-494b-abda-c0215d6ec284"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  0.867,   1.924,  -6.816,  -2.305,  -7.145,   2.09 ],\n",
              "       [  0.698,   0.991,  -3.66 ,  -0.943,  -6.728,   0.86 ],\n",
              "       [ -0.512,   0.381,   0.982,   0.409,   3.64 ,   0.787],\n",
              "       ...,\n",
              "       [  3.665,   0.944,  -2.667,   0.546, -12.22 ,  -1.127],\n",
              "       [ -0.626,   0.606,   0.419,  -1.02 ,   3.479,   1.406],\n",
              "       [ -1.361,   0.135,  -1.56 ,  -1.479,   0.074,   1.162]])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e22181c",
      "metadata": {
        "id": "3e22181c",
        "outputId": "a1cb034c-3e45-40c3-e192-6f061bf34c15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.924],\n",
              "       [0.991],\n",
              "       [0.381],\n",
              "       ...,\n",
              "       [0.944],\n",
              "       [0.606],\n",
              "       [0.135]])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[:, [1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38d2353f",
      "metadata": {
        "id": "38d2353f",
        "outputId": "e1f41a96-9743-488e-cb2e-9489d7cb87a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 1, 0, 1, 0],\n",
              "       [1, 0, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 1, 0, 1, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 1, 0, 1, 0]])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "true_dag"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86c338ee",
      "metadata": {
        "id": "86c338ee"
      },
      "source": [
        "# Test for linear_gauss_6nodes_15edge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a534445b",
      "metadata": {
        "id": "a534445b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "data = np.load('dataset/linear_gauss_6nodes_15edges.npz', allow_pickle=True)\n",
        "x_ = data['x']\n",
        "true_dag = data['y']\n",
        "x = x_[:, [0]]\n",
        "y = x_[:, [1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e43e26ec",
      "metadata": {
        "id": "e43e26ec"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fda25943",
      "metadata": {
        "id": "fda25943"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "celltoolbar": "原始单元格格式",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}